<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Redis为什么这么快？一文深入了解Redis内存模型]]></title>
    <url>%2F2018%2F04%2F24%2F2018-04-24-redis%2F</url>
    <content type="text"><![CDATA[Redis 是目前最火爆的内存数据库之一，通过在内存中读写数据，大大提高了读写速度，可以说 Redis 是实现网站高并发不可或缺的一部分。 我们使用 Redis 时，会接触 Redis 的 5 种对象类型（字符串、哈希、列表、集合、有序集合），丰富的类型是 Redis 相对于 Memcached 等的一大优势。 在了解 Redis 的 5 种对象类型的用法和特点的基础上，进一步了解 Redis 的内存模型，对 Redis 的使用有很大帮助，例如： 估算 Redis 内存使用量。目前为止，内存的使用成本仍然相对较高，使用内存不能无所顾忌；根据需求合理的评估 Redis 的内存使用量，选择合适的机器配置，可以在满足需求的情况下节约成本。 优化内存占用。了解 Redis 内存模型可以选择更合适的数据类型和编码，更好的利用 Redis 内存。 分析解决问题。当 Redis 出现阻塞、内存占用等问题时，尽快发现导致问题的原因，便于分析解决问题。 这篇文章主要介绍 Redis 的内存模型（以 3.0 为例），包括 Redis 占用内存的情况及如何查询、不同的对象类型在内存中的编码方式、内存分配器(jemalloc)、简单动态字符串(SDS)、RedisObject 等；然后在此基础上介绍几个 Redis 内存模型的应用。 Redis 内存统计工欲善其事必先利其器，在说明 Redis 内存之前首先说明如何统计 Redis 使用内存的情况。 在客户端通过 redis-cli 连接服务器后（后面如无特殊说明，客户端一律使用redis-cli），通过 info 命令可以查看内存使用情况：info memory。 其中，info 命令可以显示 Redis 服务器的许多信息，包括服务器基本信息、CPU、内存、持久化、客户端连接信息等等；Memory 是参数，表示只显示内存相关的信息。 返回结果中比较重要的几个说明如下： used_memoryRedis 分配器分配的内存总量（单位是字节），包括使用的虚拟内存（即 swap）；Redis 分配器后面会介绍。used_memory_human 只是显示更友好。 used_memory_rssRedis 进程占据操作系统的内存（单位是字节），与 top 及 ps 命令看到的值是一致的。 除了分配器分配的内存之外，used_memory_rss 还包括进程运行本身需要的内存、内存碎片等，但是不包括虚拟内存。 因此，used_memory 和 used_memory_rss，前者是从 Redis 角度得到的量，后者是从操作系统角度得到的量。 二者之所以有所不同，一方面是因为内存碎片和 Redis 进程运行需要占用内存，使得前者可能比后者小，另一方面虚拟内存的存在，使得前者可能比后者大。 由于在实际应用中，Redis 的数据量会比较大，此时进程运行占用的内存与 Redis 数据量和内存碎片相比，都会小得多。 因此 used_memory_rss 和 used_memory 的比例，便成了衡量 Redis 内存碎片率的参数；这个参数就是 mem_fragmentation_ratio。 mem_fragmentation_ratio内存碎片比率，该值是 used_memory_rss / used_memory 的比值。 mem_fragmentation_ratio 一般大于 1，且该值越大，内存碎片比例越大；mem_fragmentation_ratio&lt;1，说明 Redis 使用了虚拟内存，由于虚拟内存的媒介是磁盘，比内存速度要慢很多。 当这种情况出现时，应该及时排查，如果内存不足应该及时处理，如增加 Redis 节点、增加 Redis 服务器的内存、优化应用等。 一般来说，mem_fragmentation_ratio 在 1.03 左右是比较健康的状态（对于 jemalloc 来说）。 上面截图中的 mem_fragmentation_ratio 值很大，是因为还没有向 Redis 中存入数据，Redis 进程本身运行的内存使得 used_memory_rss 比 used_memory 大得多。 mem_allocatorRedis 使用的内存分配器，在编译时指定；可以是 libc 、jemalloc 或者 tcmalloc，默认是 jemalloc；截图中使用的便是默认的 jemalloc。 Redis 内存划分Redis 作为内存数据库，在内存中存储的内容主要是数据（键值对）；通过前面的叙述可以知道，除了数据以外，Redis 的其他部分也会占用内存。 Redis 的内存占用主要可以划分为以下几个部分： 数据作为数据库，数据是最主要的部分；这部分占用的内存会统计在 used_memory 中。 Redis 使用键值对存储数据，其中的值（对象）包括 5 种类型，即字符串、哈希、列表、集合、有序集合。 这 5 种类型是 Redis 对外提供的，实际上，在 Redis 内部，每种类型可能有 2 种或更多的内部编码实现。 此外，Redis 在存储对象时，并不是直接将数据扔进内存，而是会对对象进行各种包装：如 RedisObject、SDS 等；这篇文章后面将重点介绍 Redis 中数据存储的细节。 进程本身运行需要的内存Redis 主进程本身运行肯定需要占用内存，如代码、常量池等等；这部分内存大约几兆，在大多数生产环境中与 Redis 数据占用的内存相比可以忽略。 这部分内存不是由 jemalloc 分配，因此不会统计在 used_memory 中。 补充说明：除了主进程外，Redis 创建的子进程运行也会占用内存，如 Redis 执行 AOF、RDB 重写时创建的子进程。 当然，这部分内存不属于 Redis 进程，也不会统计在 used_memory 和 used_memory_rss 中。 缓冲内存缓冲内存包括客户端缓冲区、复制积压缓冲区、AOF 缓冲区等；其中，客户端缓冲区存储客户端连接的输入输出缓冲；复制积压缓冲区用于部分复制功能；AOF 缓冲区用于在进行 AOF 重写时，保存最近的写入命令。 在了解相应功能之前，不需要知道这些缓冲的细节；这部分内存由 jemalloc 分配，因此会统计在 used_memory 中。 内存碎片内存碎片是 Redis 在分配、回收物理内存过程中产生的。例如，如果对数据的更改频繁，而且数据之间的大小相差很大，可能导致 Redis 释放的空间在物理内存中并没有释放,但 Redis 又无法有效利用，这就形成了内存碎片，内存碎片不会统计在 used_memory 中。 内存碎片的产生与对数据进行的操作、数据的特点等都有关；此外，与使用的内存分配器也有关系：如果内存分配器设计合理，可以尽可能的减少内存碎片的产生。后面将要说到的 jemalloc 便在控制内存碎片方面做的很好。 如果 Redis 服务器中的内存碎片已经很大，可以通过安全重启的方式减小内存碎片：因为重启之后，Redis 重新从备份文件中读取数据，在内存中进行重排，为每个数据重新选择合适的内存单元，减小内存碎片。 Redis 数据存储的细节关于 Redis 数据存储的细节，涉及到内存分配器（如 jemalloc）、简单动态字符串（SDS）、5 种对象类型及内部编码、RedisObject。在讲述具体内容之前，先说明一下这几个概念之间的关系。 下图是执行 set hello world 时，所涉及到的数据模型： dictEntry：Redis 是 Key-Value 数据库，因此对每个键值对都会有一个 dictEntry，里面存储了指向 Key 和 Value 的指针；next 指向下一个 dictEntry，与本 Key-Value 无关。 Key：图中右上角可见，Key（”hello”）并不是直接以字符串存储，而是存储在 SDS结构中。 RedisObject：Value(“world”)既不是直接以字符串存储，也不是像 Key 一样直接存储在 SDS 中，而是存储在 RedisObject 中。 实际上，不论 Value 是 5 种类型的哪一种，都是通过 RedisObject 来存储的；而 RedisObject 中的 type 字段指明了 Value 对象的类型，ptr 字段则指向对象所在的地址。 不过可以看出，字符串对象虽然经过了 RedisObject 的包装，但仍然需要通过 SDS 存储。 实际上，RedisObject 除了 type 和 ptr 字段以外，还有其他字段图中没有给出，如用于指定对象内部编码的字段。 jemalloc：无论是 DictEntry 对象，还是 RedisObject、SDS 对象，都需要内存分配器（如 jemalloc）分配内存进行存储。 以 DictEntry 对象为例，有 3 个指针组成，在 64 位机器下占 24 个字节，jemalloc 会为它分配 32 字节大小的内存单元。 下面来分别介绍 jemalloc、RedisObject、SDS、对象类型及内部编码。 jemallocRedis 在编译时便会指定内存分配器；内存分配器可以是 libc 、jemalloc 或者 tcmalloc，默认是 jemalloc。 jemalloc 作为 Redis 的默认内存分配器，在减小内存碎片方面做的相对比较好。 jemalloc 在 64 位系统中，将内存空间划分为小、大、巨大三个范围；每个范围内又划分了许多小的内存块单位；当 Redis 存储数据时，会选择大小最合适的内存块进行存储。 jemalloc 划分的内存单元如下图所示： RedisObject前面说到，Redis 对象有 5 种类型；无论是哪种类型，Redis 都不会直接存储，而是通过 RedisObject 对象进行存储。 RedisObject 对象非常重要，Redis 对象的类型、内部编码、内存回收、共享对象等功能，都需要 RedisObject 支持，下面将通过 RedisObject 的结构来说明它是如何起作用的。 RedisObject 的定义如下（不同版本的 Redis 可能稍稍有所不同）： RedisObject 的每个字段的含义和作用如下： type type 字段表示对象的类型，占 4 个比特；目前包括 REDIS_STRING(字符串)、REDIS_LIST (列表)、REDIS_HASH(哈希)、REDIS_SET(集合)、REDIS_ZSET(有序集合)。 当我们执行 type 命令时，便是通过读取 RedisObject 的 type 字段获得对象的类型；如下图所示： encoding encoding 表示对象的内部编码，占 4 个比特。对于 Redis 支持的每种类型，都有至少两种内部编码，例如对于字符串，有 int、embstr、raw 三种编码。 通过 encoding 属性，Redis 可以根据不同的使用场景来为对象设置不同的编码，大大提高了 Redis 的灵活性和效率。 以列表对象为例，有压缩列表和双端链表两种编码方式；如果列表中的元素较少，Redis 倾向于使用压缩列表进行存储，因为压缩列表占用内存更少，而且比双端链表可以更快载入。 当列表对象元素较多时，压缩列表就会转化为更适合存储大量元素的双端链表。 通过 object encoding 命令，可以查看对象采用的编码方式，如下图所示： 5 种对象类型对应的编码方式以及使用条件，将在后面介绍。 lru lru 记录的是对象最后一次被命令程序访问的时间，占据的比特数不同的版本有所不同（如 4.0 版本占 24 比特，2.6 版本占 22 比特）。 通过对比 lru 时间与当前时间，可以计算某个对象的空转时间；object idletime 命令可以显示该空转时间（单位是秒）。object idletime 命令的一个特殊之处在于它不改变对象的 lru 值。 lru 值除了通过 object idletime 命令打印之外，还与 Redis 的内存回收有关系。 如果 Redis 打开了 maxmemory 选项，且内存回收算法选择的是 volatile-lru 或 allkeys—lru，那么当 Redis 内存占用超过 maxmemory 指定的值时，Redis 会优先选择空转时间最长的对象进行释放。 refcount refcount 与共享对象：refcount 记录的是该对象被引用的次数，类型为整型。refcount 的作用，主要在于对象的引用计数和内存回收。 当创建新对象时，refcount 初始化为 1；当有新程序使用该对象时，refcount 加 1；当对象不再被一个新程序使用时，refcount 减 1；当 refcount 变为 0 时，对象占用的内存会被释放。 Redis 中被多次使用的对象(refcount&gt;1)，称为共享对象。Redis 为了节省内存，当有一些对象重复出现时，新的程序不会创建新的对象，而是仍然使用原来的对象。 这个被重复使用的对象，就是共享对象。目前共享对象仅支持整数值的字符串对象。 共享对象的具体实现：Redis 的共享对象目前只支持整数值的字符串对象。之所以如此，实际上是对内存和 CPU（时间）的平衡：共享对象虽然会降低内存消耗，但是判断两个对象是否相等却需要消耗额外的时间。 对于整数值，判断操作复杂度为 O(1)；对于普通字符串，判断复杂度为 O(n)；而对于哈希、列表、集合和有序集合，判断的复杂度为 O(n^2)。 虽然共享对象只能是整数值的字符串对象，但是5种类型都可能使用共享对象（如哈希、列表等的元素可以使用）。 就目前的实现来说，Redis 服务器在初始化时，会创建 10000 个字符串对象，值分别是 0~9999 的整数值；当 Redis 需要使用值为 0~9999 的字符串对象时，可以直接使用这些共享对象。 10000 这个数字可以通过调整参数 REDIS_SHARED_INTEGERS（4.0 中是 OBJ_SHARED_INTEGERS）的值进行改变。 共享对象的引用次数可以通过 object refcount 命令查看，如下图所示。命令执行的结果页佐证了只有 0~9999 之间的整数会作为共享对象。 ptr ptr 指针指向具体的数据，如前面的例子中，set hello world，ptr 指向包含字符串 world 的 SDS。 综上所述，RedisObject 的结构与对象类型、编码、内存回收、共享对象都有关系。 一个 RedisObject 对象的大小为 16 字节：4bit+4bit+24bit+4Byte+8Byte=16Byte。 SDS Redis 没有直接使用 C 字符串(即以空字符’0’结尾的字符数组)作为默认的字符串表示，而是使用了 SDS。SDS 是简单动态字符串(Simple Dynamic String)的缩写。 SDS 结构 SDS 的结构如下： 其中，buf 表示字节数组，用来存储字符串；len 表示 buf 已使用的长度；free 表示 buf 未使用的长度。 下面是两个例子： 通过 SDS 的结构可以看出，buf 数组的长度=free+len+1（其中 1 表示字符串结尾的空字符）。 所以，一个 SDS 结构占据的空间为：free 所占长度+len 所占长度+ buf 数组的长度=4+4+free+len+1=free+len+9。 SDS 与 C 字符串的比较 SDS 在 C 字符串的基础上加入了 free 和 len 字段，带来了很多好处： 获取字符串长度：SDS 是 O(1)，C 字符串是 O(n)。 缓冲区溢出：使用 C 字符串的 API 时，如果字符串长度增加（如 strcat 操作）而忘记重新分配内存，很容易造成缓冲区的溢出。 而 SDS 由于记录了长度，相应的 API 在可能造成缓冲区溢出时会自动重新分配内存，杜绝了缓冲区溢出。 修改字符串时内存的重分配：对于 C 字符串，如果要修改字符串，必须要重新分配内存（先释放再申请），因为如果没有重新分配，字符串长度增大时会造成内存缓冲区溢出，字符串长度减小时会造成内存泄露。 而对于 SDS，由于可以记录 len 和 free，因此解除了字符串长度和空间数组长度之间的关联，可以在此基础上进行优化。 空间预分配策略（即分配内存时比实际需要的多）使得字符串长度增大时重新分配内存的概率大大减小；惰性空间释放策略使得字符串长度减小时重新分配内存的概率大大减小。 存取二进制数据：SDS 可以，C 字符串不可以。因为 C 字符串以空字符作为字符串结束的标识，而对于一些二进制文件（如图片等）。 内容可能包括空字符串，因此 C 字符串无法正确存取；而 SDS 以字符串长度 len 来作为字符串结束标识，因此没有这个问题。 此外，由于 SDS 中的 buf 仍然使用了 C 字符串（即以’0’结尾），因此 SDS 可以使用 C 字符串库中的部分函数。 但是需要注意的是，只有当 SDS 用来存储文本数据时才可以这样使用，在存储二进制数据时则不行（’0’不一定是结尾）。 SDS 与 C 字符串的应用 Redis 在存储对象时，一律使用 SDS 代替 C 字符串。例如 set hello world 命令，hello 和 world 都是以 SDS 的形式存储的。 而 sadd myset member1 member2 member3 命令，不论是键（“myset”），还是集合中的元素（“member1”、 “member2”和“member3”），都是以 SDS 的形式存储。 除了存储对象，SDS 还用于存储各种缓冲区。只有在字符串不会改变的情况下，如打印日志时，才会使用 C 字符串。 Redis 的对象类型与内部编码 前面已经说过，Redis 支持 5 种对象类型，而每种结构都有至少两种编码。 这样做的好处在于：一方面接口与实现分离，当需要增加或改变内部编码时，用户使用不受影响，另一方面可以根据不同的应用场景切换内部编码，提高效率。 Redis 各种对象类型支持的内部编码如下图所示(图中版本是 Redis3.0，Redis 后面版本中又增加了内部编码，略过不提；本章所介绍的内部编码都是基于 3.0 的)： 关于 Redis 内部编码的转换，都符合以下规律：编码转换在 Redis 写入数据时完成，且转换过程不可逆，只能从小内存编码向大内存编码转换。 字符串 字符串是最基础的类型，因为所有的键都是字符串类型，且字符串之外的其他几种复杂类型的元素也是字符串，字符串长度不能超过 512MB。 内部编码 字符串类型的内部编码有 3 种，它们的应用场景如下： int：8 个字节的长整型。字符串值是整型时，这个值使用 long 整型表示。 embstr：&lt;=39 字节的字符串。embstr 与 raw 都使用 RedisObject 和 sds 保存数据。 区别在于：embstr 的使用只分配一次内存空间（因此 RedisObject 和 sds 是连续的），而 raw 需要分配两次内存空间（分别为 RedisObject 和 sds 分配空间）。 因此与 raw 相比，embstr 的好处在于创建时少分配一次空间，删除时少释放一次空间，以及对象的所有数据连在一起，寻找方便。 而 embstr 的坏处也很明显，如果字符串的长度增加需要重新分配内存时，整个 RedisObject 和 sds 都需要重新分配空间，因此 Redis 中的 embstr 实现为只读。 raw：大于 39 个字节的字符串。 示例如下图所示： embstr 和 raw 进行区分的长度，是 39；是因为 RedisObject 的长度是 16 字节，sds 的长度是 9+ 字符串长度。 因此当字符串长度是 39 时，embstr 的长度正好是 16+9+39=64，jemalloc 正好可以分配 64 字节的内存单元。 编码转换 当 int 数据不再是整数，或大小超过了 long 的范围时，自动转化为 raw。 而对于 embstr，由于其实现是只读的，因此在对 embstr 对象进行修改时，都会先转化为 raw 再进行修改。 因此，只要是修改 embstr 对象，修改后的对象一定是 raw 的，无论是否达到了 39 个字节。 示例如下图所示： 列表 列表（list）用来存储多个有序的字符串，每个字符串称为元素；一个列表可以存储 2^32-1 个元素。 Redis 中的列表支持两端插入和弹出，并可以获得指定位置（或范围）的元素，可以充当数组、队列、栈等。 内部编码 列表的内部编码可以是压缩列表（ziplist）或双端链表（linkedlist)。 双端链表：由一个 list 结构和多个 listNode 结构组成；典型结构如下图所示： 通过图中可以看出，双端链表同时保存了表头指针和表尾指针，并且每个节点都有指向前和指向后的指针。 链表中保存了列表的长度；dup、free 和 match 为节点值设置类型特定函数。 所以链表可以用于保存各种不同类型的值，而链表中每个节点指向的是type为字符串的 RedisObject。 压缩列表：压缩列表是 Redis 为了节约内存而开发的，是由一系列特殊编码的连续内存块(而不是像双端链表一样每个节点是指针)组成的顺序型数据结构，具体结构相对比较复杂。 与双端链表相比，压缩列表可以节省内存空间，但是进行修改或增删操作时，复杂度较高。 因此当节点数量较少时，可以使用压缩列表；但是节点数量多时，还是使用双端链表划算。 压缩列表不仅用于实现列表，也用于实现哈希、有序列表；使用非常广泛。 编码转换 只有同时满足下面两个条件时，才会使用压缩列表：列表中元素数量小于 512 个；列表中所有字符串对象都不足 64 字节。 如果有一个条件不满足，则使用双端列表；且编码只可能由压缩列表转化为双端链表，反方向则不可能。 下图展示了列表编码转换的特点： 其中，单个字符串不能超过 64 字节，是为了便于统一分配每个节点的长度。 这里的 64 字节是指字符串的长度，不包括 SDS 结构，因为压缩列表使用连续、定长内存块存储字符串，不需要 SDS 结构指明长度。 后面提到压缩列表，也会强调长度不超过 64 字节，原理与这里类似。 哈希 哈希（作为一种数据结构），不仅是 Redis 对外提供的 5 种对象类型的一种（与字符串、列表、集合、有序结合并列），也是 Redis 作为 Key-Value 数据库所使用的数据结构。 为了说明的方便，在本文后面当使用“内层的哈希”时，代表的是 Redis 对外提供的 5 种对象类型的一种；使用“外层的哈希”代指 Redis 作为 Key-Value 数据库所使用的数据结构。 内部编码 内层的哈希使用的内部编码可以是压缩列表（ziplist）和哈希表（hashtable）2 种；Redis 的外层的哈希则只使用了 hashtable。 压缩列表前面已介绍，与哈希表相比，压缩列表用于元素个数少、元素长度小的场景；其优势在于集中存储，节省空间。 同时，虽然对于元素的操作复杂度也由 O(n)变为了 O(1)，但由于哈希中元素数量较少，因此操作的时间并没有明显劣势。 hashtable：一个 hashtable 由 1 个 dict 结构、2 个 dictht 结构、1 个 dictEntry 指针数组（称为 bucket）和多个 dictEntry 结构组成。 正常情况下（即 hashtable 没有进行 rehash 时），各部分关系如下图所示： 下面从底层向上依次介绍各个部分： dictEntry：dictEntry 结构用于保存键值对，结构定义如下。 其中，各个属性的功能如下： key：键值对中的键。 val：键值对中的值，使用 union(即共用体)实现，存储的内容既可能是一个指向值的指针，也可能是 64 位整型，或无符号 64 位整型。 next：指向下一个 dictEntry，用于解决哈希冲突问题。 在 64 位系统中，一个 dictEntry 对象占 24 字节（key/val/next 各占 8 字节）。 bucket：bucket 是一个数组，数组的每个元素都是指向 dictEntry 结构的指针。 Redis 中 bucket 数组的大小计算规则如下：大于 dictEntry 的、最小的 2^n。 例如，如果有 1000 个 dictEntry，那么 bucket 大小为 1024；如果有 1500 个 dictEntry，则 bucket 大小为 2048。 dictht：dictht 结构如下。 其中，各个属性的功能说明如下： table 属性是一个指针，指向 bucket。 size 属性记录了哈希表的大小，即 bucket 的大小。 used 记录了已使用的 dictEntry 的数量。 sizemask 属性的值总是为 size-1，这个属性和哈希值一起决定一个键在 table 中存储的位置。 dict：一般来说，通过使用 dictht 和 dictEntry 结构，便可以实现普通哈希表的功能。 但是 Redis 的实现中，在 dictht 结构的上层，还有一个 dict 结构。下面说明 dict 结构的定义及作用。 dict 结构如下： 其中，type 属性和 privdata 属性是为了适应不同类型的键值对，用于创建多态字典。 ht 属性和 trehashidx 属性则用于 rehash，即当哈希表需要扩展或收缩时使用。 ht 是一个包含两个项的数组，每项都指向一个 dictht 结构，这也是 Redis 的哈希会有 1 个 dict、2 个 dictht 结构的原因。 通常情况下，所有的数据都是存在放 dict 的 ht[0] 中，ht[1] 只在 rehash 的时候使用。 dict 进行 rehash 操作的时候，将 ht[0] 中的所有数据 rehash 到 ht[1] 中。然后将 ht[1] 赋值给 ht[0]，并清空 ht[1]。 因此，Redis 中的哈希之所以在 dictht 和 dictEntry 结构之外还有一个 dict 结构，一方面是为了适应不同类型的键值对，另一方面是为了 rehash。 编码转换 如前所述，Redis 中内层的哈希既可能使用哈希表，也可能使用压缩列表。 只有同时满足下面两个条件时，才会使用压缩列表：哈希中元素数量小于 512 个；哈希中所有键值对的键和值字符串长度都小于 64 字节。 如果有一个条件不满足，则使用哈希表；且编码只可能由压缩列表转化为哈希表，反方向则不可能。 下图展示了 Redis 内层的哈希编码转换的特点： 集合 集合（set）与列表类似，都是用来保存多个字符串，但集合与列表有两点不同：集合中的元素是无序的，因此不能通过索引来操作元素；集合中的元素不能有重复。 一个集合中最多可以存储 2^32-1 个元素；除了支持常规的增删改查，Redis 还支持多个集合取交集、并集、差集。 内部编码 集合的内部编码可以是**整数集合（intset）或哈希表（hashtable）。 哈希表前面已经讲过，这里略过不提；需要注意的是，集合在使用哈希表时，值全部被置为 null。 整数集合的结构定义如下： 其中，encoding 代表 contents 中存储内容的类型，虽然 contents（存储集合中的元素）是 int8_t 类型。 但实际上其存储的值是 int16_t、int32_t 或 int64_t，具体的类型便是由 encoding 决定的，length 表示元素个数。 整数集合适用于集合所有元素都是整数且集合元素数量较小的时候，与哈希表相比，整数集合的优势在于集中存储，节省空间。 同时，虽然对于元素的操作复杂度也由 O(n) 变为了 O(1)，但由于集合数量较少，因此操作的时间并没有明显劣势。 编码转换 只有同时满足下面两个条件时，集合才会使用整数集合：集合中元素数量小于 512 个，集合中所有元素都是整数值。 如果有一个条件不满足，则使用哈希表；且编码只可能由整数集合转化为哈希表，反方向则不可能。 下图展示了集合编码转换的特点： 有序集合 有序集合与集合一样，元素都不能重复；但与集合不同的是，有序集合中的元素是有顺序的。 与列表使用索引下标作为排序依据不同，有序集合为每个元素设置一个分数（score）作为排序依据。 内部编码 有序集合的内部编码可以是压缩列表（ziplist）或跳跃表（skiplist）。ziplist 在列表和哈希中都有使用，前面已经讲过，这里略过不提。 跳跃表是一种有序数据结构，通过在每个节点中维持多个指向其他节点的指针，从而达到快速访问节点的目的。 除了跳跃表，实现有序数据结构的另一种典型实现是平衡树；大多数情况下，跳跃表的效率可以和平衡树媲美，且跳跃表实现比平衡树简单很多，因此 Redis 中选用跳跃表代替平衡树。 跳跃表支持平均 O(logN)、最坏 O(N) 的复杂点进行节点查找，并支持顺序操作。 Redis 的跳跃表实现由 zskiplist 和 zskiplistNode 两个结构组成：前者用于保存跳跃表信息（如头结点、尾节点、长度等），后者用于表示跳跃表节点，具体结构相对比较复杂。 编码转换 只有同时满足下面两个条件时，才会使用压缩列表：有序集合中元素数量小于 128 个；有序集合中所有成员长度都不足 64 字节。 如果有一个条件不满足，则使用跳跃表；且编码只可能由压缩列表转化为跳跃表，反方向则不可能。 下图展示了有序集合编码转换的特点： 应用举例了解 Redis 的内存模型之后，下面通过几个例子说明它的应用。 估算 Redis 内存使用量要估算 Redis 中的数据占据的内存大小，需要对 Redis 的内存模型有比较全面的了解，包括前面介绍的 hashtable、sds、redisobject、各种对象类型的编码方式等。 下面以最简单的字符串类型来进行说明。 假设有 90000 个键值对，每个 key 的长度是 7 个字节，每个 value 的长度也是 7 个字节（且 key 和 value 都不是整数），下面来估算这 90000 个键值对所占用的空间。 在估算占据空间之前，首先可以判定字符串类型使用的编码方式：embstr。 90000 个键值对占据的内存空间主要可以分为两部分： 90000 个 dictEntry 占据的空间。键值对所需要的 bucket 空间。每个 dictEntry 占据的空间包括： 一个 dictEntry，24 字节，jemalloc 会分配 32 字节的内存块。一个 key，7 字节，所以 SDS(key)需要 7+9=16 个字节，jemalloc 会分配 16 字节的内存块。一个 RedisObject，16 字节，jemalloc 会分配 16 字节的内存块。一个 value，7 字节，所以 SDS(value)需要 7+9=16 个字节，jemalloc 会分配 16 字节的内存块。综上，一个 dictEntry 需要 32+16+16+16=80 个字节。 bucket 空间：bucket 数组的大小为大于 90000 的最小的 2^n，是 131072；每个 bucket 元素为 8 字节（因为 64 位系统中指针大小为 8 字节）。 因此，可以估算出这 90000 个键值对占据的内存大小为：9000080 + 1310728 = 8248576。 下面写个程序在 Redis 中验证一下： 运行结果：8247552。 理论值与结果值误差在万分之 1.2，对于计算需要多少内存来说，这个精度已经足够了。 之所以会存在误差，是因为在我们插入 90000 条数据之前 Redis 已分配了一定的 bucket 空间，而这些 bucket 空间尚未使用。 作为对比将 key 和 value 的长度由 7 字节增加到 8 字节，则对应的 SDS 变为 17 个字节，jemalloc 会分配 32 个字节，因此每个 dictEntry 占用的字节数也由 80 字节变为 112 字节。 此时估算这 90000 个键值对占据内存大小为：90000112 + 1310728 = 11128576。 在Redis 中验证代码如下（只修改插入数据的代码）： 运行结果：8247552。 理论值与结果值误差在万分之 1.2，对于计算需要多少内存来说，这个精度已经足够了。 之所以会存在误差，是因为在我们插入 90000 条数据之前 Redis 已分配了一定的 bucket 空间，而这些 bucket 空间尚未使用。 作为对比将 key 和 value 的长度由 7 字节增加到 8 字节，则对应的 SDS 变为 17 个字节，jemalloc 会分配 32 个字节，因此每个 dictEntry 占用的字节数也由 80 字节变为 112 字节。 此时估算这 90000 个键值对占据内存大小为：90000112 + 1310728 = 11128576。 在Redis 中验证代码如下（只修改插入数据的代码）： 运行结果：11128576，估算准确。 对于字符串类型之外的其他类型，对内存占用的估算方法是类似的，需要结合具体类型的编码方式来确定。 优化内存占用了解 Redis 的内存模型，对优化 Redis 内存占用有很大帮助。下面介绍几种优化场景。 利用 jemalloc 特性进行优化 上一小节所讲述的 90000 个键值便是一个例子。由于 jemalloc 分配内存时数值是不连续的，因此 key/value 字符串变化一个字节，可能会引起占用内存很大的变动，在设计时可以利用这一点。 例如，如果 key 的长度是 8 个字节，则 SDS 为 17 字节，jemalloc 分配 32 字节。 此时将 key 长度缩减为 7 个字节，则 SDS 为 16 字节，jemalloc 分配 16 字节；则每个 key 所占用的空间都可以缩小一半。 使用整型/长整型 如果是整型/长整型，Redis 会使用 int 类型（8 字节）存储来代替字符串，可以节省更多空间。 因此在可以使用长整型/整型代替字符串的场景下，尽量使用长整型/整型。 共享对象 利用共享对象，可以减少对象的创建（同时减少了 RedisObject 的创建），节省内存空间。 目前 Redis 中的共享对象只包括 10000 个整数（0-9999）；可以通过调整 REDIS_SHARED_INTEGERS 参数提高共享对象的个数。 例如将 REDIS_SHARED_INTEGERS 调整到 20000，则 0-19999 之间的对象都可以共享。 考虑这样一种场景：论坛网站在 Redis 中存储了每个帖子的浏览数，而这些浏览数绝大多数分布在 0-20000 之间。 这时候通过适当增大 REDIS_SHARED_INTEGERS 参数，便可以利用共享对象节省内存空间。 避免过度设计 然而需要注意的是，不论是哪种优化场景，都要考虑内存空间与设计复杂度的权衡；而设计复杂度会影响到代码的复杂度、可维护性。 如果数据量较小，那么为了节省内存而使得代码的开发、维护变得更加困难并不划算；还是以前面讲到的 90000 个键值对为例，实际上节省的内存空间只有几 MB。 但是如果数据量有几千万甚至上亿，考虑内存的优化就比较必要了。 关注内存碎片率 内存碎片率是一个重要的参数，对 Redis 内存的优化有重要意义。 如果内存碎片率过高（jemalloc 在 1.03 左右比较正常），说明内存碎片多，内存浪费严重。 这时便可以考虑重启 Redis 服务，在内存中对数据进行重排，减少内存碎片。 如果内存碎片率小于 1，说明 Redis 内存不足，部分数据使用了虚拟内存（即 swap）。 由于虚拟内存的存取速度比物理内存差很多（2-3 个数量级），此时 Redis 的访问速度可能会变得很慢。 因此必须设法增大物理内存（可以增加服务器节点数量，或提高单机内存），或减少 Redis 中的数据。 要减少 Redis 中的数据，除了选用合适的数据类型、利用共享对象等，还有一点是要设置合理的数据回收策略（maxmemory-policy），当内存达到一定量后，根据不同的优先级对内存进行回收。]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[interrupt()到底意味着什么?]]></title>
    <url>%2F2018%2F04%2F11%2F2018-04-11-interrupt%2F</url>
    <content type="text"><![CDATA[什么是中断？在Thread类中有以下与interrupt相关的方法:12345678#将调用者线程的中断状态设为true。public void interrupt()#判断调用者线程的中断状态，不会改变当前中断状态public boolean isInterrupted()#判断调用者线程的中断状态，把当前状态设为falsepublic static boolean interrupted() 首先，一个线程不应该由其他线程来强制中断或停止，而是应该由线程自己自行停止。所以，Thread.stop, Thread.suspend, Thread.resume 都已经被废弃了。 而 Thread.interrupt 的作用其实也不是中断线程，而是「通知线程应该中断了」，具体到底中断还是继续运行，应该由被通知的线程自己处理。 具体来说，当对一个线程，调用 interrupt() 时: 如果线程处于被阻塞状态（例如处于sleep, wait, join 等状态），那么线程将立即退出被阻塞状态，并抛出一个InterruptedException异常。仅此而已。 如果线程处于正常活动状态，那么会将该线程的中断标志设置为 true，仅此而已。被设置中断标志的线程将继续正常运行，不受影响。 interrupt() 并不能真正的中断线程，需要被调用的线程自己进行配合才行。也就是说，一个线程如果有被中断的需求，那么就可以这样做。 在正常运行任务时，经常检查本线程的中断标志位，如果被设置了中断标志就自行停止线程。 在调用阻塞方法时正确处理InterruptedException异常。（例如，catch异常后就结束线程。） 对于正常运行的线程thread，对它调用 thread.interrupt()后，再调用 thread.interrupted()会返回true，同时把标志位清除，下次再调用 thread.interrupted() thread.interrupted()会清除标志位，并不是代表线程又恢复了，可以理解为仅仅是代表它已经响应完了这个中断信号然后又重新置为可以再次接收信号的状态。 如何使用中断？要使用中断，首先需要在可能会发生中断的线程中不断监听中断状态，一旦发生中断，就执行相应的中断处理代码。 当需要中断线程时，调用该线程对象的interrupt函数即可。 1.监听中断1234567891011Thread t1 = new Thread( new Runnable()&#123; public void run()&#123; // 若未发生中断，就正常执行任务 while(!Thread.currentThread.isInterrupted())&#123; // 正常任务代码…… &#125; // 中断的处理代码…… doSomething(); &#125;&#125; ).start(); 正常的任务代码被封装在while循环中，每次执行完一遍任务代码就检查一下中断状态；一旦发生中断，则跳过while循环，直接执行后面的中断处理代码。 2.触发中断1t1.interrupt(); 上述代码执行后会将t1对象的中断状态设为true，此时t1线程的正常任务代码执行完成后，进入下一次while循环前Thread.currentThread.isInterrupted()的结果为true，此时退出循环，执行循环后面的中断处理代码。 如何安全地停止线程？stop函数停止线程过于暴力，它会立即停止线程，不给任何资源释放的余地，下面介绍两种安全停止线程的方法。 1.自定义中断标记 自定义一个共享的boolean类型变量，表示当前线程是否需要中断。1234567891011121314151617181920#中断标识volatile boolean interrupted = false;#任务执行函数Thread t1 = new Thread( new Runnable()&#123; public void run()&#123; while(!interrupted)&#123; // 正常任务代码…… &#125; // 中断处理代码…… // 可以在这里进行资源的释放等操作…… &#125;&#125; );#中断函数Thread t2 = new Thread( new Runnable()&#123; public void run()&#123; interrupted = true; &#125;&#125; ); 2.使用Thread的中断机制t12345678910111213Thread t1 = new Thread( new Runnable()&#123; public void run()&#123; // 若未发生中断，就正常执行任务 while(!Thread.currentThread.isInterrupted())&#123; // 正常任务代码…… &#125; // 中断的处理代码…… doSomething(); &#125;&#125; ).start();t1.interrupt(); 上述两种方法本质一样，都是通过循环查看一个共享标记为来判断线程是否需要中断，他们的区别在于：第一种方法的标识位是我们自己设定的，而第二种方法的标识位是Java提供的。除此之外，他们的实现方法是一样的。上述两种方法之所以较为安全，是因为一条线程发出终止信号后，接收线程并不会立即停止，而是将本次循环的任务执行完，再跳出循环停止线程。此外，程序员又可以在跳出循环后添加额外的代码进行收尾工作。 如何处理中断？上文都在介绍如何获取中断状态，那么当我们捕获到中断状态后，究竟如何处理呢？ Java类库中提供的一些可能会发生阻塞的方法都会抛InterruptedException异常，如：BlockingQueue#put、BlockingQueue#take、Object#wait、Thread#sleep。 当你在某一条线程中调用这些方法时，这个方法可能会被阻塞很长时间，你可以在别的线程中调用当前线程对象的interrupt方法触发这些函数抛出InterruptedException异常。 当一个函数抛出InterruptedException异常时，表示这个方法阻塞的时间太久了，别人不想等它执行结束了。 当你的捕获到一个InterruptedException异常后，亦可以处理它，或者向上抛出。 抛出时要注意：当你捕获到InterruptedException异常后，当前线程的中断状态已经被修改为false(表示线程未被中断)；此时你若能够处理中断，则不用理会该值；但如果你继续向上抛InterruptedException异常，你需要再次调用interrupt方法，将当前线程的中断状态设为true。 注意：绝对不能“吞掉中断”！即捕获了InterruptedException而不作任何处理。这样违背了中断机制的规则，别人想让你线程中断，然而你自己不处理，也不将中断请求告诉调用者，调用者一直以为没有中断请求。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[一文让你明白Java字节码]]></title>
    <url>%2F2018%2F04%2F01%2F2017-04-01-bytecode%2F</url>
    <content type="text"><![CDATA[原文链接: http://www.importnew.com/24088.html 也许你写了无数行的代码，也许你能非常溜的使用高级语言，但是你未必了解那些高级语言的执行过程。例如大行其道的Java。 Java号称是一门“一次编译到处运行”的语言，但是我们对这句话的理解深度又有多少呢？从我们写的java文件到通过编译器编译成java字节码文件（也就是.class文件），这个过程是java编译过程；而我们的java虚拟机执行的就是字节码文件。不论该字节码文件来自何方，由哪种编译器编译，甚至是手写字节码文件，只要符合java虚拟机的规范，那么它就能够执行该字节码文件。那么本文主要讲讲java字节码文件相关知识。接下来我们通过具体的Demo来深入理解： 1. 首先我们来写一个java源文件 上面是我们写的一个java程序，很简单，只有一个成员变量a以及一个方法testMethod()。 2. 接下来我们用javac命令或者ide工具将该java源文件编译成java字节码文件。 上图是编译好的字节码文件，我们可以看到一堆16进制的字节。如果你使用IDE去打开，也许看到的是已经被反编译的我们所熟悉的java代码，而这才是纯正的字节码，这也是我们今天需要讲的内容重点。 也许你会对这样一堆字节码感到头疼，不过没关系，我们慢慢试着你看懂它，或许有不一样的收获。在开始之前我们先来看一张图。 3.1 魔数从上面的总览图中我们知道前4个字节表示的是魔数，对应我们Demo的是 0XCAFE BABE。什么是魔数？魔数是用来区分文件类型的一种标志，一般都是用文件的前几个字节来表示。比如0XCAFE BABE表示的是class文件，那么有人会问，文件类型可以通过文件名后缀来判断啊？是的，但是文件名是可以修改的（包括后缀），那么为了保证文件的安全性，将文件类型写在文件内部来保证不被篡改。从java的字节码文件类型我们看到，CAFE BABE翻译过来是咖啡宝贝之意，然后再看看java图标。 3.2 版本号我们识别了文件类型之后，接下来要知道版本号。版本号含主版本号和次版本号，都是各占2个字节。在此Demo种为0X0000 0033。其中前面的0000是次版本号，后面的0033是主版本号。通过进制转换得到的是次版本号为0，主版本号为51。从oracle官方网站我们能够知道，51对应的正式jdk1.7，而其次版本为0，所以该文件的版本为1.7.0。如果需要验证，可以在用java –version命令输出版本号，或者修改编译目标版本–target重新编译，查看编译后的字节码文件版本号是否做了相应的修改。 至此，我们共了解了前8字节的含义，下面讲讲常量池相关内容。 3.3 常量池紧接着主版本号之后的就是常量池入口。常量池是Class文件中的资源仓库，在接下来的内容中我们会发现很多地方会涉及，如Class Name，Interfaces等。常量池中主要存储2大类常量：字面量和符号引用。字面量如文本字符串，java中声明为final的常量值等等，而符号引用包括类和接口的全局限定名，字段的名称和描述符，方法的名称和描述符。 为什么需要类和接口的全局限定名呢？系统引用类或者接口的时候不是通过内存地址进行操作吗？这里大家仔细想想，java虚拟机在没有将类加载到内存的时候根本都没有分配内存地址，也就不存在对内存的操作，所以java虚拟机首先需要将类加载到虚拟机中，那么这个过程设计对类的定位（需要加载A包下的B类，不能加载到别的包下面的别的类中），所以需要通过全局限定名来判别唯一性。这就是为什么叫做全局，限定的意思，也就是唯一性。 在进行具体常量池分析之前，我们先来了解一下常量池的项目类型表： 上面的表中描述了11中数据类型的结构，其实在jdk1.7之后又增加了3种（CONSTANT_MethodHandle_info,CONSTANT_MethodType_info以及CONSTANT_InvokeDynamic_info)。这样算起来一共是14种。接下来我们按照Demo的字节码进行逐一翻译。 0×0015：由于常量池的数量不固定（n+2），所以需要在常量池的入口处放置一项u2类型的数据代表常量池数量。因此该16进制是21，表示有20项常量，索引范围为1~20。明明是21，为何是20呢？因为Class文件格式规定，设计者就讲第0项保留出来了，以备后患。从这里我们知道接下来我们需要翻译出20项常量。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697Constant #1 （一共有20个常量，这是第一个，以此类推…）0x0a-：从常量类型表中我们发现，第一个数据均是u1类型的tag，16进制的0a是十进制的10，对应表中的MethodRef_info。0x-00 04-：Class_info索引项#40x-00 11-：NameAndType索引项#17Constant #20x-09: FieldRef_info0×0003 :Class_info索引项#30×0012：NameAndType索引项#18Constant #30×07-: Class_info0x-00 13-: 全局限定名常量索引为#19Constant #40x-07 :Class_info0×0014:全局限定名常量索引为#20Constant #50×01:Utf-8_info0x-00 01-:字符串长度为1（选择接下来的一个字节长度转义）0x-61:”a”(十六进制转ASCII字符)Constant #60×01:Utf-8_info0x-00 01：字符串长度为10x-49:”I”Constant #70×01:Utf-8_info0x-00 06:字符串长度为60x-3c 696e 6974 3e-:”&lt;init&gt;”Constant #80×01 :UTF-8_info0×0003:字符串长度为30×2829 56:”()V”Constant #90x-01:Utf-8_info0×0004：字符串长度为40x436f 6465:”Code”Constant #100×01:Utf-8_info0×00 0f:字符串长度为150x4c 696e 654e 756d 6265 7254 6162 6c65:”LineNumberTable”Constant #11ox01: Utf-8_info0×00 12字符串长度为180x-4c 6f63 616c 5661 7269 6162 6c65 5461 626c 65:”LocalVariableTable”Constant #120×01:Utf-8_info0×0004 字符串长度为40×7468 6973 :”this”Constant #130×01:Utf-8_info0x0f:字符串长度为150x4c 636f 6d2f 6465 6d6f 2f44 656d 6f3b:”Lcom/demo/Demo;”Constant #140×01:Utf-8_info0×00 0a:字符串长度为10ox74 6573 744d 6574 686f 64:”testMethod”Constant #150×01:Utf-8_info0x000a:字符串长度为100x536f 7572 6365 4669 6c65 :”SourceFile”Constant #160×01:Utf-8_info0×0009:字符串长度为90x-44 656d 6f2e 6a61 7661 :”Demo.java”Constant #170x0c :NameAndType_info0×0007:字段或者名字名称常量项索引#70×0008:字段或者方法描述符常量索引#8Constant #180x0c:NameAndType_info0×0005:字段或者名字名称常量项索引#50×0006:字段或者方法描述符常量索引#6Constant #190×01:Utf-8_info0×00 0d:字符串长度为130×63 6f6d 2f64 656d 6f2f 4465 6d6f:”com/demo/Demo”Constant #200×01:Utf-8_info0×00 10 :字符串长度为160x6a 6176 612f 6c61 6e67 2f4f 626a 6563 74 :”java/lang/Object” 到这里为止我们解析了所有的常量。接下来是解析访问标志位。 3.4 Access_Flag 访问标志访问标志信息包括该Class文件是类还是接口，是否被定义成public，是否是abstract，如果是类，是否被声明成final。通过上面的源代码，我们知道该文件是类并且是public。 0x 00 21：是0×0020和0×0001的并集。其中0×0020这个标志值涉及到了字节码指令，后期会有专题对字节码指令进行讲解。期待中…… 3.5 类索引类索引用于确定类的全限定名0×00 03 表示引用第3个常量，同时第3个常量引用第19个常量，查找得”com/demo/Demo”。#3.#19 3.6父类索引0×00 04 同理：#4.#20(java/lang/Object) 3.7 接口索引通过java_byte图我们知道，这个接口有2+n个字节，前两个字节表示的是接口数量，后面跟着就是接口的表。我们这个类没有任何接口，所以应该是0000。果不其然，查找字节码文件得到的就是0000。 3.8 字段表集合字段表用于描述类和接口中声明的变量。这里的字段包含了类级别变量以及实例变量，但是不包括方法内部声明的局部变量。同样，接下来就是2+n个字段属性。我们只有一个属性a，按道理应该是0001。查找文件果不其然是0001。那么接下来我们要针对这样的字段进行解析。附上字段表结构图 0×00 02 ：访问标志为private（自行搜索字段访问标志）0×00 05 : 字段名称索引为#5，对应的是”a”0x 00 06 :描述符索引为#6，对应的是”I”0x 00 00 :属性表数量为0，因此没有属性表。tips:一些不太重要的表（字段，方法访问标志表）可以自行搜索，这里就不贴出来了，防止篇幅过大。 3.9 方法我们只有一个方法testMethod，按照道理应该前2个字节是0001。通过查找发现是0×00 02。这是什么原因，这代表着有2个方法呢？且继续看…… 第1个方法：12340×00 01：访问标志 ACC_PUBLIC，表明该方法是public。（可自行搜索方法访问标志表）0×00 07：方法名索引为#7，对应的是”&lt;init&gt;”0×00 08：方法描述符索引为#8，对应的是”()V”0×00 01：属性表数量为1(一个属性表) 那么这里涉及到了属性表。什么是属性表呢？可以这么理解，它是为了描述一些专有信息的，上面的方法带有一张属性表。所有属性表的结构如下图： 一个u2的属性名称索引，一个u2的属性长度加上属性长度的info。虚拟机规范预定义的属性有很多，比如Code，LineNumberTable，LocalVariableTable，SourceFile等等，这个网上可以搜索到。 按照上面的表结构解析得到下面信息：120×0009:名称索引为#9(“Code”)。0×000 00038：属性长度为56字节。 那么接下来解析一个Code属性表，按照下图解析 前面6个字节（名称索引2字节+属性长度4字节）已经解析过了，所以接下来就是解析剩下的56-6=50字节即可。12345678910111213140×00 02 ：max_stack=20×00 01 : max_locals=10×00 0000 0a : code_length=100x2a b700 012a 04b5 0002 b1 : 这是code代码，可以通过虚拟机字节码指令进行查找。2a=aload_0(将第一个引用变量推送到栈顶)b7=invokespecial(调用父类构造方法)00=什么都不做01 =将null推送到栈顶2a=同上04=iconst_1 将int型1推送到栈顶b5=putfield 为指定的类的实例变量赋值00= 同上02=iconst_m1 将int型-1推送栈顶b1=return 从当前方法返回void 整理，去除无动作指令得到下面1234560 : aload_01 : invokespecial4 : aload_05 : iconst_16 : putfield9 : return 关于虚拟机字节码指令这块内容，后期会继续深入下去…… 目前只需要了解即可。接下来顺着Code属性表继续解析下去:1230×00 00 : exception_table_length=00×00 02 : attributes_count=2(Code属性表内部还含有2个属性表)0×00 0a: 第一个属性表是”LineNumberTable” 120×00 0000 0a : “属性长度为10″0×00 02 ：line_number_table_length=2 line_number_table是一个数量为line_number_table_length，类型为line_number_info的集合，line_number_info表包括了start_pc和line_number两个u2类型的数据项，前者是字节码行号，后者是Java源码行号12340×00 00 : start_pc =00×00 03 : end_pc =30×00 04 : start_pc=40×00 04 : end_pc=4 0×00 0b 第二个属性表是：”LocalVariableTable” 123456780×00 0000 0c：属性长度为120×00 01 : local_variable_table_length=1然后按照local_variable_info表结构进行解析：0×00 00 : start_pc=00×00 0a：length=100x000c : name_index=”this”0x000d : descriptor_index #13 (“Lcom/demo/Demo”)0000 index=0 ——-到这里第一个方法就解析完成了——- Method(init) 有1个属性Code表 和 2个属性表（LineNumberTable ，LocalVariableTable）接下来解析第二个方法。 第2个方法：123456789101112131415161718192021222324252627282930310×00 04：”protected”0×00 0e: #14（”testMethod”）0×00 08 : “()V”0×0001 ： 属性数量=10×0009 ：”Code”0×0000 002b 属性长度为43//解析一个Code表0000 :max_stack =00001 : max_local =10000 0001 : code_length =10xb1 : return(该方法返回void)0×0000 异常表长度=00×0002 属性表长度为2//第一个属性表0x000a : #10，LineNumberTable0×0000 0006 : 属性长度为60×0001 : line_number_length = 10×0000 : start_pc =00×0008 : end_pc =8//第二个属性表0x000b : #11 ，LocalVariableTable0×0000 000c : 属性长度为120×0001 : local_variable_table_length =10×0000 :start_pc = 00×0001: length = 10x000c : name_index =#12 “this”0x000d : 描述索引#13 “Lcom/demo/Demo;”0000 index=0 到这里为止，方法解析都完成了，回过头看看顶部解析顺序图，我们接下来就要解析Attributes了。 3.10 Attribute12340×0001 ：同样的，表示有1个Attributes了。0x000f : #15(“SourceFile”)0×0000 0002 attribute_length=20×0010 : sourcefile_index = #16(“Demo.java”) SourceFile属性用来记录生成该Class文件的源码文件名称。 4. 另话其实，我们写了这么多确实很麻烦，不过这种过程自己体验一遍的所获所得还是不同的。现在，使用java自带的反编译器来解析字节码文件。1javap -verbose Demo //不用带后缀.class 5. 总结到此为止，讲解完成了class文件的解析，这样以后我们也能看懂字节码文件了。了解class文件的结构对后面进一步了解虚拟机执行引擎非常重要，所以这是基础并重要的一步。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>字节码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从Java源代码到字节码]]></title>
    <url>%2F2018%2F03%2F23%2F2018-03-23-javacode-to-bytecode%2F</url>
    <content type="text"><![CDATA[我们处理问题的时候，有时候仅关注Java源代码是不足够的，因为源码在编译过程中会加入各种特殊处理(重排序等)，因此，了解字节码的相关知识，能帮助我们更好地了解程序执行的实际过程和原理。 本文解释了如何将Java代码编译为字节码并在JVM上执行,要理解JVM中的内部体系结构以及字节码执行过程中使用的不同内存区域，请参看 JVM内幕。 本文分成三个主要部分，每部分将介绍不同的Java代码结构，并解释如何将这些代码编译为字节码并执行。每部分附上代码示例，以及编译生成的字节码。 字节码中每条指令（或操作码）之前的数字表示字节位置。例如，像1：iconst_1因为没有操作数,这样的指令只有一个字节的长度,所以接下来的字节位置将是2。又例如，1：bipush 5的指令需要两个字节，操作码bipush占一个字节（位置1），操作数5占一个字节（位置2），下一个字节码位置将为3。 基本编程概念变量局部变量 Java虚拟机（JVM）具有基于堆栈的体系结构。当包括初始主方法在内的每个方法被执行时，在具有一组局部变量的堆栈上创建一个栈帧。方法的局部变量数组包含执行该方法期间使用的所有变量，包括当前对象的this引用，方法参数以及本地定义的变量。对于静态方法，局部变量数组从[0]开始保存方法参数，对于实例方法，[0]用来保存this引用，然后才是方法参数。 局部变量可以是： boolean byte char long short int float double reference returnAddress 所有类型都在局部变量数组中占用一个位置(slot)，除了long和double，它们都占用两个连续的位置（(slot)），因为这些类型是双倍宽度（64位而不是32位）。 当一个新变量被创建时，它的值首先添加到操作数堆栈（operand stack），然后再添加到局部变量数组中。如果变量不是原始类型( primitive value)，那么局部变量数组槽只存储一个引用，引用指向存储在堆中的对象。 例如：1int i = 10; 编译后字节码:120: bipush 102: istore_0 bipush用于将一个字节作为整数添加到操作数堆栈中，例子中把10添加到操作数堆栈中。 istore0格式为``istore的一组操作码之一，它们都将整数存储到局部变量数组中。&lt;n&gt;表示存储在局部变量数组中的位置，只能是0,1,2或3。当出现大于3的情况时(超过3个局部变量)，直接使用操作码istore``,它需要操作数来 操作在内存中执行情况： 类文件包含每个方法的局部变量表，例如类中的方法包含上面的变量赋值，那么在类文件的局部变量表如下：123LocalVariableTable: Start Length Slot Name Signature 0 1 1 i I 实例变量（静态变量） 实例变量（或静态变量）作为实例（或Class）的一部分存储在堆上。有关该字段的信息将添加到类文件的field_info数组中，如下所示。123456789101112131415161718ClassFile &#123; u4 magic; u2 minor_version; u2 major_version; u2 constant_pool_count; cp_info contant_pool[constant_pool_count – 1]; u2 access_flags; u2 this_class; u2 super_class; u2 interfaces_count; u2 interfaces[interfaces_count]; u2 fields_count; field_info fields[fields_count]; u2 methods_count; method_info methods[methods_count]; u2 attributes_count; attribute_info attributes[attributes_count];&#125; 另外，如果初始化变量，则将执行初始化的字节码添加到构造函数中。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>java语言</tag>
        <tag>字节码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat 中的 NIO 源码分析]]></title>
    <url>%2F2018%2F03%2F20%2F2018-03-20-tomcat-nio%2F</url>
    <content type="text"><![CDATA[之前写了两篇关于 NIO 的文章，第一篇介绍了 NIO 的 Channel、Buffer、Selector 使用，第二篇介绍了非阻塞 IO 和异步 IO，并展示了简单的用例。 本文将介绍 Tomcat 中的 NIO 使用，使大家对 Java NIO 的生产使用有更加直观的认识。 虽然本文的源码篇幅也不短，但是 Tomcat 的源码毕竟不像 Doug Lea 的并发源码那么“变态”，对于大部分读者来说，阅读难度比之前介绍的其他并发源码要简单一些，所以读者不要觉得有什么压力。 本文基于 Tomcat 当前最新版本9.0.6。 先简单画一张图示意一下本文的主要内容： 源码环境准备Tomcat 9.0.6 下载地址：https://tomcat.apache.org/download-90.cgi 由于上面下载的 tomcat 的源码并没有使用 maven 进行组织，不方便我们看源码，也不方便我们进行调试。这里我们将使用 maven 仓库中的 tomcat-embed-core，自己编写代码进行启动的方式来进行调试。 首先，创建一个空的 maven 工程，然后添加以下依赖。12345&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-core&lt;/artifactId&gt; &lt;version&gt;9.0.6&lt;/version&gt;&lt;/dependency&gt; 上面的依赖，只会将 tomcat-embed-core-9.0.6.jar 和 tomcat-annotations-api-9.0.6.jar 两个包引进来，对于本文来说，已经足够了，如果你需要其他功能，需要额外引用其他的依赖，如 Jasper。 然后，使用以下启动方法：1234567891011public static void main(String[] args) throws LifecycleException &#123; Tomcat tomcat = new Tomcat(); Connector connector = new Connector("HTTP/1.1"); connector.setPort(8080); tomcat.setConnector(connector); tomcat.start(); tomcat.getServer().await();&#125; 经过以上的代码，我们的 Tomcat 就启动起来了。 Tomcat 中的其他接口感兴趣的读者请自行探索，如设置 webapp 目录，设置 resources 等 这里，介绍第一个重要的概念：Connector。在 Tomcat 中，使用 Connector 来处理连接，一个 Tomcat 可以配置多个 Connector，分别用于监听不同端口，或处理不同协议。 在 Connector 的构造方法中，我们可以传HTTP/1.1 或 AJP/1.3 用于指定协议，也可以传入相应的协议处理类，毕竟协议不是重点，将不同端口进来的连接对应不同处理类才是正道。典型地，我们可以指定以下几个协议处理类： org.apache.coyote.http11.Http11NioProtocol：对应非阻塞 IO org.apache.coyote.http11.Http11Nio2Protocol：对应异步 IO org.apache.coyote.http2.Http2Protocol：对应 http2 协议，对 http2 感兴趣的读者，赶紧看起来吧。 本文的重点当然是非阻塞 IO 了，之前已经介绍过异步 IO的基础知识了，读者看完本文后，如果对异步 IO 的处理流程感兴趣，可以自行去分析一遍。 如果你使用 9.0 以前的版本，Tomcat 在启动的时候是会自动配置一个 connector 的，我们可以不用显式配置。 9.0 版本的 Tomcat#start() 方法：1234public void start() throws LifecycleException &#123; getServer(); server.start();&#125; 8.5 及之前版本的 Tomcat#start() 方法：123456public void start() throws LifecycleException &#123; getServer(); // 自动配置一个使用非阻塞 IO 的 connector getConnector(); server.start();&#125; endpoint前面我们说过一个Connector对应一个协议，当然这描述也不太对，NIO 和 NIO2 就都是处理HTTP/1.1的，只不过一个使用非阻塞，一个使用异步。进到指定protocol代码，我们就会发现，它们的代码及其简单，只不过是指定了特定的endpoint。 打开Http11NioProtocol和Http11Nio2Protocol源码，我们可以看到，在构造方法中，它们分别指定了 NioEndpoint 和 Nio2Endpoint。1234567891011121314151617// 非阻塞模式public class Http11NioProtocol extends AbstractHttp11JsseProtocol&lt;NioChannel&gt; &#123; public Http11NioProtocol() &#123; // NioEndpoint super(new NioEndpoint()); &#125; ...&#125;// 异步模式public class Http11Nio2Protocol extends AbstractHttp11JsseProtocol&lt;Nio2Channel&gt; &#123; public Http11Nio2Protocol() &#123; // Nio2Endpoint super(new Nio2Endpoint()); &#125; ...&#125; 这里介绍第二个重要的概念：endpoint。Tomcat 使用不同的 endpoint 来处理不同的协议请求，今天我们的重点是NioEndpoint，其使用非阻塞 IO来进行处理HTTP/1.1协议的请求。 NioEndpoint 继承 =&gt; AbstractJsseEndpoint继承 =&gt; AbstractEndpoint。中间的 AbstractJsseEndpoint 主要是提供了一些关于HTTPS的方法，这块我们暂时忽略它，后面所有关于 HTTPS 的我们都直接忽略，感兴趣的读者请自行分析。 init 过程分析下面，我们看看从 tomcat.start() 一直到 NioEndpoint 的过程。 AbstractProtocol # init 123456789@Overridepublic void init() throws Exception &#123; ... String endpointName = getName(); endpoint.setName(endpointName.substring(1, endpointName.length()-1)); endpoint.setDomain(domain); // endpoint 的 name=http-nio-8089,domain=Tomcat endpoint.init();&#125; AbstractEndpoint # init 1234567public final void init() throws Exception &#123; if (bindOnInit) &#123; bind(); // 这里对应的当然是子类 NioEndpoint 的 bind() 方法 bindState = BindState.BOUND_ON_INIT; &#125; ...&#125; NioEndpoint # bind 这里就到我们的 NioEndpoint 了，要使用到我们之前学习的 NIO 的知识了。 1234567891011121314151617181920212223242526272829303132333435363738394041@Overridepublic void bind() throws Exception &#123; // initServerSocket(); 原代码是这行，我们 “内联” 过来一起说 // 开启 ServerSocketChannel serverSock = ServerSocketChannel.open(); socketProperties.setProperties(serverSock.socket()); // getPort() 会返回我们最开始设置的 8080，得到我们的 address 是 0.0.0.0:8080 InetSocketAddress addr = (getAddress()!=null?new InetSocketAddress(getAddress(),getPort()):new InetSocketAddress(getPort())); // ServerSocketChannel 绑定地址、端口， // 第二个参数 backlog 默认为 100，超过 100 的时候，新连接会被拒绝(不过源码注释也说了，这个值的真实语义取决于具体实现) serverSock.socket().bind(addr,getAcceptCount()); // ※※※ 设置 ServerSocketChannel 为阻塞模式 ※※※ serverSock.configureBlocking(true); // 设置 acceptor 和 poller 的数量，至于它们是什么角色，待会说 // acceptorThreadCount 默认为 1 if (acceptorThreadCount == 0) &#123; // FIXME: Doesn't seem to work that well with multiple accept threads // 作者想表达的意思应该是：使用多个 acceptor 线程并不见得性能会更好 acceptorThreadCount = 1; &#125; // poller 线程数，默认值定义如下，所以在多核模式下，默认为 2 // pollerThreadCount = Math.min(2,Runtime.getRuntime().availableProcessors()); if (pollerThreadCount &lt;= 0) &#123; pollerThreadCount = 1; &#125; // setStopLatch(new CountDownLatch(pollerThreadCount)); // 初始化 ssl，我们忽略 ssl initialiseSsl(); // 打开 NioSelectorPool，先忽略它 selectorPool.open();&#125; (1) ServerSocketChannel 已经打开，并且绑定要了之前指定的 8080 端口，设置成了阻塞模式。(2) 设置了 acceptor 的线程数为 1(3) 设置了 poller 的线程数，单核 CPU 为 1，多核为 2(4) 打开了一个 SelectorPool，我们先忽略这个 到这里，我们还不知道 Acceptor 和 Poller 是什么东西，我们只是设置了它们的数量，我们先来看看最后面提到的SelectorPool。 start 过程分析刚刚我们分析完了 init() 过程，下面是启动过程 start() 分析。 AbstractProtocol # start1234567891011121314151617@Overridepublic void start() throws Exception &#123; ... // 调用 endpoint 的 start 方法 endpoint.start(); // Start async timeout thread asyncTimeout = new AsyncTimeout(); Thread timeoutThread = new Thread(asyncTimeout, getNameInternal() + "-AsyncTimeout"); int priority = endpoint.getThreadPriority(); if (priority &lt; Thread.MIN_PRIORITY || priority &gt; Thread.MAX_PRIORITY) &#123; priority = Thread.NORM_PRIORITY; &#125; timeoutThread.setPriority(priority); timeoutThread.setDaemon(true); timeoutThread.start();&#125; AbstractEndpoint # start12345678910public final void start() throws Exception &#123; // 按照我们的流程，刚刚 init 的时候，已经把 bindState 改为 BindState.BOUND_ON_INIT 了， // 所以下面的 if 分支我们就不进去了 if (bindState == BindState.UNBOUND) &#123; bind(); bindState = BindState.BOUND_ON_START; &#125; // 往里看 NioEndpoint 的实现 startInternal();&#125; 下面这个方法还是比较重要的，这里会创建前面说过的 acceptor 和 poller。 NioEndpoint # startInternal1234567891011121314151617181920212223242526272829303132333435363738394041@Overridepublic void startInternal() throws Exception &#123; if (!running) &#123; running = true; paused = false; // 以下几个是缓存用的，之后我们也会看到很多这样的代码，为了减少 new 很多对象出来 processorCache = new SynchronizedStack&lt;&gt;(SynchronizedStack.DEFAULT_SIZE, socketProperties.getProcessorCache()); eventCache = new SynchronizedStack&lt;&gt;(SynchronizedStack.DEFAULT_SIZE, socketProperties.getEventCache()); nioChannels = new SynchronizedStack&lt;&gt;(SynchronizedStack.DEFAULT_SIZE, socketProperties.getBufferPool()); // 创建【工作线程池】，Tomcat 自己包装了一下 ThreadPoolExecutor， // 1. 为了在创建线程池以后，先启动 corePoolSize 个线程(这个属于线程池的知识了，不熟悉的读者可以看看我之前的文章) // 2. 自己管理线程池的增长方式（默认 corePoolSize 10, maxPoolSize 200），不是本文重点，不分析 if ( getExecutor() == null ) &#123; createExecutor(); &#125; // 设置一个栅栏（tomcat 自定义了类 LimitLatch），控制最大的连接数，默认是 10000 initializeConnectionLatch(); // 开启 poller 线程 // 还记得之前 init 的时候，默认地设置了 poller 的数量为 2，所以这里启动 2 个 poller 线程 pollers = new Poller[getPollerThreadCount()]; for (int i=0; i&lt;pollers.length; i++) &#123; pollers[i] = new Poller(); Thread pollerThread = new Thread(pollers[i], getName() + "-ClientPoller-"+i); pollerThread.setPriority(threadPriority); pollerThread.setDaemon(true); pollerThread.start(); &#125; // 开启 acceptor 线程，和开启 poller 线程组差不多。 // init 的时候，默认地，acceptor 的线程数是 1 startAcceptorThreads(); &#125;&#125; 到这里，我们启动了工作线程池、 poller 线程组、acceptor 线程组。同时，工作线程池初始就已经启动了 10 个线程。我们用 jconsole 来看看此时的线程，请看下图： !()[http://governlab.cn/images/tomcat-nio-1.png] 从 jconsole 中，我们可以看到，此时启动了BlockPoller、worker、poller、acceptor、AsyncTimeout，大家应该都已经清楚了每个线程是哪里启动的吧。 Tomcat 中并没有 Worker 这个类，此名字是我瞎编。 此时，我们还是不知道 acceptor、poller 甚至 worker 到底是干嘛的，下面，我们从 acceptor 线程开始看起。 Acceptor它的结构非常简单，在构造函数中，已经把 endpoint 传进来了，此外就只有 threadName 和 state 两个简单的属性。1234567private final AbstractEndpoint&lt;?,U&gt; endpoint;private String threadName;protected volatile AcceptorState state = AcceptorState.NEW;public Acceptor(AbstractEndpoint&lt;?,U&gt; endpoint) &#123; this.endpoint = endpoint;&#125; threadName就是一个线程名字而已，Acceptor 的状态state主要是随着 endpoint 来的。123public enum AcceptorState &#123; NEW, RUNNING, PAUSED, ENDED&#125; 我们直接来看 acceptor 的 run 方法吧： Acceptor # run12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485@Overridepublic void run() &#123; int errorDelay = 0; // 只要 endpoint 处于 running，这里就一直循环 while (endpoint.isRunning()) &#123; // 如果 endpoint 处于 pause 状态，这边 Acceptor 用一个 while 循环将自己也挂起 while (endpoint.isPaused() &amp;&amp; endpoint.isRunning()) &#123; state = AcceptorState.PAUSED; try &#123; Thread.sleep(50); &#125; catch (InterruptedException e) &#123; // Ignore &#125; &#125; // endpoint 结束了，Acceptor 自然也要结束嘛 if (!endpoint.isRunning()) &#123; break; &#125; state = AcceptorState.RUNNING; try &#123; // 如果此时达到了最大连接数(之前我们说过，默认是10000)，就等待 endpoint.countUpOrAwaitConnection(); // Endpoint might have been paused while waiting for latch // If that is the case, don't accept new connections if (endpoint.isPaused()) &#123; continue; &#125; U socket = null; try &#123; // 这里就是接收下一个进来的 SocketChannel // 之前我们设置了 ServerSocketChannel 为阻塞模式，所以这边的 accept 是阻塞的 socket = endpoint.serverSocketAccept(); &#125; catch (Exception ioe) &#123; // We didn't get a socket endpoint.countDownConnection(); if (endpoint.isRunning()) &#123; // Introduce delay if necessary errorDelay = handleExceptionWithDelay(errorDelay); // re-throw throw ioe; &#125; else &#123; break; &#125; &#125; // accept 成功，将 errorDelay 设置为 0 errorDelay = 0; if (endpoint.isRunning() &amp;&amp; !endpoint.isPaused()) &#123; // setSocketOptions() 是这里的关键方法，也就是说前面千辛万苦都是为了能到这里进行处理 if (!endpoint.setSocketOptions(socket)) &#123; // 如果上面的方法返回 false，关闭 SocketChannel endpoint.closeSocket(socket); &#125; &#125; else &#123; // 由于 endpoint 不 running 了，或者处于 pause 了，将此 SocketChannel 关闭 endpoint.destroySocket(socket); &#125; &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); String msg = sm.getString("endpoint.accept.fail"); // APR specific. // Could push this down but not sure it is worth the trouble. if (t instanceof Error) &#123; Error e = (Error) t; if (e.getError() == 233) &#123; // Not an error on HP-UX so log as a warning // so it can be filtered out on that platform // See bug 50273 log.warn(msg, t); &#125; else &#123; log.error(msg, t); &#125; &#125; else &#123; log.error(msg, t); &#125; &#125; &#125; state = AcceptorState.ENDED;&#125; 大家应该发现了，Acceptor 绕来绕去，都是在调用 NioEndpoint 的方法，我们简单分析一下这个。 在 NioEndpoint init 的时候，我们开启了一个 ServerSocketChannel，后来 start 的时候，我们开启多个 acceptor（实际上，默认是 1 个），每个 acceptor 启动以后就开始循环调用 ServerSocketChannel 的 accept() 方法获取新的连接，然后调用 endpoint.setSocketOptions(socket) 处理新的连接，之后再进入循环 accept 下一个连接。 到这里，大家应该也就知道了，为什么这个叫 acceptor 了吧？接下来，我们来看看 setSocketOptions 方法到底做了什么。 NioEndpoint # setSocketOptions123456789101112131415161718192021222324252627282930313233343536373839404142@Overrideprotected boolean setSocketOptions(SocketChannel socket) &#123; try &#123; // 设置该 SocketChannel 为非阻塞模式 socket.configureBlocking(false); Socket sock = socket.socket(); // 设置 socket 的一些属性 socketProperties.setProperties(sock); // 还记得 startInternal 的时候，说过了 nioChannels 是缓存用的。 // 限于篇幅，这里的 NioChannel 就不展开了，它包括了 socket 和 buffer NioChannel channel = nioChannels.pop(); if (channel == null) &#123; // 主要是创建读和写的两个 buffer，默认地，读和写 buffer 都是 8192 字节，8k SocketBufferHandler bufhandler = new SocketBufferHandler( socketProperties.getAppReadBufSize(), socketProperties.getAppWriteBufSize(), socketProperties.getDirectBuffer()); if (isSSLEnabled()) &#123; channel = new SecureNioChannel(socket, bufhandler, selectorPool, this); &#125; else &#123; channel = new NioChannel(socket, bufhandler); &#125; &#125; else &#123; channel.setIOChannel(socket); channel.reset(); &#125; // getPoller0() 会选取所有 poller 中的一个 poller getPoller0().register(channel); &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); try &#123; log.error("",t); &#125; catch (Throwable tt) &#123; ExceptionUtils.handleThrowable(tt); &#125; // Tell to close the socket return false; &#125; return true;&#125; 我们看到，这里又没有进行实际的处理，而是将这个 SocketChannel 注册到了其中一个 poller 上。因为我们知道，acceptor 应该尽可能的简单，只做 accept 的工作，简单处理下就往后面扔。acceptor 还得回到之前的循环去 accept 新的连接呢。 我们只需要明白，此时，往 poller 中注册了一个NioChannel实例，此实例包含客户端过来的 SocketChannel 和一个SocketBufferHandler实例。 Poller之前我们看到 acceptor 将一个 NioChannel 实例 register 到了一个 poller 中。在看 register 方法之前，我们需要先对 poller 要有个简单的认识。123456789101112131415161718192021public class Poller implements Runnable &#123; public Poller() throws IOException &#123; // 每个 poller 开启一个 Selector this.selector = Selector.open(); &#125; private Selector selector; // events 队列，此类的核心 private final SynchronizedQueue&lt;PollerEvent&gt; events = new SynchronizedQueue&lt;&gt;(); private volatile boolean close = false; private long nextExpiration = 0;//optimize expiration handling // 这个值后面有用，记住它的初始值为 0 private AtomicLong wakeupCounter = new AtomicLong(0); private volatile int keyCount = 0; ...&#125; 敲重点：每个 poller 关联了一个 Selector。 Poller 内部围着一个 events 队列转，来看看其 events() 方法：1234567891011121314151617181920public boolean events() &#123; boolean result = false; PollerEvent pe = null; for (int i = 0, size = events.size(); i &lt; size &amp;&amp; (pe = events.poll()) != null; i++ ) &#123; result = true; try &#123; // 逐个执行 event.run() pe.run(); // 该 PollerEvent 还得给以后用，这里 reset 一下(还是之前说过的缓存) pe.reset(); if (running &amp;&amp; !paused) &#123; eventCache.push(pe); &#125; &#125; catch ( Throwable x ) &#123; log.error("",x); &#125; &#125; return result;&#125; events() 方法比较简单，就是取出当前队列中的 PollerEvent 对象，逐个执行 event.run() 方法。 然后，现在来看 Poller 的 run() 方法，该方法会一直循环，直到 poller.destroy() 被调用。 Poller # run1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public void run() &#123; while (true) &#123; boolean hasEvents = false; try &#123; if (!close) &#123; // 执行 events 队列中每个 event 的 run() 方法 hasEvents = events(); // wakeupCounter 的初始值为 0，这里设置为 -1 if (wakeupCounter.getAndSet(-1) &gt; 0) &#123; //if we are here, means we have other stuff to do //do a non blocking select keyCount = selector.selectNow(); &#125; else &#123; // timeout 默认值 1 秒 keyCount = selector.select(selectorTimeout); &#125; wakeupCounter.set(0); &#125; // 篇幅所限，我们就不说 close 的情况了 if (close) &#123; events(); timeout(0, false); try &#123; selector.close(); &#125; catch (IOException ioe) &#123; log.error(sm.getString("endpoint.nio.selectorCloseFail"), ioe); &#125; break; &#125; &#125; catch (Throwable x) &#123; ExceptionUtils.handleThrowable(x); log.error("",x); continue; &#125; //either we timed out or we woke up, process events first // 这里没什么好说的，顶多就再执行一次 events() 方法 if ( keyCount == 0 ) hasEvents = (hasEvents | events()); // 如果刚刚 select 有返回 ready keys，进行处理 Iterator&lt;SelectionKey&gt; iterator = keyCount &gt; 0 ? selector.selectedKeys().iterator() : null; // Walk through the collection of ready keys and dispatch // any active event. while (iterator != null &amp;&amp; iterator.hasNext()) &#123; SelectionKey sk = iterator.next(); NioSocketWrapper attachment = (NioSocketWrapper)sk.attachment(); // Attachment may be null if another thread has called // cancelledKey() if (attachment == null) &#123; iterator.remove(); &#125; else &#123; iterator.remove(); // ※※※※※ 处理 ready key ※※※※※ processKey(sk, attachment); &#125; &#125;//while //process timeouts timeout(keyCount,hasEvents); &#125;//while getStopLatch().countDown();&#125; poller 的 run() 方法主要做了调用 events() 方法和处理注册到 Selector 上的 ready key，这里我们暂时不展开 processKey 方法，因为此方法必定是及其复杂的。 我们回过头来看之前从 acceptor 线程中调用的 register 方法。 Poller # register1234567891011121314151617181920public void register(final NioChannel socket) &#123; socket.setPoller(this); NioSocketWrapper ka = new NioSocketWrapper(socket, NioEndpoint.this); socket.setSocketWrapper(ka); ka.setPoller(this); ka.setReadTimeout(getConnectionTimeout()); ka.setWriteTimeout(getConnectionTimeout()); ka.setKeepAliveLeft(NioEndpoint.this.getMaxKeepAliveRequests()); ka.setSecure(isSSLEnabled()); PollerEvent r = eventCache.pop(); ka.interestOps(SelectionKey.OP_READ);//this is what OP_REGISTER turns into. // 注意第三个参数值 OP_REGISTER if ( r==null) r = new PollerEvent(socket,ka,OP_REGISTER); else r.reset(socket,ka,OP_REGISTER); // 添加 event 到 poller 中 addEvent(r);&#125; 这里将这个 socket（包含 socket 和 buffer 的 NioChannel 实例） 包装为一个 PollerEvent，然后添加到 events 中，此时调用此方法的 acceptor 结束返回，去处理新的 accepted 连接了。 接下来，我们已经知道了，poller 线程在循环过程中会不断调用 events() 方法，那么 PollerEvent 的 run() 方法很快就会被执行，我们就来看看刚刚这个新的连接被注册到这个 poller 后，会发生什么。 PollerEvent # run1234567891011121314151617181920212223242526272829303132333435363738394041424344@Overridepublic void run() &#123; // 对于新来的连接，前面我们说过，interestOps == OP_REGISTER if (interestOps == OP_REGISTER) &#123; try &#123; // 这步很关键！！！ // 将这个新连接 SocketChannel 注册到该 poller 的 Selector 中， // 设置监听 OP_READ 事件， // 将 socketWrapper 设置为 attachment 进行传递(这个对象可是什么鬼都有，往上看就知道了) socket.getIOChannel().register( socket.getPoller().getSelector(), SelectionKey.OP_READ, socketWrapper); &#125; catch (Exception x) &#123; log.error(sm.getString("endpoint.nio.registerFail"), x); &#125; &#125; else &#123; /* else 这块不介绍，省得大家头大 */ final SelectionKey key = socket.getIOChannel().keyFor(socket.getPoller().getSelector()); try &#123; if (key == null) &#123; // The key was cancelled (e.g. due to socket closure) // and removed from the selector while it was being // processed. Count down the connections at this point // since it won't have been counted down when the socket // closed. socket.socketWrapper.getEndpoint().countDownConnection(); &#125; else &#123; final NioSocketWrapper socketWrapper = (NioSocketWrapper) key.attachment(); if (socketWrapper != null) &#123; //we are registering the key to start with, reset the fairness counter. int ops = key.interestOps() | interestOps; socketWrapper.interestOps(ops); key.interestOps(ops); &#125; else &#123; socket.getPoller().cancelledKey(key); &#125; &#125; &#125; catch (CancelledKeyException ckx) &#123; try &#123; socket.getPoller().cancelledKey(key); &#125; catch (Exception ignore) &#123;&#125; &#125; &#125;&#125; 到这里，我们再回顾一下：刚刚在 PollerEvent 的 run() 方法中，我们看到，新的 SocketChannel 注册到了 Poller 内部的 Selector 中，监听 OP_READ 事件，然后我们再回到 Poller 的 run() 看下，一旦该 SocketChannel 是 readable 的状态，那么就会进入到 poller 的 processKey 方法。12345678910111213141516171819202122232425262728293031323334353637383940414243protected void processKey(SelectionKey sk, NioSocketWrapper attachment) &#123; try &#123; if ( close ) &#123; cancelledKey(sk); &#125; else if ( sk.isValid() &amp;&amp; attachment != null ) &#123; if (sk.isReadable() || sk.isWritable() ) &#123; // 忽略 sendfile if ( attachment.getSendfileData() != null ) &#123; processSendfile(sk,attachment, false); &#125; else &#123; // unregister 相应的 interest set， // 如接下来是处理 SocketChannel 进来的数据，那么就不再监听该 channel 的 OP_READ 事件 unreg(sk, attachment, sk.readyOps()); boolean closeSocket = false; // Read goes before write if (sk.isReadable()) &#123; // 处理读 if (!processSocket(attachment, SocketEvent.OPEN_READ, true)) &#123; closeSocket = true; &#125; &#125; if (!closeSocket &amp;&amp; sk.isWritable()) &#123; // 处理写 if (!processSocket(attachment, SocketEvent.OPEN_WRITE, true)) &#123; closeSocket = true; &#125; &#125; if (closeSocket) &#123; cancelledKey(sk); &#125; &#125; &#125; &#125; else &#123; //invalid key cancelledKey(sk); &#125; &#125; catch ( CancelledKeyException ckx ) &#123; cancelledKey(sk); &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); log.error("",t); &#125;&#125; 接下来是 processSocket 方法，注意第三个参数，上面进来的时候是 true。 AbstractEndpoint # processSocket1234567891011121314151617181920212223242526272829303132public boolean processSocket(SocketWrapperBase&lt;S&gt; socketWrapper, SocketEvent event, boolean dispatch) &#123; try &#123; if (socketWrapper == null) &#123; return false; &#125; SocketProcessorBase&lt;S&gt; sc = processorCache.pop(); if (sc == null) &#123; // 创建一个 SocketProcessor 的实例 sc = createSocketProcessor(socketWrapper, event); &#125; else &#123; sc.reset(socketWrapper, event); &#125; Executor executor = getExecutor(); if (dispatch &amp;&amp; executor != null) &#123; // 将任务放到之前建立的 worker 线程池中执行 executor.execute(sc); &#125; else &#123; sc.run(); // ps: 如果 dispatch 为 false，那么就当前线程自己执行 &#125; &#125; catch (RejectedExecutionException ree) &#123; getLog().warn(sm.getString("endpoint.executor.fail", socketWrapper) , ree); return false; &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); // This means we got an OOM or similar creating a thread, or that // the pool and its queue are full getLog().error(sm.getString("endpoint.process.fail"), t); return false; &#125; return true;&#125; NioEndpoint # createSocketProcessor12345@Overrideprotected SocketProcessorBase&lt;NioChannel&gt; createSocketProcessor( SocketWrapperBase&lt;NioChannel&gt; socketWrapper, SocketEvent event) &#123; return new SocketProcessor(socketWrapper, event);&#125; 我们看到，提交到 worker 线程池中的是 NioEndpoint.SocketProcessor 的实例，至于它的 run() 方法之后的逻辑，我们就不再继续往里分析了。 总结最后，再祭出文章开始的那张图来总结一下： 这里简单梳理下前面我们说的流程，帮大家回忆一下： 指定 Protocol，初始化相应的 Endpoint，我们分析的是 NioEndpoint； init 过程：在 NioEndpoint 中做 bind 操作； start 过程：启动 worker 线程池，启动 1 个 Acceptor 和 2 个 Poller，当然它们都是默认值，可配； Acceptor 获取到新的连接后，getPoller0() 获取其中一个 Poller，然后 register 到 Poller 中； Poller 循环 selector.select(xxx)，如果有通道 readable，那么在 processKey 中将其放到 worker 线程池中。 后续的流程，感兴趣的读者请自行分析，本文就说到这里了。]]></content>
      <categories>
        <category>NIO</category>
      </categories>
      <tags>
        <tag>NIO</tag>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat启动时为何会有大量Log4J日志]]></title>
    <url>%2F2018%2F03%2F15%2F2018-03-15-tomcat-log%2F</url>
    <content type="text"><![CDATA[问题描述经常遇到这样的问题： 1、开发人员在开发Java Web项目的时候，工程启动过程中Console就输出了大量的log4j日志，无论如何调整工程中的log4j配置都无法控制这部分日志的产生,这些日志我们往往并不关心，反而影响我们查找真正关注的日志。 2、在部署Tomcat服务的时候，Tomcat启动过程中catalina.out文件中就输出了大量的log4j的日志信息，无论如何调整Tomcat的日志配置都无法控制这部分日志的产生； 控制不了这部分日志也很正常，因为在Web中间件（Tomcat）启动的时候，Spring并没有启动，或者正在启动中，根本没有运行到加载工程log4j配置这一步，自然调整工程的log4j配置是没有任何作用的。 那么，这部分日志是如何产生的呢？ 问题分析实际上，log4j是会自动检索配置文件，Spring启动后确实会为log4j指定一个新的配置文件，但是在此过程之前，log4j会按照自己的规则寻找一个默认的配置文件。让我们看看log4j的源码设计吧。 LogManager是一个Logger的管理者，所有的Logger都由该类管理。该类中完成了对默认log4j配置文件的检索与配置，且这个过程是在LogManager类初始化过程中实现的。如下所示:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667static final public String DEFAULT_CONFIGURATION_KEY="log4j.configuration";static final public String CONFIGURATOR_CLASS_KEY="log4j.configuratorClass";public static final String DEFAULT_INIT_OVERRIDE_KEY = "log4j.defaultInitOverride";static &#123; // By default we use a DefaultRepositorySelector which always returns 'h'. Hierarchy h = new Hierarchy(new RootLogger((Level) Level.DEBUG)); repositorySelector = new DefaultRepositorySelector(h); /** Search for the properties file log4j.properties in the CLASSPATH. */ String override =OptionConverter.getSystemProperty(DEFAULT_INIT_OVERRIDE_KEY, null); // LogManager首先判断是否需要跳过默认配置过程。如果Java环境中配置了变 //量log4j.defaultInitOverride，且该变量不为false，则将跳过默认配置过程，但一般来说，是没有跳过这个过程的. if(override == null || "false".equalsIgnoreCase(override)) &#123; //从Java环境变量中获取变量log4j.configuratorClass和log4j.configuration，log4j.configuratorClass用来指定解析log4j配置的类名称，log4j.configuration则是默认的log4j配置文件地址 String configurationOptionStr = OptionConverter.getSystemProperty( DEFAULT_CONFIGURATION_KEY, null); String configuratorClassName = OptionConverter.getSystemProperty( CONFIGURATOR_CLASS_KEY, null); URL url = null; // 若没有指定log4j.configuration，则LogManager将首先尝试获取log4j.xml文件作为其配置文件， //若log4j.xml不存在，则将尝试log4j.properties文件作为其配置文件 if(configurationOptionStr == null) &#123; url = Loader.getResource(DEFAULT_XML_CONFIGURATION_FILE); if(url == null) &#123; url = Loader.getResource(DEFAULT_CONFIGURATION_FILE); &#125; &#125; else &#123; try &#123; url = new URL(configurationOptionStr); &#125; catch (MalformedURLException ex) &#123; // so, resource is not a URL: // attempt to get the resource from the class path url = Loader.getResource(configurationOptionStr); &#125; &#125; // If we have a non-null url, then delegate the rest of the // configuration to the OptionConverter.selectAndConfigure // method. if(url != null) &#123; LogLog.debug("Using URL ["+url+"] for automatic log4j configuration."); try &#123; OptionConverter.selectAndConfigure(url, configuratorClassName, LogManager.getLoggerRepository()); &#125; catch (NoClassDefFoundError e) &#123; LogLog.warn("Error during default initialization", e); &#125; &#125; else &#123; LogLog.debug("Could not find resource: ["+configurationOptionStr+"]."); &#125; &#125; else &#123; LogLog.debug("Default initialization of overridden by " + DEFAULT_INIT_OVERRIDE_KEY + "property."); &#125; &#125; 问题解决其实，产生本文所述问题的原因就在于，工程中存在的log4j配置文件被LogManager按照默认的方式初始化了，因此在Spring完成log4j配置之前，log4j将按照这些默认的配置文件的配置来进行日志输出。 最关键的问题在于，LogManager所检索的默认配置不仅仅可是存在于工程classpath中，还可以存在于工程所依赖的jar包中。因此，若工程中的log4j配置与LogManager中默认配置路径不相同，但工程依赖的jar包却中存在与默认配置路径相同的log4j配置，则LogManager将优先以jar包中的配置为准。 经过翻查，主流、成熟的第三方jar包中一般是不包含log4j配置文件的，但是不成熟的第三方jar或者自行封装的jar就不一定了，诸如antispam-keyword、common、multi_account_validator、xxsdefender等jar中全都含有log4j配置信息，因此导致了本文所述的问题。 即使依赖的jar包中存在log4j配置，本文所述问题也是可以解决的。只需为Java增加-Dlog4j.configuration=config/log4j.xml参数，将工程中的log4j配置路径设置给log4j.configuration变量即可解决问题。 在Eclipse中可如此配置： 在Tomcat中可编辑$CATALINA_HOME/bin/setenv.sh，加入如下代码： CATALINA_OPTS=”-Dlog4j.configuration=config/log4j.xml”]]></content>
      <categories>
        <category>tomcat</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 并发基础之内存模型]]></title>
    <url>%2F2018%2F03%2F14%2F2018-03-14-java-memory-model%2F</url>
    <content type="text"><![CDATA[原文链接: https://javadoop.com/post/java-memory-model 关于 Java 并发也算是写了好几篇文章了，本文将介绍一些比较基础的内容，注意，阅读本文需要一定的并发基础。 本文的主要目的是让大家对于并发程序中的重排序、内存可见性以及原子性有一定的了解，同时要能准确理解 synchronized、volatile、final 几个关键字的作用。 另外，本文还对双重检查形式的单例模式为什么需要使用 volatile 做了深入的解释。 并发三问题这节将介绍重排序、内存可见性以及原子性相关的知识，这些也是并发程序为什么难写的原因。 1. 重排序请读者先在自己的电脑上运行一下以下程序：1234567891011121314151617181920212223242526272829303132333435363738394041424344public class Test &#123; private static int x = 0, y = 0; private static int a = 0, b =0; public static void main(String[] args) throws InterruptedException &#123; int i = 0; for(;;) &#123; i++; x = 0; y = 0; a = 0; b = 0; CountDownLatch latch = new CountDownLatch(1); Thread one = new Thread(() -&gt; &#123; try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; &#125; a = 1; x = b; &#125;); Thread other = new Thread(() -&gt; &#123; try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; &#125; b = 1; y = a; &#125;); one.start();other.start(); latch.countDown(); one.join();other.join(); String result = "第" + i + "次 (" + x + "," + y + "）"; if(x == 0 &amp;&amp; y == 0) &#123; System.err.println(result); break; &#125; else &#123; System.out.println(result); &#125; &#125; &#125;&#125; 几秒后，我们就可以得到 x == 0 &amp;&amp; y == 0 这个结果，仔细看看代码就会知道，如果不发生重排序的话，这个结果是不可能出现的。 重排序由以下几种机制引起： 编译器优化：对于没有数据依赖关系的操作，编译器在编译的过程中会进行一定程度的重排。 大家仔细看看线程 1 中的代码，编译器是可以将 a = 1 和 x = b 换一下顺序的，因为它们之间没有数据依赖关系，同理，线程 2 也一样，那就不难得到 x == y == 0 这种结果了。 指令重排序：CPU 优化行为，也是会对不存在数据依赖关系的指令进行一定程度的重排。 这个和编译器优化差不多，就算编译器不发生重排，CPU 也可以对指令进行重排，这个就不用多说了。 内存系统重排序：内存系统没有重排序，但是由于有缓存的存在，使得程序整体上会表现出乱序的行为。 假设不发生编译器重排和指令重排，线程 1 修改了 a 的值，但是修改以后，a 的值可能还没有写回到主存中，那么线程 2 得到 a == 0 就是很自然的事了。同理，线程 2 对于 b 的赋值操作也可能没有及时刷新到主存中。 2. 内存可见性前面在说重排序的时候，也说到了内存可见性的问题，这里再啰嗦一下。 线程间的对于共享变量的可见性问题不是直接由多核引起的，而是由多缓存引起的。如果每个核心共享同一个缓存，那么也就不存在内存可见性问题了。 现代多核 CPU 中每个核心拥有自己的一级缓存或一级缓存加上二级缓存等，问题就发生在每个核心的独占缓存上。每个核心都会将自己需要的数据读到独占缓存中，数据修改后也是写入到缓存中，然后等待刷入到主存中。所以会导致有些核心读取的值是一个过期的值。 Java 作为高级语言，屏蔽了这些底层细节，用 JMM 定义了一套读写内存数据的规范，虽然我们不再需要关心一级缓存和二级缓存的问题，但是，JMM 抽象了主内存和本地内存的概念。 所有的共享变量存在于主内存中，每个线程有自己的本地内存，线程读写共享数据也是通过本地内存交换的，所以可见性问题依然是存在的。这里说的本地内存并不是真的是一块给每个线程分配的内存，而是 JMM 的一个抽象，是对于寄存器、一级缓存、二级缓存等的抽象。 3. 原子性在本文中，原子性不是重点，它将作为并发编程中需要考虑的一部分进行介绍。 说到原子性的时候，大家应该都能想到 long 和 double，它们的值需要占用 64 位的内存空间，Java 编程语言规范中提到，对于 64 位的值的写入，可以分为两个 32 位的操作进行写入。本来一个整体的赋值操作，被拆分为低 32 位赋值和高 32 位赋值两个操作，中间如果发生了其他线程对于这个值的读操作，必然就会读到一个奇怪的值。 这个时候我们要使用 volatile 关键字进行控制了，JMM 规定了对于 volatile long 和 volatile double，JVM 需要保证写入操作的原子性。 另外，对于引用的读写操作始终是原子的，不管是 32 位的机器还是 64 位的机器。 Java 编程语言规范同样提到，鼓励 JVM 的开发者能保证 64 位值操作的原子性，也鼓励使用者尽量使用 volatile 或使用正确的同步方式。关键词是”鼓励“。 在 64 位的 JVM 中，不加 volatile 也是可以的，同样能保证对于 long 和 double 写操作的原子性。关于这一点，我没有找到官方的材料描述它，如果读者有相关的信息，希望可以给我反馈一下。 Java 对于并发的规范约束并发问题使得我们的代码有可能会产生各种各样的执行结果，显然这是我们不能接受的，所以 Java 编程语言规范需要规定一些基本规则，JVM 实现者会在这些规则的约束下来实现 JVM，然后开发者也要按照规则来写代码，这样写出来的并发代码我们才能准确预测执行结果。下面进行一些简单的介绍。 Synchronization OrderJava 语言规范对于同步定义了一系列的规则：17.4.4. Synchronization Order，包括了如下同步关系： 对于监视器 m 的解锁与所有后续操作对于 m 的加锁操作同步 对 volatile 变量 v 的写入，与所有其他线程后续对 v 的读同步 启动线程的操作与线程中的第一个操作同步。 对于每个属性写入默认值（0， false，null）与每个线程对其进行的操作同步。 尽管在创建对象完成之前对对象属性写入默认值有点奇怪，但从概念上来说，每个对象都是在程序启动时用默认值初始化来创建的。 线程 T1 的最后操作与线程 T2 发现线程 T1 已经结束同步。 线程 T2 可以通过 T1.isAlive() 或 T1.join() 方法来判断 T1 是否已经终结。 如果线程 T1 中断了 T2，那么线程 T1 的中断操作与其他所有线程发现 T2 被中断了同步（通过抛出 InterruptedException 异常，或者调用 Thread.interrupted 或 Thread.isInterrupted ） Happens-before Order两个操作可以用 happens-before 来确定它们的执行顺序，如果一个操作 happens-before 于另一个操作，那么我们说第一个操作对于第二个操作是可见的。 如果我们分别有操作 x 和操作 y，我们写成 hb(x, y) 来表示 x happens-before y。以下几个规则也是来自于 Java 8 语言规范 Happens-before Order： 如果操作 x 和操作 y 是同一个线程的两个操作，并且在代码上操作 x 先于操作 y 出现，那么有 hb(x, y) 对象构造方法的最后一行指令 happens-before 于 finalize() 方法的第一行指令。 如果操作 x 与随后的操作 y 构成同步，那么 hb(x, y)。这条说的是前面一小节的内容。 hb(x, y) 和 hb(y, z)，那么可以推断出 hb(x, z) 这里再提一点，x happens-before y，并不是说 x 操作一定要在 y 操作之前被执行，而是说 x 的执行结果对于 y 是可见的，只要满足可见性，发生了重排序也是可以的。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>java语言</tag>
        <tag>内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出 gRPC（一）：gRPC 入门及服务端创建和调用原理]]></title>
    <url>%2F2018%2F03%2F12%2F2018-03-12-grpc-1%2F</url>
    <content type="text"><![CDATA[RPC 入门RPC 框架原理RPC 框架的目标就是让远程服务调用更加简单、透明，RPC 框架负责屏蔽底层的传输方式（TCP 或者 UDP）、序列化方式（XML/Json/ 二进制）和通信细节。服务调用者可以像调用本地接口一样调用远程的服务提供者，而不需要关心底层通信细节和调用过程。 RPC 框架的调用原理图如下所示： 业界主流的 RPC 框架业界主流的 RPC 框架整体上分为三类： 支持多语言的 RPC 框架，比较成熟的有 Google 的 gRPC、Apache（Facebook）的 Thrift； 只支持特定语言的 RPC 框架，例如新浪微博的 Motan； 支持服务治理等服务化特性的分布式服务框架，其底层内核仍然是 RPC 框架, 例如阿里的 Dubbo。 随着微服务的发展，基于语言中立性原则构建微服务，逐渐成为一种主流模式，例如对于后端并发处理要求高的微服务，比较适合采用 Go 语言构建，而对于前端的 Web 界面，则更适合 Java 和 JavaScript。 因此，基于多语言的 RPC 框架来构建微服务，是一种比较好的技术选择。例如 Netflix，API 服务编排层和后端的微服务之间就采用 gRPC 进行通信。 gRPC 简介gRPC 是一个高性能、开源和通用的 RPC 框架，面向服务端和移动端，基于HTTP/2设计。 gRPC 是由 Google 开发并开源的一种语言中立的 RPC 框架，当前支持 C、Java 和 Go 语言，其中 C 版本支持 C、C++、Node.js、C# 等。当前 Java 版本最新 Release 版为 1.6.0，GitHub地址：https://github.com/grpc/grpc-java gRPC 的调用示例如下所示： gRPC 特点 语言中立，支持多种语言； 基于IDL 文件定义服务，通过proto3工具生成指定语言的数据结构、服务端接口以及客户端 Stub； 通信协议基于标准的HTTP/2设计，支持双向流、消息头压缩、单 TCP 的多路复用、服务端推送等特性，这些特性使得 gRPC 在移动端设备上更加省电和节省网络流量； 序列化支持PB（Protocol Buffer）和 JSON，PB 是一种语言无关的高性能序列化框架，基于 HTTP/2 + PB, 保障了 RPC 调用的高性能。 gRPC 服务端创建以官方的 helloworld 为例，介绍 gRPC 服务端创建以及 service 调用流程（采用简单 RPC 模式）。 服务端创建业务代码 服务定义如下（helloworld.proto）：1234567891011service Greeter &#123; rpc SayHello (HelloRequest) returns (HelloReply) &#123;&#125;&#125;message HelloRequest &#123; string name = 1;&#125;message HelloReply &#123; string message = 1;&#125; 服务端创建代码如下（HelloWorldServer 类）：12345678private void start() throws IOException &#123; /* The port on which the server should run */ int port = 50051; server = ServerBuilder.forPort(port) .addService(new GreeterImpl()) .build() .start();... 其中，服务端接口实现类（GreeterImpl）如下所示：12345678static class GreeterImpl extends GreeterGrpc.GreeterImplBase &#123; @Override public void sayHello(HelloRequest req, StreamObserver&lt;HelloReply&gt; responseObserver) &#123; HelloReply reply = HelloReply.newBuilder().setMessage("Hello " + req.getName()).build(); responseObserver.onNext(reply); responseObserver.onCompleted(); &#125; &#125; 服务端创建流程gRPC 服务端创建采用Build 模式，对底层服务绑定、transportServer 和 NettyServer 的创建和实例化做了封装和屏蔽，让服务调用者不用关心 RPC 调用细节，整体上分为三个过程： 创建 Netty HTTP/2 服务端； 将需要调用的服务端接口实现类注册到内部的 Registry 中，RPC 调用时，可以根据 RPC 请求消息中的服务定义信息查询到服务接口实现类； 创建 gRPC Server，它是 gRPC 服务端的抽象，聚合了各种 Listener，用于 RPC 消息的统一调度和处理。下面我们看下 gRPC 服务端创建流程： gRPC 服务端创建关键流程分析： NettyServer 实例创建 gRPC 服务端创建，首先需要初始化 NettyServer，它是 gRPC 基于 Netty 4.1 HTTP/2 协议栈之上封装的 HTTP/2 服务端。NettyServer 实例由 NettyServerBuilder 的 buildTransportServer 方法构建，NettyServer 构建完成之后，监听指定的 Socket 地址，即可实现基于 HTTP/2 协议的请求消息接入。 绑定 IDL 定义的服务接口实现类 gRPC 与其它一些 RPC 框架的差异点是服务接口实现类的调用并不是通过动态代理和反射机制，而是通过 proto 工具生成代码，在服务端启动时，将服务接口实现类实例注册到 gRPC 内部的服务注册中心上。请求消息接入之后，可以根据服务名和方法名，直接调用启动时注册的服务实例，而不需要通过反射的方式进行调用，性能更优。 gRPC 服务实例（ServerImpl）构建 ServerImpl负责整个 gRPC 服务端消息的调度和处理，创建 ServerImpl 实例过程中，会对服务端依赖的对象进行初始化，例如 Netty 的线程池资源、gRPC 的线程池、内部的服务注册类（InternalHandlerRegistry）等，ServerImpl 初始化完成之后，就可以调用 NettyServer 的 start 方法启动 HTTP/2 服务端，接收 gRPC 客户端的服务调用请求。 服务端 service 调用流程gRPC 的客户端请求消息由 Netty Http2ConnectionHandler接入，由 gRPC 负责将 PB 消息（或者 JSON）反序列化为 POJO 对象，然后通过服务定义查询到该消息对应的接口实例，发起本地 Java 接口调用，调用完成之后，将响应消息反序列化为 PB（或者 JSON），通过 HTTP2 Frame发送给客户端。 流程并不复杂，但是细节却比较多，整个 service 调用可以划分为如下四个过程： gRPC 请求消息接入； gRPC 消息头和消息体处理； 内部的服务路由和调用； 响应消息发送。 gRPC 请求消息接入]]></content>
      <categories>
        <category>RPC</category>
      </categories>
      <tags>
        <tag>gRP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐技术博客(持续更新)]]></title>
    <url>%2F2018%2F03%2F06%2Fblog%2F</url>
    <content type="text"><![CDATA[James D Bloomhttp://blog.jamesdbloom.com/ 涵盖了与JVM语言和内部相关的许多不同主题。 特别是代码如何在JVM上运行和底层工作原理。 ### 技术小黑屋https://droidyue.com/ Java和安卓技术分享 阮一峰的网络日志http://www.ruanyifeng.com/blog/ 不用多介绍了，知名大牛 平凡希http://www.cnblogs.com/xiaoxi/ javadoophttps://javadoop.com 美团技术团队https://tech.meituan.com/ 你假笨http://lovestblog.cn/ JVM专家]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java 非阻塞 IO 和异步 IO]]></title>
    <url>%2F2018%2F02%2F27%2F2018-02-27-nio-and-aio%2F</url>
    <content type="text"><![CDATA[上一篇文章介绍了 Java NIO 中 Buffer、Channel 和 Selector 的基本操作，主要是一些接口操作，比较简单。 本文将介绍非阻塞 IO 和异步 IO，也就是大家耳熟能详的NIO 和 AIO。很多初学者可能分不清楚异步和非阻塞的区别，只是在各种场合能听到异步非阻塞这个词。 本文会先介绍并演示阻塞模式，然后引入非阻塞模式来对阻塞模式进行优化，最后再介绍 JDK7 引入的异步 IO，由于网上关于异步 IO 的介绍相对较少，所以这部分内容我会介绍得具体一些。 希望看完本文，读者可以对非阻塞 IO 和异步 IO 的迷雾看得更清晰些，或者为初学者解开一丝丝疑惑也是好的。 阻塞模式 IO我们已经介绍过使用 Java NIO 包组成一个简单的客户端-服务端网络通讯所需要的 ServerSocketChannel、SocketChannel 和 Buffer，我们这里整合一下它们，给出一个完整的可运行的例子：1234567891011121314151617181920public class Server &#123; public static void main(String[] args) throws IOException &#123; ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); // 监听 8080 端口进来的 TCP 链接 serverSocketChannel.socket().bind(new InetSocketAddress(8080)); while (true) &#123; // 这里会阻塞，直到有一个请求的连接进来 SocketChannel socketChannel = serverSocketChannel.accept(); // 开启一个新的线程来处理这个请求，然后在 while 循环中继续监听 8080 端口 SocketHandler handler = new SocketHandler(socketChannel); new Thread(handler).start(); &#125; &#125;&#125; 这里看一下新的线程需要做什么，SocketHandler：12345678910111213141516171819202122232425262728293031323334353637public class SocketHandler implements Runnable &#123; private SocketChannel socketChannel; public SocketHandler(SocketChannel socketChannel) &#123; this.socketChannel = socketChannel; &#125; @Override public void run() &#123; ByteBuffer buffer = ByteBuffer.allocate(1024); try &#123; // 将请求数据读入 Buffer 中 int num; while ((num = socketChannel.read(buffer)) &gt; 0) &#123; // 读取 Buffer 内容之前先 flip 一下 buffer.flip(); // 提取 Buffer 中的数据 byte[] bytes = new byte[num]; buffer.get(bytes); String re = new String(bytes, "UTF-8"); System.out.println("收到请求：" + re); // 回应客户端 ByteBuffer writeBuffer = ByteBuffer.wrap(("我已经收到你的请求，你的请求内容是：" + re).getBytes()); socketChannel.write(writeBuffer); buffer.clear(); &#125; &#125; catch (IOException e) &#123; IOUtils.closeQuietly(socketChannel); &#125; &#125;&#125; 最后，贴一下客户端 SocketChannel 的使用，客户端比较简单：1234567891011121314151617181920212223public class SocketChannelTest &#123; public static void main(String[] args) throws IOException &#123; SocketChannel socketChannel = SocketChannel.open(); socketChannel.connect(new InetSocketAddress("localhost", 8080)); // 发送请求 ByteBuffer buffer = ByteBuffer.wrap("1234567890".getBytes()); socketChannel.write(buffer); // 读取响应 ByteBuffer readBuffer = ByteBuffer.allocate(1024); int num; if ((num = socketChannel.read(readBuffer)) &gt; 0) &#123; readBuffer.flip(); byte[] re = new byte[num]; readBuffer.get(re); String result = new String(re, "UTF-8"); System.out.println("返回值: " + result); &#125; &#125;&#125; 上面介绍的阻塞模式的代码应该很好理解：来一个新的连接，我们就新开一个线程来处理这个连接，之后的操作全部由那个线程来完成。 那么，这个模式下的性能瓶颈在哪里呢？ 首先，每次来一个连接都开一个新的线程这肯定是不合适的。当活跃连接数在几十几百的时候当然是可以这样做的，但如果活跃连接数是几万几十万的时候，这么多线程明显就不行了。每个线程都需要一部分内存，内存会被迅速消耗，同时，线程切换的开销非常大。 其次，阻塞操作在这里也是一个问题。首先，accept() 是一个阻塞操作，当 accept() 返回的时候，代表有一个连接可以使用了，我们这里是马上就新建线程来处理这个 SocketChannel 了，但是，但是这里不代表对方就将数据传输过来了。所以，SocketChannel#read 方法将阻塞，等待数据，明显这个等待是不值得的。同理，write 方法也需要等待通道可写才能执行写入操作，这边的阻塞等待也是不值得的。 非阻塞 IO说完了阻塞模式的使用及其缺点以后，我们这里就可以介绍非阻塞 IO 了。 非阻塞 IO 的核心在于使用一个 Selector 来管理多个通道，可以是 SocketChannel，也可以是 ServerSocketChannel，将各个通道注册到 Selector 上，指定监听的事件。 之后可以只用一个线程来轮询这个 Selector，看看上面是否有通道是准备好的，当通道准备好可读或可写，然后才去开始真正的读写，这样速度就很快了。我们就完全没有必要给每个通道都起一个线程。 NIO 中 Selector 是对底层操作系统实现的一个抽象，管理通道状态其实都是底层系统实现的，这里简单介绍下在不同系统下的实现。 select：上世纪 80 年代就实现了，它支持注册 FD_SETSIZE(1024) 个 socket，在那个年代肯定是够用的，不过现在嘛，肯定是不行了。 poll：1997 年，出现了 poll 作为 select 的替代者，最大的区别就是，poll 不再限制 socket 数量。 select 和 poll 都有一个共同的问题，那就是它们都只会告诉你有几个通道准备好了，但是不会告诉你具体是哪几个通道。所以，一旦知道有通道准备好以后，自己还是需要进行一次扫描，显然这个不太好，通道少的时候还行，一旦通道的数量是几十万个以上的时候，扫描一次的时间都很可观了，时间复杂度 O(n)。所以，后来才催生了以下实现。 epoll：2002 年随 Linux 内核 2.5.44 发布，epoll 能直接返回具体的准备好的通道，时间复杂度 O(1)。 除了 Linux 中的 epoll，2000 年 FreeBSD 出现了 Kqueue，还有就是，Solaris 中有 /dev/poll。 前面说了那么多实现，但是没有出现 Windows，Windows 平台的非阻塞 IO 使用 select，我们也不必觉得 Windows 很落后，在 Windows 中 IOCP 提供的异步 IO 是比较强大的。 我们回到 Selector，毕竟 JVM 就是这么一个屏蔽底层实现的平台，我们面向 Selector 编程就可以了。 之前在介绍 Selector 的时候已经了解过了它的基本用法，这边来一个可运行的实例代码，大家不妨看看：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class SelectorServer &#123; public static void main(String[] args) throws IOException &#123; Selector selector = Selector.open(); ServerSocketChannel server = ServerSocketChannel.open(); server.socket().bind(new InetSocketAddress(8080)); // 将其注册到 Selector 中，监听 OP_ACCEPT 事件 server.configureBlocking(false); server.register(selector, SelectionKey.OP_ACCEPT); while (true) &#123; int readyChannels = selector.select(); if (readyChannels == 0) &#123; continue; &#125; Set&lt;SelectionKey&gt; readyKeys = selector.selectedKeys(); // 遍历 Iterator&lt;SelectionKey&gt; iterator = readyKeys.iterator(); while (iterator.hasNext()) &#123; SelectionKey key = iterator.next(); iterator.remove(); if (key.isAcceptable()) &#123; // 有已经接受的新的到服务端的连接 SocketChannel socketChannel = server.accept(); // 有新的连接并不代表这个通道就有数据， // 这里将这个新的 SocketChannel 注册到 Selector，监听 OP_READ 事件，等待数据 socketChannel.configureBlocking(false); socketChannel.register(selector, SelectionKey.OP_READ); &#125; else if (key.isReadable()) &#123; // 有数据可读 // 上面一个 if 分支中注册了监听 OP_READ 事件的 SocketChannel SocketChannel socketChannel = (SocketChannel) key.channel(); ByteBuffer readBuffer = ByteBuffer.allocate(1024); int num = socketChannel.read(readBuffer); if (num &gt; 0) &#123; // 处理进来的数据... System.out.println("收到数据：" + new String(readBuffer.array()).trim()); ByteBuffer buffer = ByteBuffer.wrap("返回给客户端的数据...".getBytes()); socketChannel.write(buffer); &#125; else if (num == -1) &#123; // -1 代表连接已经关闭 socketChannel.close(); &#125; &#125; &#125; &#125; &#125;&#125; 至于客户端，大家可以继续使用上一节介绍阻塞模式时的客户端进行测试。 NIO.2 异步 IOMore New IO，或称 NIO.2，随 JDK 1.7 发布，包括了引入异步 IO 接口和 Paths 等文件访问接口。 异步这个词，我想对于绝大多数开发者来说都很熟悉，很多场景下我们都会使用异步。 通常，我们会有一个线程池用于执行异步任务，提交任务的线程将任务提交到线程池就可以立马返回，不必等到任务真正完成。如果想要知道任务的执行结果，通常是通过传递一个回调函数的方式，任务结束后去调用这个函数。 同样的原理，Java 中的异步 IO 也是一样的，都是由一个线程池来负责执行任务，然后使用回调或自己去查询结果。 大部分开发者都知道为什么要这么设计了，这里再啰嗦一下。异步 IO 主要是为了控制线程数量，减少过多的线程带来的内存消耗和 CPU 在线程调度上的开销。 在Unix/Linux 等系统中，JDK 使用了并发包中的线程池来管理任务，具体可以查看 AsynchronousChannelGroup 的源码。 在 Windows 操作系统中，提供了一个叫做I/O Completion Ports 的方案，通常简称为IOCP，操作系统负责管理线程池，其性能非常优异，所以在 Windows 中 JDK 直接采用了 IOCP 的支持，使用系统支持，把更多的操作信息暴露给操作系统，也使得操作系统能够对我们的 IO 进行一定程度的优化。 在 Linux 中其实也是有异步 IO 系统实现的，但是限制比较多，性能也一般，所以 JDK 采用了自建线程池的方式。 本文还是以实用为主，想要了解更多信息请自行查找其他资料，下面对 Java 异步 IO 进行实践性的介绍。 总共有三个类需要我们关注，分别是 AsynchronousSocketChannel，AsynchronousServerSocketChannel和AsynchronousFileChannel，只不过是在之前介绍的 FileChannel、SocketChannel 和 ServerSocketChannel 的类名上加了个前缀 Asynchronous。 Java 异步 IO 提供了两种使用方式，分别是返回 Future 实例和使用回调函数。 返回 Future 实例返回 java.util.concurrent.Future 实例的方式我们应该很熟悉，JDK 线程池就是这么使用的。Future 接口的几个方法语义在这里也是通用的，这里先做简单介绍。 future.isDone(); 判断操作是否已经完成，包括了正常完成、异常抛出、取消 future.cancel(true); 取消操作，方式是中断。参数 true 说的是，即使这个任务正在执行，也会进行中断。 future.isCancelled(); 是否被取消，只有在任务正常结束之前被取消，这个方法才会返回 true future.get(); 这是我们的老朋友，获取执行结果，阻塞。 future.get(10, TimeUnit.SECONDS); 如果上面的 get() 方法的阻塞你不满意，那就设置个超时时间,超时抛出TimeoutException异常 提供 CompletionHandler 回调函数12345678java.nio.channels.CompletionHandler 接口定义：public interface CompletionHandler&lt;V,A&gt; &#123; void completed(V result, A attachment); void failed(Throwable exc, A attachment);&#125; 注意，参数上有个 attachment，虽然不常用，我们可以在各个支持的方法中传递这个参数值 123456789101112AsynchronousServerSocketChannel listener = AsynchronousServerSocketChannel.open().bind(null);// accept 方法的第一个参数可以传递 attachmentlistener.accept(attachment, new CompletionHandler&lt;AsynchronousSocketChannel, Object&gt;() &#123; public void completed( AsynchronousSocketChannel client, Object attachment) &#123; // &#125; public void failed(Throwable exc, Object attachment) &#123; // &#125;&#125;); AsynchronousFileChannel网上关于 Non-Blocking IO 的介绍文章很多，但是 Asynchronous IO 的文章相对就少得多了，所以我这边会多介绍一些相关内容。 首先，我们就来关注异步的文件 IO，前面我们说了，文件 IO 在所有的操作系统中都不支持非阻塞模式，但是我们可以对文件 IO 采用异步的方式来提高性能。 下面，我会介绍 AsynchronousFileChannel 里面的一些重要的接口，都很简单，读者要是觉得无趣，直接滑到下一个标题就可以了。 实例化：1AsynchronousFileChannel channel = AsynchronousFileChannel.open(Paths.get("/Users/hongjie/test.txt")); 一旦实例化完成，我们就可以着手准备将数据读入到 Buffer 中：12ByteBuffer buffer = ByteBuffer.allocate(1024);Future&lt;Integer&gt; result = channel.read(buffer, 0); 异步文件通道的读操作和写操作都需要提供一个文件的开始位置，文件开始位置为 0 顺便也贴一下写操作的两个版本的接口：123456public abstract Future&lt;Integer&gt; write(ByteBuffer src, long position);public abstract &lt;A&gt; void write(ByteBuffer src, long position, A attachment, CompletionHandler&lt;Integer,? super A&gt; handler); 我们可以看到，AIO 的读写主要也还是与 Buffer 打交道，这个与 NIO 是一脉相承的。 另外，还提供了用于将内存中的数据刷入到磁盘的方法：1public abstract void force(boolean metaData) throws IOException; 因为我们对文件的写操作，操作系统并不会直接针对文件操作，系统会缓存，然后周期性地刷入到磁盘。如果希望将数据及时写入到磁盘中，以免断电引发部分数据丢失，可以调用此方法。参数如果设置为 true，意味着同时也将文件属性信息更新到磁盘, 否则磁盘只更新文件内容。 还有，还提供了对文件的锁定功能，我们可以锁定文件的部分数据，这样可以进行排他性的操作。1public abstract Future&lt;FileLock&gt; lock(long position, long size, boolean shared); position 是要锁定内容的开始位置，size 指示了要锁定的区域大小，shared 指示需要的是共享锁还是排他锁 当然，也可以使用回调函数的版本：12345public abstract &lt;A&gt; void lock(long position, long size, boolean shared, A attachment, CompletionHandler&lt;FileLock,? super A&gt; handler); 文件锁定功能上还提供了 tryLock 方法，此方法会快速返回结果：12public abstract FileLock tryLock(long position, long size, boolean shared) throws IOException; 这个方法很简单，就是尝试去获取锁，如果该区域已被其他线程或其他应用锁住，那么立刻返回 null，否则返回 FileLock 对象。 AsynchronousFileChannel 操作大体上也就以上介绍的这些接口，还是比较简单的，这里就少一些废话早点结束好了。 AsynchronousServerSocketChannel这个类对应的是非阻塞 IO 的 ServerSocketChannel，大家可以类比下使用方式。 我们就废话少说，用代码说事吧：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.javadoop.aio;import java.io.IOException;import java.net.InetSocketAddress;import java.net.SocketAddress;import java.nio.ByteBuffer;import java.nio.channels.AsynchronousServerSocketChannel;import java.nio.channels.AsynchronousSocketChannel;import java.nio.channels.CompletionHandler;public class Server &#123; public static void main(String[] args) throws IOException &#123; // 实例化，并监听端口 AsynchronousServerSocketChannel server = AsynchronousServerSocketChannel.open().bind(new InetSocketAddress(8080)); // 自己定义一个 Attachment 类，用于传递一些信息 Attachment att = new Attachment(); att.setServer(server); server.accept(att, new CompletionHandler&lt;AsynchronousSocketChannel, Attachment&gt;() &#123; @Override public void completed(AsynchronousSocketChannel client, Attachment att) &#123; try &#123; SocketAddress clientAddr = client.getRemoteAddress(); System.out.println("收到新的连接：" + clientAddr); // 收到新的连接后，server 应该重新调用 accept 方法等待新的连接进来 att.getServer().accept(att, this); Attachment newAtt = new Attachment(); newAtt.setServer(server); newAtt.setClient(client); newAtt.setReadMode(true); newAtt.setBuffer(ByteBuffer.allocate(2048)); // 这里也可以继续使用匿名实现类，不过代码不好看，所以这里专门定义一个类 client.read(newAtt.getBuffer(), newAtt, new ChannelHandler()); &#125; catch (IOException ex) &#123; ex.printStackTrace(); &#125; &#125; @Override public void failed(Throwable t, Attachment att) &#123; System.out.println("accept failed"); &#125; &#125;); // 为了防止 main 线程退出 try &#123; Thread.currentThread().join(); &#125; catch (InterruptedException e) &#123; &#125; &#125;&#125; 看一下 ChannelHandler 类：12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.javadoop.aio;import java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.CompletionHandler;import java.nio.charset.Charset;public class ChannelHandler implements CompletionHandler&lt;Integer, Attachment&gt; &#123; @Override public void completed(Integer result, Attachment att) &#123; if (att.isReadMode()) &#123; // 读取来自客户端的数据 ByteBuffer buffer = att.getBuffer(); buffer.flip(); byte bytes[] = new byte[buffer.limit()]; buffer.get(bytes); String msg = new String(buffer.array()).toString().trim(); System.out.println("收到来自客户端的数据: " + msg); // 响应客户端请求，返回数据 buffer.clear(); buffer.put("Response from server!".getBytes(Charset.forName("UTF-8"))); att.setReadMode(false); buffer.flip(); // 写数据到客户端也是异步 att.getClient().write(buffer, att, this); &#125; else &#123; // 到这里，说明往客户端写数据也结束了，有以下两种选择: // 1. 继续等待客户端发送新的数据过来// att.setReadMode(true);// att.getBuffer().clear();// att.getClient().read(att.getBuffer(), att, this); // 2. 既然服务端已经返回数据给客户端，断开这次的连接 try &#123; att.getClient().close(); &#125; catch (IOException e) &#123; &#125; &#125; &#125; @Override public void failed(Throwable t, Attachment att) &#123; System.out.println("连接断开"); &#125;&#125; 顺便再贴一下自定义的 Attachment 类：1234567public class Attachment &#123; private AsynchronousServerSocketChannel server; private AsynchronousSocketChannel client; private boolean isReadMode; private ByteBuffer buffer; // getter &amp; setter&#125; 这样，一个简单的服务端就写好了，接下来可以接收客户端请求了。上面我们用的都是回调函数的方式，读者要是感兴趣，可以试试写个使用 Future 的。 AsynchronousSocketChannel其实，说完上面的 AsynchronousServerSocketChannel，基本上读者也就知道怎么使用 AsynchronousSocketChannel 了，和非阻塞 IO 基本类似。 这边做个简单演示，这样读者就可以配合之前介绍的 Server 进行测试使用了。12345678910111213141516171819202122232425262728293031323334package com.javadoop.aio;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.AsynchronousSocketChannel;import java.nio.charset.Charset;import java.util.concurrent.ExecutionException;import java.util.concurrent.Future;public class Client &#123; public static void main(String[] args) throws Exception &#123; AsynchronousSocketChannel client = AsynchronousSocketChannel.open(); // 来个 Future 形式的 Future&lt;?&gt; future = client.connect(new InetSocketAddress(8080)); // 阻塞一下，等待连接成功 future.get(); Attachment att = new Attachment(); att.setClient(client); att.setReadMode(false); att.setBuffer(ByteBuffer.allocate(2048)); byte[] data = "I am obot!".getBytes(); att.getBuffer().put(data); att.getBuffer().flip(); // 异步发送数据到服务端 client.write(att.getBuffer(), att, new ClientChannelHandler()); // 这里休息一下再退出，给出足够的时间处理数据 Thread.sleep(2000); &#125;&#125; 往里面看下 ClientChannelHandler 类：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.javadoop.aio;import java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.CompletionHandler;import java.nio.charset.Charset;public class ClientChannelHandler implements CompletionHandler&lt;Integer, Attachment&gt; &#123; @Override public void completed(Integer result, Attachment att) &#123; ByteBuffer buffer = att.getBuffer(); if (att.isReadMode()) &#123; // 读取来自服务端的数据 buffer.flip(); byte[] bytes = new byte[buffer.limit()]; buffer.get(bytes); String msg = new String(bytes, Charset.forName("UTF-8")); System.out.println("收到来自服务端的响应数据: " + msg); // 接下来，有以下两种选择: // 1. 向服务端发送新的数据// att.setReadMode(false);// buffer.clear();// String newMsg = "new message from client";// byte[] data = newMsg.getBytes(Charset.forName("UTF-8"));// buffer.put(data);// buffer.flip();// att.getClient().write(buffer, att, this); // 2. 关闭连接 try &#123; att.getClient().close(); &#125; catch (IOException e) &#123; &#125; &#125; else &#123; // 写操作完成后，会进到这里 att.setReadMode(true); buffer.clear(); att.getClient().read(buffer, att, this); &#125; &#125; @Override public void failed(Throwable t, Attachment att) &#123; System.out.println("服务器无响应"); &#125;&#125; Asynchronous Channel Groups为了知识的完整性，有必要对 group 进行介绍，其实也就是介绍 AsynchronousChannelGroup 这个类。之前我们说过，异步 IO 一定存在一个线程池，这个线程池负责接收任务、处理 IO 事件、回调等。这个线程池就在 group 内部，group 一旦关闭，那么相应的线程池就会关闭。 AsynchronousServerSocketChannels 和 AsynchronousSocketChannels 是属于 group 的，当我们调用 AsynchronousServerSocketChannel 或 AsynchronousSocketChannel 的 open() 方法的时候，相应的 channel 就属于默认的 group，这个 group 由 JVM 自动构造并管理。 如果我们想要配置这个默认的 group，可以在 JVM 启动参数中指定以下系统变量： java.nio.channels.DefaultThreadPool.threadFactory 此系统变量用于设置 ThreadFactory，它应该是 java.util.concurrent.ThreadFactory 实现类的全限定类名。一旦我们指定了这个 ThreadFactory 以后，group 中的线程就会使用该类产生。 java.nio.channels.DefaultThreadPool.initialSize 此系统变量也很好理解，用于设置线程池的初始大小。 可能你会想要使用自己定义的 group，这样可以对其中的线程进行更多的控制，使用以下几个方法即可： AsynchronousChannelGroup.withCachedThreadPool(ExecutorService executor, int initialSize) AsynchronousChannelGroup.withFixedThreadPool(int nThreads, ThreadFactory threadFactory) AsynchronousChannelGroup.withThreadPool(ExecutorService executor) 至于 group 的使用就很简单了，代码一看就懂：1234AsynchronousChannelGroup group = AsynchronousChannelGroup .withFixedThreadPool(10, Executors.defaultThreadFactory());AsynchronousServerSocketChannel server = AsynchronousServerSocketChannel.open(group);AsynchronousSocketChannel client = AsynchronousSocketChannel.open(group); AsynchronousFileChannels 不属于 group。但是它们也是关联到一个线程池的，如果不指定，会使用系统默认的线程池，如果想要使用指定的线程池，可以在实例化的时候使用以下方法：123456public static AsynchronousFileChannel open(Path file, Set&lt;? extends OpenOption&gt; options, ExecutorService executor, FileAttribute&lt;?&gt;... attrs) &#123; ...&#125; 到这里，异步 IO 就算介绍完成了。 小结我想，本文应该是说清楚了非阻塞 IO 和异步 IO 了，对于异步 IO，由于网上的资料比较少，所以不免篇幅多了些。 我们也要知道，看懂了这些，确实可以学到一些东西，多了解一些知识，但是我们还是很少在工作中将这些知识变成工程代码。一般而言，我们需要在网络应用中使用 NIO 或 AIO 来提升性能，但是，在工程上，绝不是了解了一些概念，知道了一些接口就可以的，需要处理的细节还非常多。 这也是为什么 Netty/Mina 如此盛行的原因，因为它们帮助封装好了很多细节，提供给我们用户友好的接口，后面有时间我也会对 Netty 进行介绍。]]></content>
      <categories>
        <category>NIO</category>
      </categories>
      <tags>
        <tag>NIO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[看似简单但容易忽视的编程常识]]></title>
    <url>%2F2018%2F02%2F25%2F2018-02-25-coding-learning%2F</url>
    <content type="text"><![CDATA[原文链接：http://codebay.cn/post/7865.html 笔记 文章的重点是如何通过封装，抽象，写出更健壮和优雅的代码。举的代码例子都是在工作中经常会碰到的case,一些点确实是我以前写代码的时候没注意到或思考过的。 这些年写了很多的代码、也读过很多的人写的代码，这几年，写代码的机会越来越少，但是每次写代码，感觉需要思考的东西越来越多，好的代码确实难能可贵，在国内业界中，好的软件不少，但是好的代码确实有点凤毛麟角了，写得出来的人不多，有追求的也不多，看到的好的代码越来越少。 可能是因为每个人对于好的评判标准不一，程序员中，也不乏文人相轻的较劲，总觉得比人写的代码都不够好，我不想介入这些无谓的争论，这篇文章中，我将结合我的编码经验，探讨一下，如何写出设计优良的代码，希望作为大家的参考。 好的代码首先是逻辑正确的如何用编程语言表述正确的代码逻辑，这个问题好像很少有人单独拎出来讲，因为这个问题的答案很简单，简单得你都懒得去思考它，因为你肯定觉得，用编程语言正确的表述代码逻辑无非就是if 、while 之类的东西，有什么好探讨的，其实我要分享的并不是这些关键词的本身在逻辑中表达的含义，而是这些关键词的背后，编写程序的过程中，是否真的认真思考过背后的逻辑。我曾不止遇到过很多有多年编程经验的程序员，犯下类似的错误，也见过很多年轻的同学，反复强调纠正后，逻辑上还是会漏洞百出，这几年，我会经常组织我组里面的同学对代码进行走读，总结这些编码中的逻辑错误，很大一部分也是因为编程逻辑背后的思考是不够的。所以我要讲的，是很简单的知识，但是往往是最容易忽略的思考点。 我先给大家看一个例子：1234if(userInfo != null)&#123; //给用户派发一个100单位的优惠券 couponing(userInfo,100);&#125; 这段代码为的目的是判断userInfo不为空串的时候couponing，看起来这段代码非常简单，判断上似乎还算比较严谨，其实这段代码只是看到了眼前要做的事情，但是并没有看到整体逻辑，为什么这么说呢，请看下面几行代码，也许会引发最这个简单问题新的思考。12345678910111213141516171819if(userInfo == null)&#123;//思考尝试恢复不存在的userInfo情况userInfo = fetchUserInfo(userId);&#125;if(userInfo != null)&#123; couponing(userId);&#125;else&#123; //思考userInfo不存在的情况下，是否是符合整体的业务逻辑，如果整体上不符合业务逻辑，应该立刻异常终端程序。 throw new RuntimeException("userInfo not exist.");&#125; 这段代码虽说相比之前的代码长了一些，但是反映出来的是逻辑思考的严谨性，从这两个例子比较我们可以很明显的感觉到，第一段代码的问题，我们看到的只是为了保护是否能做couponing的条件，但是并没有去思考，条件不满足的时候，如何去做，是否有能力去恢复这个错误，确实无法恢复的时候，我们是否还要在错误的道路上越错越远呢，这一点非常重要，也很容易忽略，需要在编码的过程中，进行完整的思考才会意识到这个问题的，如果让错误继续执行下去，直到程序运行到下一个我们不期望的点，如果下一个不期望的点,代码上也遵循这个风格，简单的判断不为null，就跳过执行，这样下去，就会有无穷的隐患，代码整体上看上去，就漏洞百出了。所以从这里要给大家一个建议： 要有一颗勇敢的心，程序不要害怕抛出错误，越害怕，错误越多 我们应该都知道，错误越是早发现越好处理，其实程序在执行过程中也是一样的，越早发现错误，执行中就越容易处理。我一般称这种代码为代码的盲目容错，看上去这行代码很健壮，不会报错，但是不报错，不能影响错误的客观存在性，错会还是会存在的，遇到错误的时候，我们应该首先想到的是恢复这个错误，对容错问题，是需要进行非常深入很全局的思考才能做的决定，盲目的容错，只会让情况变得更加不可控制。 千万不要忘记else的思考，条件有两面性，思考要完整 每当你要用到一个条件表达式的时候，切记要思考这个条件不成立的情况。 尽可能的不要出现只有if 没有else的情况，多组条件用 else if 连接使用，最后再加一个else去做大兜底。 其他的条件表达式类似，比如switch case 最后总有一个值得我们深思default。严谨的代码其实就体现在else上面的思考。 容易造成思考不足的条件语句 有效降低逻辑的复杂度上一节的例子中，肯定会有人觉得这样写代码，是不是觉得太复杂了，已经思考了这些问题，一定要用这么复杂的方式表达出来吗？这是另外一方面的问题，我们要让代码逻辑变得简单，这一节中，我尝试分享一些我如何降低代码复杂度的方法和经验。 还是用上面的例子，我尝试将代码变得更加简单，请看下面的代码，是不是感觉舒服很多。12345userInfo = withDefault(userInfo,fetchUserInfo(userId));Assert.assertNotNull(userInfo,"userInfo not exist.");couponing(userInfo,100); 这段代码中，表达了上面所有的逻辑，而且没有引入分支，其实这里我想强调的是 减少分支就是降低复杂度 我一般的编码思想是，尽可能的不要用分支处理异常，也不要因为异常引入分支，分支的使用场景最好是业务逻辑所需要的，应该用分支尽可能的表达清楚业务逻辑，而尽量不要用分支去适应异常的处理。这里进一步又引入了一个被忽略的尝试。 不要混淆分支和异常的概念 这一点看起来很难做到，但是根据我的实际经验，我们是有办法做到的，通过优雅的定义和处理异常，是可以比较容易的明确异常和业务分支的区别的。不过在本例中，我还是希望能将减少分支的方法说清楚，关于如何优雅的处理和定义异常，本文先不做过多描述。 我想说的是，一个分支，最好是能表达一层业务的含义，用分支标示是分支的条件以及条件成立或不成立的时候，要做的动作。所以，还是基于上面的例子，我们引入一个业务条件，“当用户是VIP用户的时候，我们才能给用户发放优惠券，否则，我们不发放优惠券”，我们分支代码标示如下12345678910111213userInfo = withDefault(userInfo,fetchUserInfo(userId));Assert.assertNotNull(userInfo,"userInfo not exist.");if(userInfo.isVip())&#123; couponing(userInfo,100);&#125; else &#123; return;&#125; 这段代码正常的表述的业务的含义，注意其中的else，这里else 进入之后是直接return的，写上这一句就是上一节中，说明的一样，保证我们的代码逻辑是完整的，这一句有很明确的语义，就是表示条件不成立的时候，我们不做，如果不写的话，其实这部分语义是丢失的或是不明确的。 上面的代码能正常满足当前的业务需求，但是业务是复杂的，比如业务上我们有了新的需求，需要对发放优惠券的规则进行调整，调整会后的规则为，增加白名单可以不是VIP也要发优惠券，或者这个用户的用户UID是以00结尾，所以这时候，我们条件代码成了下面这个样子123456789if(userInfo.isVip()||inWhiteNameList(userInfo)||StringUtil.endWith(userInfo.getUserId(),"00")&#123; couponing(userInfo,100);&#125; else &#123; return;&#125; 这段代码中，我们逻辑一下就变得复杂了，虽说我们只用了一个if else 表达式，但是这里的分支复杂度其实是2的3次方，但是我们处理的情况就是两种，一种是成立，一种是不成立，所以，我们更加关心的是成立或是不成立的情况，而不是所有条件的组合形式，通过观察，我们发现，所有的逻辑都是由“或”进行连接，根据这个特性，其实我们可以提炼出逻辑工具方法，更好的表达我们更加关系的成立或不成立的条件。我们提取一个命名为any的逻辑方法来表述刚才的逻辑，这个方法接收一个不定长的参数，值要有一个为真，则返回为真。其他场景，我们也可以自己峰值其他的逻辑方法，比如all。notAll notAny。 则代码修改为:123456789if(any(userInfo.isVip(),inWhiteNameList(userInfo),StringUtil.endWith(userInfo.getUserId(),"00"))&#123; couponing(userInfo,100);&#125; else &#123; return;&#125; 这段代码有效的减少了代码的分支数量，注意，这里仅仅是从分支数量上进行了减少，增加了一点点可读性，这样做的好处是，多数情况下，我们关注的业务分支的动作本身，而对于进入这个分组形成的的组合情况做所有讨论，所以，这样做，可以有效的降低分支的数量，减少用例的个数（写过单元测试的同学都知道，这样的逻辑要覆盖有多痛苦）。 这一节中，用了一个看上去有些鸡肋的方法去封装逻辑组合，其实，在现在日常生产中，想办法去封装逻辑表达式进行封装是非常有效果的，这里只是举了一个逻辑封装的例子，还有很多其它场景，比如从一个Map中，根据一组key逐个取值，如果取到值不为null，则放入到另外一个Map中，这里其实可以写一个putNotNull的方法来封装逻辑，这种做法非常有效。所以这一节我想给大家传递的一个思想，就是尽你最大的可能，对逻辑表达式进行封装 代码和业务解耦上一节的例子中，大家可以很容易看出来，不管逻辑怎么封装，代码是始终不稳定的，其实这里就引出了我们要强调的一个常识，就是能力要和业务解耦。 如何将能力和业务解耦，我对这个问题的理解是，首先我得把这个能力定义出来，这里我暂且定义为这个能力为发优惠券（其实定义一个能力是最难做的事情，深入的思考，会发现这个问题难到需要重新思考人生，我这里不拉开篇幅讲了，结合这个例子，大家暂且先有一个模糊的理解，后面在慢慢讨论能力定义这个大的课题），有了这个能力定义之后，我们根据这个能力定义做一个面向能力的条件判断，代码示例如下：123456789if(canCouponing(userInfo))&#123; couponing(userInfo,100);&#125; else &#123; return;&#125; 从这几行代码中，可以看出，这里好像已经好了很多，我们将发优惠券的能力和判断条件canCouponing进行耦合，看上去这段代码已经稳定了，但是仔细观察后发现，canCouponing这个方法中依赖了userInfo，这个依赖貌似还是会存在很多问题，因为如果判断条件超出了userInfo的范畴，则这个地方又会变得难以解决，能力判断的要素看起来还是不可控的，为了解决这个问题，我们就要用到运行上下文或是领域模型的概念了，用一个运行时的上下文，作为数据信息载体，承载我们业务执行过程中所需要的模型数据，领域模型的发放则是我们对系统能力和业务有了足够深入理解之后，抽象出来的，能更加准确表述业务属性和行为的模型定义，在没有很好的理解和抽象之前，本节中我们还是先用运行上下文这样相对松散的概念来解决这个问题。根据这个思想，我们将代码进行修改：123456789if(canCouponing(runtimeContext))&#123; couponing(userInfo,100);&#125; else &#123; return;&#125; 在上面代码中，让runtimeContext中包含userInfo，通过一个更松散的对象来传递对象，交给canCouponing这个方法处理，这里也许有人会问，canCouponing这个方法内部还不是一堆逻辑，整体上还是控制不住复杂度。其实这类问题，我们将关键的业务点从硬代码中剥离出来，并且将业务逻辑集中起来进行管理的话，就可以使用规则引擎来处理了。通过规则引擎和专家系统，将这些规则交给业务人员或是运营人员统一进行管理就可以了，而我们的功能性代码可以做到非常的干净和稳定。 也许有另外的人会问，为什么couponing(userInfo,100);这行代码中没有用runtimeContext，而是直接使用的userInfo，在实际编程中，你可能真的需要用到runtimeContext，但是这里的目的是让大家理解如何让业务代码和能力解耦，关于能力本身这块如何更好的设计，这一方面的内容也有很多值得我们思考的，本文暂不做过多探讨。 思考能力的定义，用代码描述能力，将业务从代码中抽出来，交给规则引擎或是专家系统处理]]></content>
      <categories>
        <category>编程技巧</category>
      </categories>
      <tags>
        <tag>编程常识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java7/8 中的 HashMap 和 ConcurrentHashMap 全解析]]></title>
    <url>%2F2018%2F02%2F24%2F2018-02-24-hashmap%2F</url>
    <content type="text"><![CDATA[今天发一篇”水文”，可能很多读者都会表示不理解，不过我想把它作为并发序列文章中不可缺少的一块来介绍。本来以为花不了多少时间的，不过最终还是投入了挺多时间来完成这篇文章的。 网上关于 HashMap 和 ConcurrentHashMap 的文章确实不少，不过缺斤少两的文章比较多，所以才想自己也写一篇，把细节说清楚说透，尤其像 Java8 中的 ConcurrentHashMap，大部分文章都说不清楚。终归是希望能降低大家学习的成本，不希望大家到处找各种不是很靠谱的文章，看完一篇又一篇，可是还是模模糊糊。 阅读建议：四节基本上可以进行独立阅读，建议初学者可按照 Java7 HashMap -&gt; Java7 ConcurrentHashMap -&gt; Java8 HashMap -&gt; Java8 ConcurrentHashMap 顺序进行阅读，可适当降低阅读门槛。 阅读前提：本文分析的是源码，所以至少读者要熟悉它们的接口使用，同时，对于并发，读者至少要知道 CAS、ReentrantLock、UNSAFE 操作这几个基本的知识，文中不会对这些知识进行介绍。Java8 用到了红黑树，不过本文不会进行展开，感兴趣的读者请自行查找相关资料。 Java7 HashMapHashMap 是最简单的，一来我们非常熟悉，二来就是它不支持并发操作，所以源码也非常简单。 首先，我们用下面这张图来介绍 HashMap 的结构。 这个仅仅是示意图，因为没有考虑到数组要扩容的情况，具体的后面再说。 大方向上，HashMap 里面是一个数组，然后数组中每个元素是一个单向链表。 上图中，每个绿色的实体是嵌套类 Entry 的实例，Entry 包含四个属性：key, value, hash 值和用于单向链表的 next。 capacity：当前数组容量，始终保持 2^n，可以扩容，扩容后数组大小为当前的 2 倍。 loadFactor：负载因子，默认为 0.75。 threshold：扩容的阈值，等于 capacity * loadFactor put 过程分析还是比较简单的，跟着代码走一遍吧。1234567891011121314151617181920212223242526272829public V put(K key, V value) &#123; // 当插入第一个元素的时候，需要先初始化数组大小 if (table == EMPTY_TABLE) &#123; inflateTable(threshold); &#125; // 如果 key 为 null，感兴趣的可以往里看，最终会将这个 entry 放到 table[0] 中 if (key == null) return putForNullKey(value); // 1. 求 key 的 hash 值 int hash = hash(key); // 2. 找到对应的数组下标 int i = indexFor(hash, table.length); // 3. 遍历一下对应下标处的链表，看是否有重复的 key 已经存在， // 如果有，直接覆盖，put 方法返回旧值就结束了 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; // 4. 不存在重复的 key，将此 entry 添加到链表中，细节后面说 addEntry(hash, key, value, i); return null;&#125; 数组初始化在第一个元素插入 HashMap 的时候做一次数组的初始化，就是先确定初始的数组大小，并计算数组扩容的阈值。12345678910private void inflateTable(int toSize) &#123; // 保证数组大小一定是 2 的 n 次方。 // 比如这样初始化：new HashMap(20)，那么处理成初始数组大小是 32 int capacity = roundUpToPowerOf2(toSize); // 计算扩容阈值：capacity * loadFactor threshold = (int) Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1); // 算是初始化数组吧 table = new Entry[capacity]; initHashSeedAsNeeded(capacity); //ignore&#125; 这里有一个将数组大小保持为 2 的 n 次方的做法，Java7 和 Java8 的 HashMap 和 ConcurrentHashMap 都有相应的要求，只不过实现的代码稍微有些不同，后面再看到的时候就知道了。 计算具体数组位置这个简单，我们自己也能 YY 一个：使用 key 的 hash 值对数组长度进行取模就可以了。1234static int indexFor(int hash, int length) &#123; // assert Integer.bitCount(length) == 1 : "length must be a non-zero power of 2"; return hash &amp; (length-1);&#125; 这个方法很简单，简单说就是取 hash 值的低 n 位。如在数组长度为 32 的时候，其实取的就是 key 的 hash 值的低 5 位，作为它在数组中的下标位置。 添加节点到链表中找到数组下标后，会先进行 key 判重，如果没有重复，就准备将新值放入到链表的表头。12345678910111213141516171819void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 如果当前 HashMap 大小已经达到了阈值，并且新值要插入的数组位置已经有元素了，那么要扩容 if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; // 扩容，后面会介绍一下 resize(2 * table.length); // 扩容以后，重新计算 hash 值 hash = (null != key) ? hash(key) : 0; // 重新计算扩容后的新的下标 bucketIndex = indexFor(hash, table.length); &#125; // 往下看 createEntry(hash, key, value, bucketIndex);&#125;// 这个很简单，其实就是将新值放到链表的表头，然后 size++void createEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;&#125; 这个方法的主要逻辑就是先判断是否需要扩容，需要的话先扩容，然后再将这个新的数据插入到扩容后的数组的相应位置处的链表的表头。 数组扩容前面我们看到，在插入新值的时候，如果当前的 size 已经达到了阈值，并且要插入的数组位置上已经有元素，那么就会触发扩容，扩容后，数组大小为原来的 2 倍。1234567891011121314void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; // 新的数组 Entry[] newTable = new Entry[newCapacity]; // 将原来数组中的值迁移到新的更大的数组中 transfer(newTable, initHashSeedAsNeeded(newCapacity)); table = newTable; threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);&#125; 扩容就是用一个新的大数组替换原来的小数组，并将原来数组中的值迁移到新的数组中。 由于是双倍扩容，迁移过程中，会将原来 table[i] 中的链表的所有节点，分拆到新的数组的 newTable[i] 和 newTable[i + oldLength] 位置上(若hash值最高位为0，移动到i，若为1，移动到i + oldLength])。如原来数组长度是 16，那么扩容后，原来 table[0] 处的链表中的所有元素会被分配到新数组中 newTable[0] 和 newTable[16] 这两个位置。代码比较简单，这里就不展开了。 get 过程分析相对于 put 过程，get 过程是非常简单的。 根据 key 计算 hash 值。找到相应的数组下标：hash &amp; (length - 1)。遍历该数组位置处的链表，直到找到相等(==或equals)的 key。12345678910111213141516171819202122232425262728public V get(Object key) &#123; // 之前说过，key 为 null 的话，会被放到 table[0]，所以只要遍历下 table[0] 处的链表就可以了 if (key == null) return getForNullKey(); // Entry&lt;K,V&gt; entry = getEntry(key); return null == entry ? null : entry.getValue();&#125;getEntry(key):final Entry&lt;K,V&gt; getEntry(Object key) &#123; if (size == 0) &#123; return null; &#125; int hash = (key == null) ? 0 : hash(key); // 确定数组下标，然后从头开始遍历链表，直到找到为止 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; return null;&#125; Java7 ConcurrentHashMapConcurrentHashMap 和 HashMap 思路是差不多的，但是因为它支持并发操作，所以要复杂一些。 整个 ConcurrentHashMap 由一个个 Segment 组成，Segment 代表”部分“或”一段“的意思，所以很多地方都会将其描述为分段锁。注意，行文中，我很多地方用了“槽”来代表一个 segment。 简单理解就是，ConcurrentHashMap 是一个 Segment 数组，Segment 通过继承 ReentrantLock 来进行加锁，所以每次需要加锁的操作锁住的是一个 segment，这样只要保证每个 Segment 是线程安全的，也就实现了全局的线程安全。 concurrencyLevel：并行级别、并发数、Segment 数，怎么翻译不重要，理解它。默认是 16，也就是说 ConcurrentHashMap 有 16 个 Segments，所以理论上，这个时候，最多可以同时支持 16 个线程并发写，只要它们的操作分别分布在不同的 Segment 上。这个值可以在初始化的时候设置为其他值，但是一旦初始化以后，它是不可以扩容的。 再具体到每个 Segment 内部，其实每个 Segment 很像之前介绍的 HashMap，不过它要保证线程安全，所以处理起来要麻烦些。 初始化initialCapacity：初始容量，这个值指的是整个 ConcurrentHashMap 的初始容量，实际操作的时候需要平均分给每个 Segment。 loadFactor：负载因子，之前我们说了，Segment 数组不可以扩容，所以这个负载因子是给每个 Segment 内部使用的。1234567891011121314151617181920212223242526272829303132333435363738394041424344public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; if (!(loadFactor &gt; 0) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if (concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; // Find power-of-two sizes best matching arguments int sshift = 0; int ssize = 1; // 计算并行级别 ssize，因为要保持并行级别是 2 的 n 次方 while (ssize &lt; concurrencyLevel) &#123; ++sshift; ssize &lt;&lt;= 1; &#125; // 我们这里先不要那么烧脑，用默认值，concurrencyLevel 为 16，sshift 为 4 // 那么计算出 segmentShift 为 28，segmentMask 为 15，后面会用到这两个值 this.segmentShift = 32 - sshift; this.segmentMask = ssize - 1; if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; // initialCapacity 是设置整个 map 初始的大小， // 这里根据 initialCapacity 计算 Segment 数组中每个位置可以分到的大小 // 如 initialCapacity 为 64，那么每个 Segment 或称之为"槽"可以分到 4 个 int c = initialCapacity / ssize; if (c * ssize &lt; initialCapacity) ++c; // 默认 MIN_SEGMENT_TABLE_CAPACITY 是 2，这个值也是有讲究的，因为这样的话，对于具体的槽上， // 插入一个元素不至于扩容，插入第二个的时候才会扩容 int cap = MIN_SEGMENT_TABLE_CAPACITY; while (cap &lt; c) cap &lt;&lt;= 1; // 创建 Segment 数组， // 并创建数组的第一个元素 segment[0] Segment&lt;K,V&gt; s0 = new Segment&lt;K,V&gt;(loadFactor, (int)(cap * loadFactor), (HashEntry&lt;K,V&gt;[])new HashEntry[cap]); Segment&lt;K,V&gt;[] ss = (Segment&lt;K,V&gt;[])new Segment[ssize]; // 往数组写入 segment[0] UNSAFE.putOrderedObject(ss, SBASE, s0); // ordered write of segments[0] this.segments = ss;&#125; 初始化完成，我们得到了一个 Segment 数组。 我们就当是用 new ConcurrentHashMap() 无参构造函数进行初始化的，那么初始化完成后： Segment 数组长度为 16，不可以扩容 Segment[i] 的默认大小为 2，负载因子是 0.75，得出初始阈值为 1.5，也就是以后插入第一个元素不会触发扩容，插入第二个会进行第一次扩容 这里初始化了 segment[0]，其他位置还是 null，至于为什么要初始化 segment[0]，后面的代码会介绍 当前 segmentShift 的值为 32 - 4 = 28，segmentMask 为 16 - 1 = 15，姑且把它们简单翻译为移位数和掩码，这两个值马上就会用到 put 过程分析我们先看 put 的主流程，对于其中的一些关键细节操作，后面会进行详细介绍。123456789101112131415161718public V put(K key, V value) &#123; Segment&lt;K,V&gt; s; if (value == null) throw new NullPointerException(); // 1. 计算 key 的 hash 值 int hash = hash(key); // 2. 根据 hash 值找到 Segment 数组中的位置 j // hash 是 32 位，无符号右移 segmentShift(28) 位，剩下低 4 位， // 然后和 segmentMask(15) 做一次与操作，也就是说 j 是 hash 值的最后 4 位，也就是槽的数组下标 int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; // 刚刚说了，初始化的时候初始化了 segment[0]，但是其他位置还是 null， // ensureSegment(j) 对 segment[j] 进行初始化 if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject // nonvolatile; recheck (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) // in ensureSegment s = ensureSegment(j); // 3. 插入新值到 槽 s 中 return s.put(key, hash, value, false);&#125; 第一层皮很简单，根据 hash 值很快就能找到相应的 Segment，之后就是 Segment 内部的 put 操作了。 Segment 内部是由 数组+链表 组成的。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859final V put(K key, int hash, V value, boolean onlyIfAbsent) &#123; // 在往该 segment 写入前，需要先获取该 segment 的独占锁 // 先看主流程，后面还会具体介绍这部分内容 HashEntry&lt;K,V&gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); V oldValue; try &#123; // 这个是 segment 内部的数组 HashEntry&lt;K,V&gt;[] tab = table; // 再利用 hash 值，求应该放置的数组下标 int index = (tab.length - 1) &amp; hash; // first 是数组该位置处的链表的表头 HashEntry&lt;K,V&gt; first = entryAt(tab, index); // 下面这串 for 循环虽然很长，不过也很好理解，想想该位置没有任何元素和已经存在一个链表这两种情况 for (HashEntry&lt;K,V&gt; e = first;;) &#123; if (e != null) &#123; K k; if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) &#123; oldValue = e.value; if (!onlyIfAbsent) &#123; // 覆盖旧值 e.value = value; ++modCount; &#125; break; &#125; // 继续顺着链表走 e = e.next; &#125; else &#123; // node 到底是不是 null，这个要看获取锁的过程，不过和这里都没有关系。 // 如果不为 null，那就直接将它设置为链表表头；如果是null，初始化并设置为链表表头。 if (node != null) node.setNext(first); else node = new HashEntry&lt;K,V&gt;(hash, key, value, first); int c = count + 1; // 如果超过了该 segment 的阈值，这个 segment 需要扩容 if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) rehash(node); // 扩容后面也会具体分析 else // 没有达到阈值，将 node 放到数组 tab 的 index 位置， // 其实就是将新的节点设置成原链表的表头 setEntryAt(tab, index, node); ++modCount; count = c; oldValue = null; break; &#125; &#125; &#125; finally &#123; // 解锁 unlock(); &#125; return oldValue;&#125; 整体流程还是比较简单的，由于有独占锁的保护，所以 segment 内部的操作并不复杂。至于这里面的并发问题，我们稍后再进行介绍。 到这里 put 操作就结束了，接下来，我们说一说其中几步关键的操作。 初始化槽: ensureSegmentConcurrentHashMap 初始化的时候会初始化第一个槽 segment[0]，对于其他槽来说，在插入第一个值的时候进行初始化。 这里需要考虑并发，因为很可能会有多个线程同时进来初始化同一个槽 segment[k]，不过只要有一个成功了就可以。1234567891011121314151617181920212223242526272829private Segment&lt;K,V&gt; ensureSegment(int k) &#123; final Segment&lt;K,V&gt;[] ss = this.segments; long u = (k &lt;&lt; SSHIFT) + SBASE; // raw offset Segment&lt;K,V&gt; seg; if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; // 这里看到为什么之前要初始化 segment[0] 了， // 使用当前 segment[0] 处的数组长度和负载因子来初始化 segment[k] // 为什么要用“当前”，因为 segment[0] 可能早就扩容过了 Segment&lt;K,V&gt; proto = ss[0]; int cap = proto.table.length; float lf = proto.loadFactor; int threshold = (int)(cap * lf); // 初始化 segment[k] 内部的数组 HashEntry&lt;K,V&gt;[] tab = (HashEntry&lt;K,V&gt;[])new HashEntry[cap]; if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; // 再次检查一遍该槽是否被其他线程初始化了。 Segment&lt;K,V&gt; s = new Segment&lt;K,V&gt;(lf, threshold, tab); // 使用 while 循环，内部用 CAS，当前线程成功设值或其他线程成功设值后，退出 while ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; if (UNSAFE.compareAndSwapObject(ss, u, null, seg = s)) break; &#125; &#125; &#125; return seg;&#125; 总的来说，ensureSegment(int k) 比较简单，对于并发操作使用 CAS 进行控制。 获取写入锁: scanAndLockForPut前面我们看到，在往某个 segment 中 put 的时候，首先会调用 node = tryLock() ? null : scanAndLockForPut(key, hash, value)，也就是说先进行一次 tryLock() 快速获取该 segment 的独占锁，如果失败，那么进入到 scanAndLockForPut 这个方法来获取锁。 下面我们来具体分析这个方法中是怎么控制加锁的。123456789101112131415161718192021222324252627282930313233343536373839private HashEntry&lt;K,V&gt; scanAndLockForPut(K key, int hash, V value) &#123; HashEntry&lt;K,V&gt; first = entryForHash(this, hash); HashEntry&lt;K,V&gt; e = first; HashEntry&lt;K,V&gt; node = null; int retries = -1; // negative while locating node // 循环获取锁 while (!tryLock()) &#123; HashEntry&lt;K,V&gt; f; // to recheck first below if (retries &lt; 0) &#123; if (e == null) &#123; if (node == null) // speculatively create node // 进到这里说明数组该位置的链表是空的，没有任何元素 // 当然，进到这里的另一个原因是 tryLock() 失败，所以该槽存在并发，不一定是该位置 node = new HashEntry&lt;K,V&gt;(hash, key, value, null); retries = 0; &#125; else if (key.equals(e.key)) retries = 0; else // 顺着链表往下走 e = e.next; &#125; // 重试次数如果超过 MAX_SCAN_RETRIES（单核1多核64），那么不抢了，进入到阻塞队列等待锁 // lock() 是阻塞方法，直到获取锁后返回 else if (++retries &gt; MAX_SCAN_RETRIES) &#123; lock(); break; &#125; else if ((retries &amp; 1) == 0 &amp;&amp; // 这个时候是有大问题了，那就是有新的元素进到了链表，成为了新的表头 // 所以这边的策略是，相当于重新走一遍这个 scanAndLockForPut 方法 (f = entryForHash(this, hash)) != first) &#123; e = first = f; // re-traverse if entry changed retries = -1; &#125; &#125; return node;&#125; 这个方法有两个出口，一个是 tryLock() 成功了，循环终止，另一个就是重试次数超过了 MAX_SCAN_RETRIES，进到 lock() 方法，此方法会阻塞等待，直到成功拿到独占锁。 这个方法就是看似复杂，但是其实就是做了一件事，那就是获取该 segment 的独占锁，如果需要的话顺便实例化了一下 node。 扩容: rehash重复一下，segment 数组不能扩容，扩容是 segment 数组某个位置内部的数组 HashEntry[] 进行扩容，扩容后，容量为原来的 2 倍。 首先，我们要回顾一下触发扩容的地方，put 的时候，如果判断该值的插入会导致该 segment 的元素个数超过阈值，那么先进行扩容，再插值，读者这个时候可以回去 put 方法看一眼。 该方法不需要考虑并发，因为到这里的时候，是持有该 segment 的独占锁的。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960// 方法参数上的 node 是这次扩容后，需要添加到新的数组中的数据。private void rehash(HashEntry&lt;K,V&gt; node) &#123; HashEntry&lt;K,V&gt;[] oldTable = table; int oldCapacity = oldTable.length; // 2 倍 int newCapacity = oldCapacity &lt;&lt; 1; threshold = (int)(newCapacity * loadFactor); // 创建新数组 HashEntry&lt;K,V&gt;[] newTable = (HashEntry&lt;K,V&gt;[]) new HashEntry[newCapacity]; // 新的掩码，如从 16 扩容到 32，那么 sizeMask 为 31，对应二进制 ‘000...00011111’ int sizeMask = newCapacity - 1; // 遍历原数组，老套路，将原数组位置 i 处的链表拆分到 新数组位置 i 和 i+oldCap 两个位置 for (int i = 0; i &lt; oldCapacity ; i++) &#123; // e 是链表的第一个元素 HashEntry&lt;K,V&gt; e = oldTable[i]; if (e != null) &#123; HashEntry&lt;K,V&gt; next = e.next; // 计算应该放置在新数组中的位置， // 假设原数组长度为 16，e 在 oldTable[3] 处，那么 idx 只可能是 3 或者是 3 + 16 = 19 int idx = e.hash &amp; sizeMask; if (next == null) // 该位置处只有一个元素，那比较好办 newTable[idx] = e; else &#123; // Reuse consecutive sequence at same slot // e 是链表表头 HashEntry&lt;K,V&gt; lastRun = e; // idx 是当前链表的头结点 e 的新位置 int lastIdx = idx; // 下面这个 for 循环会找到一个 lastRun 节点，这个节点之后的所有元素是将要放到一起的 for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) &#123; int k = last.hash &amp; sizeMask; if (k != lastIdx) &#123; lastIdx = k; lastRun = last; &#125; &#125; // 将 lastRun 及其之后的所有节点组成的这个链表放到 lastIdx 这个位置 newTable[lastIdx] = lastRun; // 下面的操作是处理 lastRun 之前的节点， // 这些节点可能分配在另一个链表中，也可能分配到上面的那个链表中 for (HashEntry&lt;K,V&gt; p = e; p != lastRun; p = p.next) &#123; V v = p.value; int h = p.hash; int k = h &amp; sizeMask; HashEntry&lt;K,V&gt; n = newTable[k]; newTable[k] = new HashEntry&lt;K,V&gt;(h, p.key, v, n); &#125; &#125; &#125; &#125; // 将新来的 node 放到新数组中刚刚的 两个链表之一 的 头部 int nodeIndex = node.hash &amp; sizeMask; // add the new node node.setNext(newTable[nodeIndex]); newTable[nodeIndex] = node; table = newTable;&#125; 这里的扩容比之前的 HashMap 要复杂一些，代码难懂一点。上面有两个挨着的 for 循环，第一个 for 有什么用呢？ 仔细一看发现，如果没有第一个 for 循环，也是可以工作的，但是，这个 for 循环下来，如果 lastRun 的后面还有比较多的节点，那么这次就是值得的。因为我们只需要克隆 lastRun 前面的节点，后面的一串节点跟着 lastRun 走就是了，不需要做任何操作。 我觉得 Doug Lea 的这个想法也是挺有意思的，不过比较坏的情况就是每次 lastRun 都是链表的最后一个元素或者很靠后的元素，那么这次遍历就有点浪费了。不过 Doug Lea 也说了，根据统计，如果使用默认的阈值，大约只有 1/6 的节点需要克隆。 get 过程分析相对于 put 来说，get 真的不要太简单。 计算 hash 值，找到 segment 数组中的具体位置，或我们前面用的“槽” 槽中也是一个数组，根据 hash 找到数组中具体的位置 到这里是链表了，顺着链表进行查找即可 1234567891011121314151617181920public V get(Object key) &#123; Segment&lt;K,V&gt; s; // manually integrate access methods to reduce overhead HashEntry&lt;K,V&gt;[] tab; // 1. hash 值 int h = hash(key); long u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE; // 2. 根据 hash 找到对应的 segment if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(segments, u)) != null &amp;&amp; (tab = s.table) != null) &#123; // 3. 找到segment 内部数组相应位置的链表，遍历 for (HashEntry&lt;K,V&gt; e = (HashEntry&lt;K,V&gt;) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE); e != null; e = e.next) &#123; K k; if ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k))) return e.value; &#125; &#125; return null;&#125; 并发问题分析现在我们已经说完了 put 过程和 get 过程，我们可以看到 get 过程中是没有加锁的，那自然我们就需要去考虑并发问题。 添加节点的操作 put 和删除节点的操作 remove 都是要加 segment 上的独占锁的，所以它们之间自然不会有问题，我们需要考虑的问题就是 get 的时候在同一个 segment 中发生了 put 或 remove 操作。 1、 put 操作的线程安全性。 初始化槽，这个我们之前就说过了，使用了 CAS 来初始化 Segment 中的数组。 添加节点到链表的操作是插入到表头的，所以，如果这个时候 get 操作在链表遍历的过程已经到了中间，是不会影响的。当然，另一个并发问题就是 get 操作在 put 之后，需要保证刚刚插入表头的节点被读取，这个依赖于 setEntryAt 方法中使用的 UNSAFE.putOrderedObject。 扩容。扩容是新创建了数组，然后进行迁移数据，最后面将 newTable 设置给属性 table。所以，如果 get 操作此时也在进行，那么也没关系，如果 get 先行，那么就是在旧的 table 上做查询操作；而 put 先行，那么 put 操作的可见性保证就是 table 使用了 volatile 关键字。 2、 remove 操作的线程安全性。 get 操作需要遍历链表，但是 remove 操作会”破坏”链表。 如果 remove 破坏的节点 get 操作已经过去了，那么这里不存在任何问题。 如果 remove 先破坏了一个节点，分两种情况考虑。 1、如果此节点是头结点，那么需要将头结点的 next 设置为数组该位置的元素，table 虽然使用了 volatile 修饰，但是 volatile 并不能提供数组内部操作的可见性保证，所以源码中使用了 UNSAFE 来操作数组，请看方法 setEntryAt。2、如果要删除的节点不是头结点，它会将要删除节点的后继节点接到前驱节点中，这里的并发保证就是 next 属性是 volatile 的。 Java8 HashMapJava8 对 HashMap 进行了一些修改，最大的不同就是利用了红黑树，所以其由数组+链表+红黑树组成。 根据 Java7 HashMap 的介绍，我们知道，查找的时候，根据 hash 值我们能够快速定位到数组的具体下标，但是之后的话，需要顺着链表一个个比较下去才能找到我们需要的，时间复杂度取决于链表的长度，为 O(n)。 为了降低这部分的开销，在 Java8 中，当链表中的元素超过了 8 个以后，会将链表转换为红黑树，在这些位置进行查找的时候可以降低时间复杂度为 O(logN)。 来一张图简单示意一下吧： 注意，上图是示意图，主要是描述结构，不会达到这个状态的，因为这么多数据的时候早就扩容了。 下面，我们还是用代码来介绍吧，个人感觉，Java8 的源码可读性要差一些，不过精简一些。 Java7 中使用 Entry 来代表每个 HashMap 中的数据节点，Java8 中使用 Node，基本没有区别，都是 key，value，hash 和 next 这四个属性，不过，Node 只能用于链表的情况，红黑树的情况需要使用 TreeNode。 我们根据数组元素中，第一个节点数据类型是 Node 还是 TreeNode 来判断该位置下是链表还是红黑树的。 put 过程分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;// 第三个参数 onlyIfAbsent 如果是 true，那么只有在不存在该 key 时才会进行 put 操作// 第四个参数 evict 我们这里不关心final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 第一次 put 值的时候，会触发下面的 resize()，类似 java7 的第一次 put 也要初始化数组长度 // 第一次 resize 和后续的扩容有些不一样，因为这次是数组从 null 初始化到默认的 16 或自定义的初始容量 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 找到具体的数组下标，如果此位置没有值，那么直接初始化一下 Node 并放置在这个位置就可以了 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123;// 数组该位置有数据 Node&lt;K,V&gt; e; K k; // 首先，判断该位置的第一个数据和我们要插入的数据，key 是不是"相等"，如果是，取出这个节点 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果该节点是代表红黑树的节点，调用红黑树的插值方法，本文不展开说红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 到这里，说明数组该位置上是一个链表 for (int binCount = 0; ; ++binCount) &#123; // 插入到链表的最后面(Java7 是插入到链表的最前面) if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // TREEIFY_THRESHOLD 为 8，所以，如果新插入的值是链表中的第 9 个 // 会触发下面的 treeifyBin，也就是将链表转换为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // 如果在该链表中找到了"相等"的 key(== 或 equals) if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) // 此时 break，那么 e 为链表中[与要插入的新值的 key "相等"]的 node break; p = e; &#125; &#125; // e!=null 说明存在旧值的key与要插入的key"相等" // 对于我们分析的put操作，下面这个 if 其实就是进行 "值覆盖"，然后返回旧值 if (e != null) &#123; V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 如果 HashMap 由于新插入这个值导致 size 已经超过了阈值，需要进行扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 和 Java7 稍微有点不一样的地方就是，Java7 是先扩容后插入新值的，Java8 先插值再扩容，不过这个不重要。 数组扩容resize() 方法用于初始化数组或数组扩容，每次扩容后，容量为原来的 2 倍，并进行数据迁移。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 对应数组扩容 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 将数组大小扩大一倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) // 将阈值扩大一倍 newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // 对应使用 new HashMap(int initialCapacity) 初始化后，第一次 put 的时候 newCap = oldThr; else &#123;// 对应使用 new HashMap() 初始化后，第一次 put 的时候 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; // 用新的数组大小初始化新的数组 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; // 如果是初始化数组，到这里就结束了，返回 newTab 即可 if (oldTab != null) &#123; // 开始遍历原数组，进行数据迁移。 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; // 如果该数组位置上只有单个元素，那就简单了，简单迁移这个元素就可以了 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; // 如果是红黑树，具体我们就不展开了 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // 这块是处理链表的情况， // 需要将此链表拆成两个链表，放到新的数组中，并且保留原来的先后顺序 // loHead、loTail 对应一条链表，hiHead、hiTail 对应另一条链表，代码还是比较简单的 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; // 第一条链表 newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; // 第二条链表的新的位置是 j + oldCap，这个很好理解 newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; get 过程分析相对于 put 来说，get 真的太简单了。 计算 key 的 hash 值，根据 hash 值找到对应数组下标: hash &amp; (length-1) 判断数组该位置处的元素是否刚好就是我们要找的，如果不是，走第三步 判断该元素类型是否是 TreeNode，如果是，用红黑树的方法取数据，如果不是，走第四步 遍历链表，直到找到相等(==或equals)的 key123456789101112131415161718192021222324252627public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 判断第一个节点是不是就是需要的 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; // 判断是否是红黑树 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 链表遍历 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; Java8 ConcurrentHashMapJava7 中实现的 ConcurrentHashMap 说实话还是比较复杂的，Java8 对 ConcurrentHashMap 进行了比较大的改动。建议读者可以参考 Java8 中 HashMap 相对于 Java7 HashMap 的改动，对于 ConcurrentHashMap，Java8 也引入了红黑树。 说实话，Java8 ConcurrentHashMap 源码真心不简单，最难的在于扩容，数据迁移操作不容易看懂。 我们先用一个示意图来描述下其结构：结构上和 Java8 的 HashMap 基本上一样，不过它要保证线程安全性，所以在源码上确实要复杂一些。 初始化1234567891011// 这构造函数里，什么都不干public ConcurrentHashMap() &#123;&#125;public ConcurrentHashMap(int initialCapacity) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(); int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); this.sizeCtl = cap;&#125; 这个初始化方法有点意思，通过提供初始容量，计算了 sizeCtl，sizeCtl = 【 (1.5 * initialCapacity + 1)，然后向上取最近的 2 的 n 次方】。如 initialCapacity 为 10，那么得到 sizeCtl 为 16，如果 initialCapacity 为 11，得到 sizeCtl 为 32。 sizeCtl 这个属性使用的场景很多，不过只要跟着文章的思路来，就不会被它搞晕了。 如果你爱折腾，也可以看下另一个有三个参数的构造方法，这里我就不说了，大部分时候，我们会使用无参构造函数进行实例化，我们也按照这个思路来进行源码分析吧。 put 过程分析仔细地一行一行代码看下去：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091public V put(K key, V value) &#123; return putVal(key, value, false);&#125;final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); // 得到 hash 值 int hash = spread(key.hashCode()); // 用于记录相应链表的长度 int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; // 如果数组"空"，进行数组初始化 if (tab == null || (n = tab.length) == 0) // 初始化数组，后面会详细介绍 tab = initTable(); // 找该 hash 值对应的数组下标，得到第一个节点 f else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; // 如果数组该位置为空， // 用一次 CAS 操作将这个新值放入其中即可，这个 put 操作差不多就结束了，可以拉到最后面了 // 如果 CAS 失败，那就是有并发操作，进到下一个循环就好了 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; // hash 居然可以等于 MOVED，这个需要到后面才能看明白，不过从名字上也能猜到，肯定是因为在扩容 else if ((fh = f.hash) == MOVED) // 帮助数据迁移，这个等到看完数据迁移部分的介绍后，再理解这个就很简单了 tab = helpTransfer(tab, f); else &#123; // 到这里就是说，f 是该位置的头结点，而且不为空 V oldVal = null; // 获取数组该位置的头结点的监视器锁 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; // 头结点的 hash 值大于 0，说明是链表 // 用于累加，记录链表的长度 binCount = 1; // 遍历链表 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; // 如果发现了"相等"的 key，判断是否要进行值覆盖，然后也就可以 break 了 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; // 到了链表的最末端，将这个新值放到链表的最后面 Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; else if (f instanceof TreeBin) &#123; // 红黑树 Node&lt;K,V&gt; p; binCount = 2; // 调用红黑树的插值方法插入新节点 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; // binCount != 0 说明上面在做链表操作 if (binCount != 0) &#123; // 判断是否要将链表转换为红黑树，临界值和 HashMap 一样，也是 8 if (binCount &gt;= TREEIFY_THRESHOLD) // 这个方法和 HashMap 中稍微有一点点不同，那就是它不是一定会进行红黑树转换， // 如果当前数组的长度小于 64，那么会选择进行数组扩容，而不是转换为红黑树 // 具体源码我们就不看了，扩容部分后面说 treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; // addCount(1L, binCount); return null;&#125; put 的主流程看完了，但是至少留下了几个问题，第一个是初始化，第二个是扩容，第三个是帮助数据迁移，这些我们都会在后面进行一一介绍。 初始化数组：initTable这个比较简单，主要就是初始化一个合适大小的数组，然后会设置 sizeCtl。 初始化方法中的并发问题是通过对 sizeCtl 进行一个 CAS 操作来控制的。1234567891011121314151617181920212223242526272829private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; // 初始化的"功劳"被其他线程"抢去"了 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin // CAS 一下，将 sizeCtl 设置为 -1，代表抢到了锁 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if ((tab = table) == null || tab.length == 0) &#123; // DEFAULT_CAPACITY 默认初始容量是 16 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; // 初始化数组，长度为 16 或初始化时提供的长度 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; // 将这个数组赋值给 table，table 是 volatile 的 table = tab = nt; // 如果 n 为 16 的话，那么这里 sc = 12 // 其实就是 0.75 * n sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; // 设置 sizeCtl 为 sc，我们就当是 12 吧 sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; 链表转红黑树: treeifyBin前面我们在 put 源码分析也说过，treeifyBin 不一定就会进行红黑树转换，也可能是仅仅做数组扩容。我们还是进行源码分析吧。123456789101112131415161718192021222324252627282930313233private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n, sc; if (tab != null) &#123; // MIN_TREEIFY_CAPACITY 为 64 // 所以，如果数组长度小于 64 的时候，其实也就是 32 或者 16 或者更小的时候，会进行数组扩容 if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) // 后面我们再详细分析这个方法 tryPresize(n &lt;&lt; 1); // b 是头结点 else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; // 加锁 synchronized (b) &#123; if (tabAt(tab, index) == b) &#123; // 下面就是遍历链表，建立一颗红黑树 TreeNode&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; e = b; e != null; e = e.next) &#123; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; &#125; // 将红黑树设置到数组相应位置中 setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)); &#125; &#125; &#125; &#125;&#125; 扩容：tryPresize如果说 Java8 ConcurrentHashMap 的源码不简单，那么说的就是扩容操作和迁移操作。 这个方法要完完全全看懂还需要看之后的 transfer 方法，读者应该提前知道这点。 这里的扩容也是做翻倍扩容的，扩容后数组容量为原来的 2 倍。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 首先要说明的是，方法参数 size 传进来的时候就已经翻了倍了private final void tryPresize(int size) &#123; // c：size 的 1.5 倍，再加 1，再往上取最近的 2 的 n 次方。 int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); int sc; while ((sc = sizeCtl) &gt;= 0) &#123; Node&lt;K,V&gt;[] tab = table; int n; // 这个 if 分支和之前说的初始化数组的代码基本上是一样的，在这里，我们可以不用管这块代码 if (tab == null || (n = tab.length) == 0) &#123; n = (sc &gt; c) ? sc : c; if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if (table == tab) &#123; @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = nt; sc = n - (n &gt;&gt;&gt; 2); // 0.75 * n &#125; &#125; finally &#123; sizeCtl = sc; &#125; &#125; &#125; else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) break; else if (tab == table) &#123; // 我没看懂 rs 的真正含义是什么，不过也关系不大 int rs = resizeStamp(n); if (sc &lt; 0) &#123; Node&lt;K,V&gt;[] nt; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; // 2. 用 CAS 将 sizeCtl 加 1，然后执行 transfer 方法 // 此时 nextTab 不为 null if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; // 1. 将 sizeCtl 设置为 (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2) // 我是没看懂这个值真正的意义是什么？不过可以计算出来的是，结果是一个比较大的负数 // 调用 transfer 方法，此时 nextTab 参数为 null else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); &#125; &#125;&#125; 这个方法的核心在于 sizeCtl 值的操作，首先将其设置为一个负数，然后执行 transfer(tab, null)，再下一个循环将 sizeCtl 加 1，并执行 transfer(tab, nt)，之后可能是继续 sizeCtl 加 1，并执行 transfer(tab, nt)。 所以，可能的操作就是执行 1 次 transfer(tab, null) + 多次 transfer(tab, nt)，这里怎么结束循环的需要看完 transfer 源码才清楚。 数据迁移：transfer下面这个方法很点长，将原来的 tab 数组的元素迁移到新的 nextTab 数组中。 虽然我们之前说的 tryPresize 方法中多次调用 transfer 不涉及多线程，但是这个 transfer 方法可以在其他地方被调用，典型地，我们之前在说 put 方法的时候就说过了，请往上看 put 方法，是不是有个地方调用了 helpTransfer 方法，helpTransfer 方法会调用 transfer 方法的。 此方法支持多线程执行，外围调用此方法的时候，会保证第一个发起数据迁移的线程，nextTab 参数为 null，之后再调用此方法的时候，nextTab 不会为 null。 阅读源码之前，先要理解并发操作的机制。原数组长度为 n，所以我们有 n 个迁移任务，让每个线程每次负责一个小任务是最简单的，每做完一个任务再检测是否有其他没做完的任务，帮助迁移就可以了，而 Doug Lea 使用了一个 stride，简单理解就是步长，每个线程每次负责迁移其中的一部分，如每次迁移 16 个小任务。所以，我们就需要一个全局的调度者来安排哪个线程执行哪几个任务，这个就是属性 transferIndex 的作用。 第一个发起数据迁移的线程会将 transferIndex 指向原数组最后的位置，然后从后往前的 stride 个任务属于第一个线程，然后将 transferIndex 指向新的位置，再往前的 stride 个任务属于第二个线程，依此类推。当然，这里说的第二个线程不是真的一定指代了第二个线程，也可以是同一个线程，这个读者应该能理解吧。其实就是将一个大的迁移任务分为了一个个任务包。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; // stride 在单核下直接等于 n，多核模式下为 (n&gt;&gt;&gt;3)/NCPU，最小值是 16 // stride 可以理解为”步长“，有 n 个位置是需要进行迁移的， // 将这 n 个任务分为多个任务包，每个任务包有 stride 个任务 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range // 如果 nextTab 为 null，先进行一次初始化 // 前面我们说了，外围会保证第一个发起迁移的线程调用此方法时，参数 nextTab 为 null // 之后参与迁移的线程调用此方法时，nextTab 不会为 null if (nextTab == null) &#123; try &#123; // 容量翻倍 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; // nextTable 是 ConcurrentHashMap 中的属性 nextTable = nextTab; // transferIndex 也是 ConcurrentHashMap 的属性，用于控制迁移的位置 transferIndex = n; &#125; int nextn = nextTab.length; // ForwardingNode 翻译过来就是正在被迁移的 Node // 这个构造方法会生成一个Node，key、value 和 next 都为 null，关键是 hash 为 MOVED // 后面我们会看到，原数组中位置 i 处的节点完成迁移工作后， // 就会将位置 i 处设置为这个 ForwardingNode，用来告诉其他线程该位置已经处理过了 // 所以它其实相当于是一个标志。 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); // advance 指的是做完了一个位置的迁移工作，可以准备做下一个位置的了 boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab /* * 下面这个 for 循环，最难理解的在前面，而要看懂它们，应该先看懂后面的，然后再倒回来看 * */ // i 是位置索引，bound 是边界，注意是从后往前 for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; // 下面这个 while 真的是不好理解 // advance 为 true 表示可以进行下一个位置的迁移了 // 简单理解结局：i 指向了 transferIndex，bound 指向了 transferIndex-stride while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; // 将 transferIndex 值赋给 nextIndex // 这里 transferIndex 一旦小于等于 0，说明原数组的所有位置都有相应的线程去处理了 else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; // 看括号中的代码，nextBound 是这次迁移任务的边界，注意，是从后往前 bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; if (finishing) &#123; // 所有的迁移操作已经完成 nextTable = null; // 将新的 nextTab 赋值给 table 属性，完成迁移 table = nextTab; // 重新计算 sizeCtl：n 是原数组长度，所以 sizeCtl 得出的值将是新数组长度的 0.75 倍 sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; // 之前我们说过，sizeCtl 在迁移前会设置为 (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2 // 然后，每有一个线程参与迁移就会将 sizeCtl 加 1， // 这里使用 CAS 操作对 sizeCtl 进行减 1，代表做完了属于自己的任务 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; // 任务结束，方法退出 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; // 到这里，说明 (sc - 2) == resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT， // 也就是说，所有的迁移任务都做完了，也就会进入到上面的 if(finishing)&#123;&#125; 分支了 finishing = advance = true; i = n; // recheck before commit &#125; &#125; // 如果位置 i 处是空的，没有任何节点，那么放入刚刚初始化的 ForwardingNode ”空节点“ else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); // 该位置处是一个 ForwardingNode，代表该位置已经迁移过了 else if ((fh = f.hash) == MOVED) advance = true; // already processed else &#123; // 对数组该位置处的结点加锁，开始处理数组该位置处的迁移工作 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; // 头结点的 hash 大于 0，说明是链表的 Node 节点 if (fh &gt;= 0) &#123; // 下面这一块和 Java7 中的 ConcurrentHashMap 迁移是差不多的， // 需要将链表一分为二， // 找到原链表中的 lastRun，然后 lastRun 及其之后的节点是一起进行迁移的 // lastRun 之前的节点需要进行克隆，然后分到两个链表中 int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; // 其中的一个链表放在新数组的位置 i setTabAt(nextTab, i, ln); // 另一个链表放在新数组的位置 i+n setTabAt(nextTab, i + n, hn); // 将原数组该位置处设置为 fwd，代表该位置已经处理完毕， // 其他线程一旦看到该位置的 hash 值为 MOVED，就不会进行迁移了 setTabAt(tab, i, fwd); // advance 设置为 true，代表该位置已经迁移完毕 advance = true; &#125; else if (f instanceof TreeBin) &#123; // 红黑树的迁移 TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; // 如果一分为二后，节点数少于 8，那么将红黑树转换回链表 ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; // 将 ln 放置在新数组的位置 i setTabAt(nextTab, i, ln); // 将 hn 放置在新数组的位置 i+n setTabAt(nextTab, i + n, hn); // 将原数组该位置处设置为 fwd，代表该位置已经处理完毕， // 其他线程一旦看到该位置的 hash 值为 MOVED，就不会进行迁移了 setTabAt(tab, i, fwd); // advance 设置为 true，代表该位置已经迁移完毕 advance = true; &#125; &#125; &#125; &#125; &#125;&#125; get 过程分析get 方法从来都是最简单的，这里也不例外： 计算 hash 值 根据 hash 值找到数组对应位置: (n - 1) &amp; h 根据该位置处结点性质进行相应查找 如果该位置为 null，那么直接返回 null 就可以了 如果该位置处的节点刚好就是我们需要的，返回该节点的值即可 如果该位置节点的 hash 值小于 0，说明正在扩容，或者是红黑树，后面我们再介绍 find 方法 如果以上 3 条都不满足，那就是链表，进行遍历比对即可 简单说一句，此方法的大部分内容都很简单，只有正好碰到扩容的情况，ForwardingNode.find(int h, Object k) 稍微复杂一些，不过在了解了数据迁移的过程后，这个也就不难了，所以限于篇幅这里也不展开说了。 总结其实也不是很难嘛，虽然没有像之前的 AQS 和线程池一样一行一行源码进行分析，但还是把所有初学者可能会糊涂的地方都进行了深入的介绍，只要是稍微有点基础的读者，应该是很容易就能看懂 HashMap 和 ConcurrentHashMap 源码了。 看源码不算是目的吧，深入地了解 Doug Lea 的设计思路，我觉得还挺有趣的，大师就是大师，代码写得真的是好啊。 我发现很多人都以为我写博客主要是源码分析，说真的，我对于源码分析没有那么大热情，主要都是为了用源码说事罢了，可能之后的文章还是会有比较多的源码分析成分，大家该怎么看就怎么看吧。 不要脸地自以为本文的质量还是挺高的，信息量比较大，如果你觉得有写得不好的地方，或者说看完本文你还是没看懂它们，那么请提出来~~~ （全文完）]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>java语言</tag>
        <tag>JAVA集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何创建并运行Java线程]]></title>
    <url>%2F2018%2F02%2F19%2F2018-02-19-create-thread%2F</url>
    <content type="text"><![CDATA[Java线程类也是一个object类,它的实例都继承自java.lang.Thread或其子类。 可以用如下方式用java中创建一个线程：1Tread thread = new Thread(); 执行该线程可以调用该线程的start()方法:1thread.start(); 在上面的例子中，我们并没有为线程编写运行代码，因此调用该方法后线程就终止了。 编写线程运行时执行的代码有两种方式：一种是创建Thread子类的一个实例并重写run方法，第二种是创建类的时候实现Runnable接口。接下来我们会具体讲解这两种方法： 创建Thread的子类创建Thread子类的一个实例并重写run方法，run方法会在调用start()方法之后被执行。例子如下：12345public class MyThread extends Thread &#123; public void run()&#123; System.out.println("MyThread running"); &#125;&#125; 可以用如下方式创建并运行上述Thread子类12MyThread myThread = new MyThread();myTread.start(); 一旦线程启动后start方法就会立即返回，而不会等待到run方法执行完毕才返回。就好像run方法是在另外一个cpu上执行一样。当run方法执行后，将会打印出字符串MyThread running。你也可以如下创建一个Thread的匿名子类：123456Thread thread = new Thread()&#123; public void run()&#123; System.out.println("Thread Running"); &#125;&#125;;thread.start(); 当新的线程的run方法执行以后，计算机将会打印出字符串”Thread Running”。 实现Runnable接口第二种编写线程执行代码的方式是新建一个实现了java.lang.Runnable接口的类的实例，实例中的方法可以被线程调用。下面给出例子：12345public class MyRunnable implements Runnable &#123; public void run()&#123; System.out.println("MyRunnable running"); &#125;&#125; 为了使线程能够执行run()方法，需要在Thread类的构造函数中传入 MyRunnable的实例对象。示例如下：12Thread thread = new Thread(new MyRunnable());thread.start(); 当线程运行时，它将会调用实现了Runnable接口的run方法。上例中将会打印出”MyRunnable running”。同样，也可以创建一个实现了Runnable接口的匿名类，如下所示：1234567Runnable myRunnable = new Runnable()&#123; public void run()&#123; System.out.println("Runnable running"); &#125;&#125;Thread thread = new Thread(myRunnable);thread.start(); 创建子类还是实现Runnable接口？对于这两种方式哪种好并没有一个确定的答案，它们都能满足要求。就我个人意见，我更倾向于实现Runnable接口这种方法。因为线程池可以有效的管理实现了Runnable接口的线程，如果线程池满了，新的线程就会排队等候执行，直到线程池空闲出来为止。而如果线程是通过实现Thread子类实现的，这将会复杂一些。 有时我们要同时融合实现Runnable接口和Thread子类两种方式。例如，实现了Thread子类的实例可以执行多个实现了Runnable接口的线程。一个典型的应用就是线程池。 常见错误：调用run()方法而非start()方法创建并运行一个线程所犯的常见错误是调用线程的run()方法而非start()方法，如下所示：12Thread newThread = new Thread(MyRunnable());newThread.run(); //should be start(); 起初你并不会感觉到有什么不妥，因为run()方法的确如你所愿的被调用了。但是，事实上,run()方法并非是由刚创建的新线程所执行的，而是被创建新线程的当前线程所执行了。也就是被执行上面两行代码的线程所执行的。想要让创建的新线程执行run()方法，必须调用新线程的start方法。 线程名当创建一个线程的时候，可以给线程起一个名字。它有助于我们区分不同的线程。例如：如果有多个线程写入System.out，我们就能够通过线程名容易的找出是哪个线程正在输出。例子如下：1234MyRunnable runnable = new MyRunnable();Thread thread = new Thread(runnable, "New Thread");thread.start();System.out.println(thread.getName()); 需要注意的是，因为MyRunnable并非Thread的子类，所以MyRunnable类并没有getName()方法。可以通过以下方式得到当前线程的引用：1Thread.currentThread(); 因此，通过如下代码可以得到当前线程的名字：1String threadName = Thread.currentThread().getName(); 线程代码举例：这里是一个小小的例子。首先输出执行main()方法线程名字。这个线程JVM分配的。然后开启10个线程，命名为1~10。每个线程输出自己的名字后就退出。123456789101112public class ThreadExample &#123; public static void main(String[] args)&#123; System.out.println(Thread.currentThread().getName()); for(int i=0; i&lt;10; i++)&#123; new Thread("" + i)&#123; public void run()&#123; System.out.println("Thread: " + getName() + "running"); &#125; &#125;.start(); &#125; &#125;&#125; 需要注意的是，尽管启动线程的顺序是有序的，但是执行的顺序并非是有序的。也就是说，1号线程并不一定是第一个将自己名字输出到控制台的线程。这是因为线程是并行执行而非顺序的。Jvm和操作系统一起决定了线程的执行顺序，他和线程的启动顺序并非一定是一致的。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 Consistent Hashing]]></title>
    <url>%2F2018%2F02%2F12%2F2017-01-28-consistent-hashing%2F</url>
    <content type="text"><![CDATA[原文链接: http://wsfdl.com/algorithm/2017/01/28/理解一致性哈希.html 笔记 一致性哈希的实现原理原来并不复杂，既解决了哈希热点问题，又没有引入太多的运算。 要了解一致性哈希的起源，解决问题，原理，实现和优化，看这一篇文章就足够了。 简介为了解决分布式 web 中的热点问题，David Karger 于 1997 年提出 一致性哈希(Consistent Hashing)，论文请见 Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web(较难理解)。一致性哈希广泛的运用于多种分布式系统中，如 memcache/redis，gluster file system, zookooper 等。 Swift: Building a Consistent Hashing RingCassandra: Consistent Hashing in CassandraRedis/Memcache: Memcache的一致性hash算法使用…本文主要介绍一致性哈希的原理。 原理去中心与水平扩展以 web 为例，cache 节点(如 Redis, Memcache)广泛用于 web 中，以提升性能。随着 web 规模扩大，单个节点无论是从性能上还是容量上都无法支撑大规模的业务，所以需要用多个节点以提升性能和容量。 在多个节点下，给定某个 object，客户端如何知道它存储在哪个节点呢？最简单的做法是选取某个节点做为元数据服务器，记录每个 object 存放的节点，每次访问该 object 时，客户端首先访问元数据服务器，获取其所存放的节点，最后从该节点获取 object。元数据服务器需要维护所有 object 的位置信息，并且每次访问 object 都需要先访问元数据服务器，如此，元数据服务器容易成为性能和容量的瓶颈，不利于大规模扩展。1234567 1. Fetch +-----------------+client -------------&gt; | Metadata Server | bottleneck Location +-----------------+ 2. Access +-----------------+ -------------&gt; | Cache Server X | Cache Server +-----------------+ 去中心化的系统最大优点之一就是易水平扩展，那是否有办法可以去除上述元数据服务器呢。答案是肯定的，比如计算 object 的 hash 值，然后均匀的分布到各个节点上。 hash(object) mod N其中 N 为节点数目但是如果某个节点宕机，剔除宕机节点后数目为 N - 1，此时映射关系变为：hash(object) mod (N - 1)另外，可能由于负载过高，需要新增一个节点，此时映射关系为：hash(object) mod (N + 1)无论是增加还是减少节点，都有可能会改变映射关系，造成大量请求的 miss。那是否能避免大量的 miss 呢？答案也是肯定的，一致性哈希解决了节点增减造成大量 hash 重定位的问题。 原理与增删节点一致性哈希的原理如下： 将每个节点(node)映射到数值空间 [0, 2^32 - 1]，映射的规则可为 IP、hostname 等。 将每个 object 映射到数值空间 [0, 2^32 - 1]。 对于某个 object，对于所有满足 hash(node) &lt;= hash(object) 的节点，选择 hash(node) 最大的节点存放 * object。如果没有满足上述条件的节点，选择 hash(node) 最小的节点存放该 object，如下图(hash ring)。 当某个节点宕机时，仅有该节点的对象被重哈希到相邻节点上(hash ring delete)。 与此类似，当新增一个节点时，仅有一个节点的部分 object 需要重哈希。 虚节点与平衡性节点的位置是由自身哈希值决定的，它们的分布并非均匀，特别当节点数目很少时，容易造成 object 的分布不均匀，即平衡性低，例如： 为了解决这个问题，我们引入虚拟节点，虚拟节点实际上是物理节点的复制品，一个物理节点包含多个虚拟节点，我们将这些虚拟节点映射到数值空间 [0, 2^32 - 1]，对于某个 object，我们根据上节步骤计算该 object 存放的虚拟节点，进而得出物理节点。如下图共有 2 个物理节点，每个物理节点有三个虚拟机节点。当虚拟节点越多，虚拟节点的位置分布越均匀，相应的，映射到物理节点的 object 数目也越均匀，提高了平衡性。 总结通过一致性哈希，客户端可以在本地计算 object 的存放位置，去除了中心节点，使得系统容易水平扩展，便于按需提升/降低整体的容量和性能，同时避免了每次增删节点造成大量哈希重定位的问题，最大化的减少了数据迁移。为使 object 能够尽可能均衡的分散在各个节点上，一致性哈希引入了虚节点，以提高平衡性。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo常用命令]]></title>
    <url>%2F2018%2F02%2F12%2F2018-02-12-hexo%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server | s More info: Server Generate static files1$ hexo generate | g More info: Generating Deploy to remote sites1$ hexo deploy | d More info: Deployment Gernate and Deploy to remote sites1$ hexo d -g Index Algolia$ export HEXO_ALGOLIA_INDEXING_KEY=7362b84fed21bc83930f198a0927bb82 API key # Use Git Bash set HEXO_ALGOLIA_INDEXING_KEY=Search-Only API key # Use Windows command line$ hexo clean$ hexo algolia]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java NIO：Buffer、Channel 和 Selector]]></title>
    <url>%2F2018%2F02%2F01%2F2018-02-01-java-nio%2F</url>
    <content type="text"><![CDATA[本文将介绍 Java NIO 中三大组件 Buffer、Channel、Selector 的使用。 本来要一起介绍非阻塞 IO 和 JDK7 的异步 IO 的，不过因为之前的文章真的太长了，有点影响读者阅读，所以这里将它们放到另一篇文章中进行介绍。 Buffer一个 Buffer 本质上是内存中的一块，我们可以将数据写入这块内存，之后从这块内存获取数据。 java.nio 定义了以下几个 Buffer 的实现，这个图读者应该也在不少地方见过了吧。 其实核心是最后的 ByteBuffer，前面的一大串类只是包装了一下它而已，我们使用最多的通常也是 ByteBuffer。 我们应该将 Buffer 理解为一个数组，IntBuffer、CharBuffer、DoubleBuffer 等分别对应 int[]、char[]、double[] 等。 MappedByteBuffer 用于实现内存映射文件，也不是本文关注的重点。 我觉得操作 Buffer 和操作数组、类集差不多，只不过大部分时候我们都把它放到了 NIO 的场景里面来使用而已。下面介绍 Buffer 中的几个重要属性和几个重要方法。 position、limit、capacity就像数组有数组容量，每次访问元素要指定下标，Buffer 中也有几个重要属性：position、limit、capacity。 最好理解的当然是 capacity，它代表这个缓冲区的容量，一旦设定就不可以更改。比如 capacity 为 1024 的 IntBuffer，代表其一次可以存放 1024 个 int 类型的值。一旦 Buffer 的容量达到 capacity，需要清空 Buffer，才能重新写入值。 position 和 limit 是变化的，我们分别看下读和写操作下，它们是如何变化的。 position的初始值是 0，每往 Buffer 中写入一个值，position 就自动加 1，代表下一次的写入位置。读操作的时候也是类似的，每读一个值，position 就自动加 1,代表下一次的读入位置。 从写操作模式到读操作模式切换的时候（flip），position 都会归零，这样就可以从头开始读写了。 limit：写操作模式下，limit 代表的是最大能写入的数据，这个时候 limit 等于 capacity。写结束后，切换到读模式，此时的 limit 等于 Buffer 中实际的数据大小，因为 Buffer 不一定被写满了。 初始化 Buffer每个 Buffer 实现类都提供了一个静态方法allocate(int capacity)帮助我们快速实例化一个 Buffer。如：1234ByteBuffer byteBuf = ByteBuffer.allocate(1024);IntBuffer intBuf = IntBuffer.allocate(1024);LongBuffer longBuf = LongBuffer.allocate(1024);// ... 另外，我们经常使用 wrap 方法来初始化一个 Buffer。123public static ByteBuffer wrap(byte[] array) &#123; ...&#125; 填充 Buffer各个 Buffer 类都提供了一些 put 方法用于将数据填充到 Buffer 中，如 ByteBuffer 中的几个 put 方法：12345678// 填充一个 byte 值public abstract ByteBuffer put(byte b);// 在指定位置填充一个 int 值public abstract ByteBuffer put(int index, byte b);// 将一个数组中的值填充进去public final ByteBuffer put(byte[] src) &#123;...&#125;// 指定填充数组的偏移和长度进行填充public ByteBuffer put(byte[] src, int offset, int length) &#123;...&#125; 上述这些方法需要自己控制 Buffer 大小，不能超过 capacity，超过会抛 java.nio.BufferOverflowException 异常。 对于 Buffer 来说，另一个常见的操作中就是，我们要将来自 Channel 的数据填充到 Buffer 中，在系统层面上，这个操作我们称为读操作，因为数据是从外部（文件或网络等）读到内存中。1int num = channel.read(buf); 上述方法会返回从 Channel 中读入到 Buffer 的数据大小。 提取 Buffer 中的值前面介绍了写操作，每写入一个值，position 的值都需要加 1，所以 position 最后会指向最后一次写入的位置的后面一个，如果 Buffer 写满了，那么 position 等于 capacity（position 从 0 开始）。 如果要读 Buffer 中的值，需要切换模式，从写入模式切换到读出模式。注意，通常在说 NIO 的读操作的时候，我们说的是从 Channel 中读数据到 Buffer 中，对应的是对 Buffer 的写入操作，初学者需要理清楚这个。 NIO的读操作(从Channel读数据写到内存)，对应Buffer的写操作。 NIO的写操作(从内存写数据到Channel)，对应Buffer的读操作。 两者刚好相反。 调用 Buffer 的 flip() 方法，可以从写入模式切换到读取模式。其实这个方法也就是设置了一下 position 和 limit 值罢了。123456public final Buffer flip() &#123; limit = position; // 将 limit 设置为实际写入的数据数量 position = 0; // 重置 position 为 0 mark = -1; // mark 之后再说 return this;&#125; 对应写入操作的一系列 put 方法，读操作提供了一系列的 get 方法：123456// 根据 position 来获取数据public abstract byte get();// 获取指定位置的数据public abstract byte get(int index);// 将 Buffer 中的数据写入到数组中public ByteBuffer get(byte[] dst) 附一个经常使用的方法：1new String(buffer.array()).trim(); 当然了，除了将数据从 Buffer 取出来使用，更常见的操作是将我们写入的数据传输到 Channel 中，如通过 FileChannel 将数据写入到文件中，通过 SocketChannel 将数据写入网络发送到远程机器等。对应的，这种操作，我们称之为写操作。1int num = channel.write(buf); 上述方法会返回从 Buffer 中写入到 Channel的数据大小。 mark() &amp; reset()除了 position、limit、capacity 这三个基本的属性外，还有一个常用的属性就是 mark。 mark 用于临时保存 position 的值，每次调用 mark() 方法都会将 mark 设值为当前的 position，便于后续需要的时候使用。1234public final Buffer mark() &#123; mark = position; return this;&#125; 那到底什么时候用呢？考虑以下场景，我们在 position 为 5 的时候，先 mark() 一下，然后继续往下读，读到第 10 的时候，我想重新回到 position 为 5 的地方重新来一遍，那只要调一下 reset() 方法，position 就回到 5 了。1234567public final Buffer reset() &#123; int m = mark; if (m &lt; 0) throw new InvalidMarkException(); position = m; return this;&#125; rewind() &amp; clear() &amp; compact()rewind()：会重置 position 为 0，通常用于重新从头读写 Buffer。12345public final Buffer rewind() &#123; position = 0; mark = -1; return this;&#125; clear()：有点重置 Buffer 的意思，相当于重新实例化了一样。 通常，我们会先填充 Buffer，然后从 Buffer 读取数据，之后我们再重新往里填充新的数据，我们一般在重新填充之前先调用 clear()。clear() 方法并不会将 Buffer 中的数据清空，只不过后续的写入会覆盖掉原来的数据，也就相当于清空了数据了。123456public final Buffer clear() &#123; position = 0; limit = capacity; mark = -1; return this;&#125; compact()：和 clear() 一样的是，它们都是在准备往 Buffer 填充新的数据之前调用,调用这个方法以后，会先处理还没有读取的数据，也就是 position 到 limit 之间的数据（还没有读过的数据），先将这些数据移到最左边，然后在这个基础上再开始写入。很明显，此时 limit 还是等于 capacity，position 指向未读数据的右边。123456789public ByteBuffer compact() &#123; //把未读的数据复制到头部 System.arraycopy(hb, ix(position()), hb, ix(0), remaining()); //设置position为未读元素后一位 position(remaining()); limit(capacity()); discardMark(); return this;&#125; Channel所有的 NIO 操作始于通道，通道是数据来源或数据写入的目的地，主要地，我们将关心 java.nio 包中实现的以下几个 Channel： FileChannel：文件通道，用于文件的读和写 DatagramChannel：用于 UDP 连接的接收和发送 SocketChannel：把它理解为 TCP 连接通道，简单理解就是 TCP 客户端 ServerSocketChannel：TCP 对应的服务端，用于监听某个端口进来的请求 这里不是很理解这些也没关系，后面介绍了代码之后就清晰了。还有，我们最应该关注，也是后面将会重点介绍的是SocketChannel 和 ServerSocketChannel。 Channel 经常翻译为通道，类似 IO 中的流，用于读取和写入。它与前面介绍的 Buffer 打交道，读操作的时候将 Channel 中的数据填充到 Buffer 中，而写操作时将 Buffer 中的数据写入到 Channel 中。 至少读者应该记住一点，这两个方法都是 channel 实例的方法。 FileChannel我想文件操作对于大家来说应该是最熟悉的，不过我们在说 NIO 的时候，其实 FileChannel 并不是关注的重点。而且后面我们说非阻塞的时候会看到，FileChannel 是不支持非阻塞的。 这里算是简单介绍下常用的操作吧，感兴趣的读者瞄一眼就是了。 初始化：12FileInputStream inputStream = new FileInputStream(new File("/data.txt"));FileChannel fileChannel = inputStream.getChannel(); 当然了，我们也可以从 RandomAccessFile#getChannel 来得到 FileChannel。 读取文件内容：12ByteBuffer buffer = ByteBuffer.allocate(1024);int num = fileChannel.read(buffer); 前面我们也说了，所有的 Channel 都是和 Buffer 打交道的。 写入文件内容：12345678ByteBuffer buffer = ByteBuffer.allocate(1024);buffer.put("随机写入一些内容到 Buffer 中".getBytes());// Buffer 切换为读模式buffer.flip();while(buffer.hasRemaining()) &#123; // 将 Buffer 中的内容写入文件 fileChannel.write(buffer);&#125; SocketChannel我们前面说了，我们可以将 SocketChannel 理解成一个 TCP 客户端。虽然这么理解有点狭隘，因为我们在介绍 ServerSocketChannel 的时候会看到另一种使用方式。 打开一个 TCP 连接：1SocketChannel socketChannel = SocketChannel.open(new InetSocketAddress("https://www.baidu.com", 80)); 当然了，上面的这行代码等价于下面的两行：1234// 打开一个通道SocketChannel socketChannel = SocketChannel.open();// 发起连接socketChannel.connect(new InetSocketAddress("https://www.baidu.com", 80)); SocketChannel 的读写和 FileChannel 没什么区别，就是操作缓冲区。1234567// 读取数据socketChannel.read(buffer);// 写入数据到网络连接中while(buffer.hasRemaining()) &#123; socketChannel.write(buffer);&#125; 不要在这里停留太久，先继续往下走。 ServerSocketChannel之前说 SocketChannel 是 TCP 客户端，这里说的 ServerSocketChannel 就是对应的服务端。 ServerSocketChannel 用于监听机器端口，管理从这个端口进来的 TCP 连接。123456789// 实例化ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();// 监听 8080 端口serverSocketChannel.socket().bind(new InetSocketAddress(8080));while (true) &#123; // 一旦有一个 TCP 连接进来，就对应创建一个 SocketChannel 进行处理 SocketChannel socketChannel = serverSocketChannel.accept();&#125; 这里我们可以看到 SocketChannel 的第二个实例化方式 到这里，我们应该能理解 SocketChannel 了，它不仅仅是 TCP 客户端，它代表的是一个网络通道，可读可写。 ServerSocketChannel 不和 Buffer 打交道了，因为它并不实际处理数据，它一旦接收到请求后，实例化 SocketChannel，之后在这个连接通道上的数据传递它就不管了，因为它需要继续监听端口，等待下一个连接。 DatagramChannelUDP 和 TCP 不一样，DatagramChannel 一个类处理了服务端和客户端。 科普一下，UDP 是面向无连接的，不需要和对方握手，不需要通知对方，就可以直接将数据包投出去，至于能不能送达，它是不知道的。 监听端口：123456DatagramChannel channel = DatagramChannel.open();channel.socket().bind(new InetSocketAddress(9090));ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();channel.receive(buf); 发送数据：123456789String newData = "New String to write to file..." + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();buf.put(newData.getBytes());buf.flip();int bytesSent = channel.send(buf, new InetSocketAddress("jenkov.com", 80)); SelectorNIO 三大组件就剩 Selector 了，Selector 建立在非阻塞的基础之上，大家经常听到的多路复用在 Java 世界中指的就是它，用于实现一个线程管理多个 Channel。 读者在这一节不能消化 Selector 也没关系，因为后续在介绍非阻塞 IO 的时候还得说到这个，这里先介绍一些基本的接口操作。 首先，我们开启一个 Selector。你们爱翻译成选择器也好，多路复用器也好。 1Selector selector = Selector.open(); 将 Channel 注册到 Selector 上。前面我们说了，Selector 建立在非阻塞模式之上，所以注册到 Selector 的 Channel 必须要支持非阻塞模式，FileChannel 不支持非阻塞，我们这里讨论最常见的 SocketChannel 和 ServerSocketChannel。 1234// 将通道设置为非阻塞模式，因为默认都是阻塞模式的channel.configureBlocking(false);// 注册SelectionKey key = channel.register(selector, SelectionKey.OP_READ); register 方法的第二个 int 型参数（使用二进制的标记位）用于表明需要监听哪些感兴趣的事件，共以下四种事件： SelectionKey.OP_READ 对应 00000001，通道中有数据可以进行读取 SelectionKey.OP_WRITE 对应 00000100，可以往通道中写入数据 SelectionKey.OP_CONNECT 对应 00001000，成功建立 TCP 连接 SelectionKey.OP_ACCEPT 对应 00010000，接受 TCP 连接 我们可以同时监听一个 Channel 中的发生的多个事件，比如我们要监听 ACCEPT 和 READ 事件，那么指定参数为二进制的 00010001 即十进制数值 17 即可(SelectionKey.OP_ACCEPT|SelectionKey.OP_READ)。 注册方法返回值是 SelectionKey 实例，它包含了 Channel和 Selector 信息，也包括了一个叫做 Interest Set 的信息，即我们设置的我们感兴趣的正在监听的事件集合。 调用 select() 方法获取通道信息。用于判断是否有我们感兴趣的事件已经发生了。 Selector 的操作就是以上 3 步，这里来一个简单的示例，大家看一下就好了。之后在介绍非阻塞 IO 的时候，会演示一份可执行的示例代码。```JavaSelector selector = Selector.open(); channel.configureBlocking(false); SelectionKey key = channel.register(selector, SelectionKey.OP_READ); while(true) { // 判断是否有事件准备好 int readyChannels = selector.select(); if(readyChannels == 0) continue; // 遍历 Set selectedKeys = selector.selectedKeys(); Iterator keyIterator = selectedKeys.iterator(); while(keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if(key.isAcceptable()) { // a connection was accepted by a ServerSocketChannel. } else if (key.isConnectable()) { // a connection was established with a remote server. } else if (key.isReadable()) { // a channel is ready for reading } else if (key.isWritable()) { // a channel is ready for writing } keyIterator.remove(); }} 小结到此为止，介绍了 Buffer、Channel 和 Selector 的常见接口。 Buffer 和数组差不多，它有 position、limit、capacity 几个重要属性。put() 一下数据、flip() 切换到读模式、然后用 get() 获取数据、clear() 一下清空数据、重新回到 put() 写入数据。 Channel 基本上只和 Buffer 打交道，最重要的接口就是 channel.read(buffer) 和 channel.write(buffer)。 Selector 用于实现非阻塞 IO，这里仅仅介绍接口使用，后续请关注非阻塞 IO 的介绍。]]></content>
      <categories>
        <category>NIO</category>
      </categories>
      <tags>
        <tag>NIO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈空指针]]></title>
    <url>%2F2018%2F01%2F31%2F2018-01-31-java-nopointer-exception%2F</url>
    <content type="text"><![CDATA[浅谈空指针翻译整理自： http://www.yegor256.com/2014/05/13/why-null-is-bad.html 笔记 空指针应该是Java开发中最常遇到的异常，我们往往机械式地运用一大堆if判断来避免NPE，但除此之外，还有没有其他方法？ 其中 Optional 是我在本文中学习到的Java8新特性, 非常有意思，值得深入研究。 这是Java中常见的编码:1234567public Employee getByName(String name) &#123; int id = database.find(name); if (id == 0) &#123; return null; &#125; return new Employee(id);&#125; 在我看来，这个方法最大的问题就是选择返回 NULL 对象而不是有意义的对象。空指针（NULL）在面向对象编程中是个很糟糕的的设计，会引发很严重的问题，应该不惜一切代价进行规避。甚至 Tony Hoare 向全世界道歉，忏悔他曾经发明了「空指针」这个玩意(Null References, The Billion Dollar Mistake )。David West 在Object Thinking一书中也有类似的观点。 在程序语言中加入空指针设计，其实并非是经过深思熟虑的结果，而仅仅是因为它很容易实现而已。这个设计是如此的影响深远，以至于后来的编程语言都不假思索的继承了这一设计，这个范围几乎包括了目前业界所有的流行的编程语言。 对许多程序员来说，早就已经习惯了空指针的存在，就像日常生活中的空气和水一样。那么，空指针究竟有什么问题？ 额外的错处处理当然，有经验的编码人员自然会时刻意识到空指针问题的存在，那么有可能在编码的过程中加入额外的判断，就会规避这个问题，例如下面的写法：12345678// this is a terrible design, don&apos;t reuseEmployee employee = dept.getByName(&quot;Jeffrey&amp;quot&quot;);if (employee == null) &#123; System.out.println(&quot;can&apos;t find an employee&quot;); System.exit(-1);&#125; else &#123; employee.transferTo(dept2);&#125; 那么这样的话就会多了额外的流程去处理因为空指针判断而多加的判断。例如，上面的处理逻辑和流程中，如果按照面向对象的思维书写，不考虑空指针的情况，只是简单的：1dept.getByName(&quot;Jeffrey&quot;).transferTo(dept2); 就可以。加入空指针判断以后，程序往往会有额外以及复杂的逻辑判断。 语义不清晰为了更好地理解，getByName 方法中加入异常判断后，那么重新命名这个方法为getByNameOrNullIfNotFound()。这样子从代码可读性的角度上说，从方法名字就可以判断这个方法会返回什么。但是自然而然为了表意清晰就会让方法的名字变得越来越长。 对此，通常有两种替代NULL的方式：第一种方式是使用Null Object design pattern，把Null替换成一个常量:1234567public Employee getByName(String name) &#123; int id = database.find(name); if (id == 0) &#123; return Employee.NOBODY; &#125; return Employee(id);&#125; 第二种方式是使用fail-fast原则，当无法返回对象的时候抛出异常:1234567public Employee getByName(String name) &#123; int id = database.find(name); if (id == 0) &#123; throw new EmployeeNotFoundException(name); &#125; return Employee(id);&#125; 有些时候基于性能考虑，我们不得不使用NULL，例如Map的get()方法，当Map中不存在指定元素时会返回NULL。利用这一特性，当我们很容易实现在Map不存在指定元素时，抛出业务异常的场景。12345Employee employee = employees.get(&quot;Jeffrey&quot;);if (employee == null) &#123; throw new EmployeeNotFoundException();&#125;return employee; 如果我们重构一下Map的get()方法，当指定元素不存在时抛出异常，要实现上述代码功能，我们就要这样做：1234if (!employees.containsKey(&quot;Jeffrey&quot;)) &#123; // first search throw new EmployeeNotFoundException();&#125;return employees.get(&quot;Jeffrey&quot;); // second search 但由于这样需对Map进行两次搜索，性能显然不如第一种方式。 还有其他解决方法吗？参考C++ STP map::find()的实现。可以在Map中引入类似方法，返回Iterator(可是Java设计者暂时并没有这样做)12345Iterator found = Map.search(&quot;Jeffrey&quot;);if (!found.hasNext()) &#123; throw new EmployeeNotFoundException();&#125;return found.next(); 如何最大程度得避免空指针？本篇开头的这个问题一样，空指针的问题可以追溯到计算法发展史时期，同时空指针异常的情况也很多，甚至在程序运行阶段也无法避免空指针的情况。那么，在编码层面，我们需要注意哪些呢？ 确认调用的的每个变量都已经被初始化这点说起来很简单，但事实上随着业务的发展项目代码也会越来越庞大。这时候方法之间调用的关系也会越来越复杂，很难避免使用到的方法都已经明确被初始化。 所以这块单独放在这里，需要我们在编码的实话重点考虑变量存在的可能性，这其实大体上基于自己的实际编码经验。 尽量使用明确的值调用如果已经明确某个变量（常量）的值，那么是可以安全调用它的方法的。例如对比下面的几行代码： String a = null; a.equal(&quot;b&quot;); // 会产生空指针异常 &quot;b&quot;.equal(a); // 推荐的写法 很明显使用常量去做调用这代码会更健壮一些。 尽量避免在函数中返回 NULL当如果在编写方法中考虑返回 NULL，这个时候则需要冷静下是否真的需要这样子做。因为，通常来说会有比返回 NULL 更好的处理方式。 自动装箱需谨慎自动装箱确实为编写程序带来很多方便，但我们在编程时候也不能滥用自动装箱。 比如，下面这个程序依然存在空指针异常隐患： Person jack = new Person(&quot;jack&quot;); int weight = jack.getWeight(); 这种异常在我们使用一些 ORM 框架中会碰到，如果数据库对应的对象并不存在该值，而我们又在类中使用了一个基本类型与之对应，依然就会抛出空指针异常。在这种情况下就尽量使用包装类来对应，并且在使用该值时候先判断是否为空,关于自动装箱、拆箱的问题，可以看看Java中的自动装箱与拆箱 遍历谨防集合为空for (int num : list) { // NPE occur if list is null } 及时验证外部数据在代码运行的过程中，尤其在解析外部数据的时候可能会引发影响不到的问题。例如下面的 Json 数据 {&quot;name&quot;: null, age: 28} 如果不处理完善，直接使用 name 属性也会导致空指针的问题。 使用第三方库加强验证很多第三方的 Common 库都会有验证空指针的方法，例如 Guava 中针对空指针的判断有个单独的包去处理。 Optional&lt;Integer&gt; possible = Optional.of(5); possible.isPresent(); // returns true possible.get(); // returns 5 或者过滤 NULL 也会更加的方便 Joiner joiner = Joiner.on(&quot;;&quot;&quot;).skipNulls(); return joiner.join(&quot;Harry&quot;, null, &quot;Ron&quot;, &quot;Hermione&quot;); 使用 @NotNull 或 @Nullable 注解强烈建议多使用注解来增加代码的可读性，例如多增加 @NotNull 或 @Nullable 注解，也可以加强代码静态检查方面可能会造成空指针的可能性，具体可以参见这里 。 使用 Java8 的 Optional很多「现代」语言都会有针对变量为空的可选链式判断，例如 Grovvy 语言有一个 ?. 的操作符，可以安全地处理潜在可能的空引用（据说 Java7 曾被建议引入这个但是并没有发布）。它是这么用的： String version = computer?.getSoundcard()?.getUSB()?.getVersion(); 虽然 Java 看起来非常的保守，但好在 Java8 中增加了 Option[T] 这个对象包来代表类型 T 的某一个值存在或者没有。 那么上面的代码可以写成 String name = computer.flatMap(Computer::getSoundcard) .flatMap(Soundcard::getUSB) .map(USB::getVersion) .orElse(&quot;UNKNOWN&quot;); 看起来似乎有点麻烦，但相信我你会爱上这样的写法，具体可以参见这里。 参考资源 https://github.com/google/guava/wiki/UsingAndAvoidingNullExplained https://stackoverflow.com/questions/271526/avoiding-null-statements https://www.jianshu.com/p/e216330e0a89 http://www.deadcoderising.com/java-8-no-more-loops/ http://www.yegor256.com/2014/05/13/why-null-is-bad.html]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>Java语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单例模式的安全性]]></title>
    <url>%2F2018%2F01%2F08%2F2018-01-08-singleton-not-single%2F</url>
    <content type="text"><![CDATA[单例模式，我想大家再熟悉不过了，不过本文不是介绍单例模式该怎么写的。来了解单例怎么写的，请移步。 本文来说说怎么破坏一个单例，让你写的单例变成一个假的单例。当然，本文也会给出怎么进行防守的方法。 一个简单的单例来一个简单的单例模式例子：1234567891011121314151617public class Singleton &#123; private static final Singleton INSTANCE = new Singleton(); private String name; public String getName() &#123; return this.name; &#125; private Singleton() &#123; this.name = "Neo"; &#125; public static Singleton getInstance() &#123; return INSTANCE; &#125;&#125; 上面是一个比较简单的饿汉写法的单例模式，我们看看客户端调用：1234567public class APP &#123; // 由于构造方法上加了 private 修饰，所以我们已经不能通过 ‘new’ 来产生实例了 // Singleton intance = new Singleton(); Singleton instance = Singleton.getInstance(); System.out.println(instance.getName());&#125; 通过反射破坏单例原理很简单，通过反射获取其构造方法，然后重新生成一个实例。12345678910111213class APP &#123; public static void main(String[] args) throws Exception &#123; Singleton instance1 = Singleton.getInstance(); // 下面我们通过反射得到其构造方法，并且修改其构造方法的访问权限，并用这个构造方法构造一个对象 Constructor constructor = Singleton.class.getDeclaredConstructor(); constructor.setAccessible(true); Singleton instance2 = (Singleton) constructor.newInstance(); // 是不是产生了两个实例了？ System.out.println(instance1 == instance2); // false &#125;&#125; 显然，说好的单例已经不单一了，上面的程序运行结果肯定是：false 防止反射方式破坏如果要避免单例被反射破坏，Java 提供了枚举，举个例子：12345678910111213141516public enum Singleton &#123; INSTANCE;// 这里只有一项 private String name; Singleton() &#123; this.name = "Neo"; &#125; public static Singleton getInstance() &#123; return INSTANCE; &#125; public String getName() &#123; return this.name; &#125;&#125; 这个时候，如果我们再想通过反射获取类的构造方法：1Constructor constructor = Singleton.class.getDeclaredConstructor(); 会抛出 NoSuchMethodException 异常：1234Exception in thread &quot;main&quot; java.lang.NoSuchMethodException: com.javadoop.Singleton.&lt;init&gt;() at java.lang.Class.getConstructor0(Class.java:3082) at java.lang.Class.getDeclaredConstructor(Class.java:2178) at com.javadoop.singleton.APP.main(APP.java:11) 对于枚举，JVM 会自动进行实例的创建，其构造方法由 JVM 在创建实例的时候进行调用。 我们在代码中是获取不到 enum 类的构造方法的。 通过序列化破坏下面，我们再说说另一种破坏方法：序列化、反序列化。 我们知道，序列化是将 java 对象转换为字节流，反序列化是从字节流转换为 java 对象。1234567891011121314151617181920212223242526272829class APP &#123; public static void main(String[] args) throws ... &#123; Singleton instance1 = Singleton.getInstance(); Constructor constructor = Singleton.class.getDeclaredConstructor(); constructor.setAccessible(true); Singleton instance2 = (Singleton) constructor.newInstance(); // instance3 将从 instance1 序列化后，反序列化而来 Singleton instance3 = null; ByteArrayOutputStream bout = null; ObjectOutputStream out = null; try &#123; bout = new ByteArrayOutputStream(); out = new ObjectOutputStream(bout); out.writeObject(instance1); ByteArrayInputStream bin = new ByteArrayInputStream(bout.toByteArray()); ObjectInputStream in = new ObjectInputStream(bin); instance3 = (Singleton) in.readObject(); &#125; catch (Exception e) &#123; &#125; finally &#123; // close bout&amp;out &#125; // 显然，instance3 和 instance1 不是同一个对象了 System.out.println(instance1 == instance3); // false &#125;&#125; 毫无疑问，instance1 == instance3 也会返回 false。 防止序列化破坏在序列化之前，我们要在类上面加上 implements Serializable。 我们需要做的是，在类中加上 readResolve() 这个方法，返回实例。12345678910111213141516171819202122public class Singleton implements Serializable &#123; private static final Singleton INSTANCE = new Singleton(); private String name; public String getName() &#123; return this.name; &#125; private Singleton() &#123; this.name = "Neo"; &#125; public static Singleton getInstance() &#123; return INSTANCE; &#125; // 看这里 public Object readResolve() throws ObjectStreamException &#123; return INSTANCE; &#125;&#125; 你再试一下，会发现变成 true 了。 因为在反序列化的时候，JVM 会自动调用 readResolve() 这个方法，我们可以在这个方法中替换掉从流中反序列化回来的对象。 更具体的信息，请参考官方文档： 对象序列化规范 ，这个方法完整的描述是这样的：1ANY-ACCESS-MODIFIER Object readResolve() throws ObjectStreamException;]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>单例模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 中无处不在的 Properties]]></title>
    <url>%2F2018%2F01%2F03%2F2018-01-03-spring-properties%2F</url>
    <content type="text"><![CDATA[原文链接: https://javadoop.com/post/spring-properties 对 Spring 里面的 Properties 不理解的开发者可能会觉得有点乱，主要是因为配置方式很多种，使用方式也很多种。 本文不是原理分析、源码分析文章，只是希望可以帮助读者更好地理解和使用 Spring Properties。 Properties 的使用先来看看 Properties 是怎么使用的，Spring 中常用的有以下几种使用方式： 1. 在 xml 配置文件中使用即自动替换 ${} 里面的值。123&lt;bean id="xxx" class="com.javadoop.Xxx"&gt; &lt;property name="url" value="$&#123;javadoop.jdbc.url&#125;" /&gt;&lt;/bean&gt; 2. 通过 @Value 注入使用12@Value("$&#123;javadoop.jdbc.url&#125;")private String url; 3. 通过 Environment 获取此法有需要注意的地方。并不是所有的配置方式都支持通过 Environment 接口来获取属性值，亲测只有使用注解 @PropertySource 的时候可以用，否则会得到 null，至于怎么配置，下面马上就会说。123456@Autowiredprivate Environment env;public String getUrl() &#123; return env.getProperty("javadoop.jdbc.url");&#125; 如果是 Spring Boot 的 application.properties 注册的，那也是可以的。 Properties 配置前面我们说了怎么使用我们配置的 Properties，那么该怎么配置呢？Spring 提供了很多种配置方式。 1. 通过 xml 配置下面这个是最常用的配置方式了，很多项目都是这么写的：1&lt;context:property-placeholder location="classpath:sys.properties" /&gt; 2. 通过 @PropertySource 配置前面的通过 xml 配置非常常用，但是如果你也有一种要消灭所有 xml 配置文件的冲动的话，你应该使用以下方式：1234567@PropertySource("classpath:config.properties")@Configurationpublic class JavaDoopConfig &#123; @Value("$&#123;demo.key&#125;") private String key;&#125; 3. PropertyPlaceholderConfigurer如果读者见过这个，也不必觉得奇怪，在 Spring 3.1 之前，经常就是这么使用的：123456789&lt;bean class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"&gt; &lt;property name="locations"&gt; &lt;list&gt; &lt;value&gt;classpath:sys.properties&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name="ignoreUnresolvablePlaceholders" value="true"/&gt; &lt;!-- 这里可以配置一些属性 --&gt;&lt;/bean&gt; 当然，我们也可以用相应的 java configuration 的版本：12345678@Beanpublic PropertyPlaceholderConfigurer propertiess() &#123; PropertyPlaceholderConfigurer ppc = new PropertyPlaceholderConfigurer(); Resource[] resources = new ClassPathResource[]&#123;new ClassPathResource("sys.properties")&#125;; ppc.setLocations(resources); ppc.setIgnoreUnresolvablePlaceholders(true); return ppc;&#125; 4. PropertySourcesPlaceholderConfigurer到了 Spring 3.1 的时候，引入了 PropertySourcesPlaceholderConfigurer，这是一个新的类，注意看和之前的 PropertyPlaceholderConfigurer 在名字上多了一个 Sources，所属的包也不一样，它在 Spring-Context 包中。 在配置上倒是没有什么区别：123456789&lt;bean class="org.springframework.context.support.PropertySourcesPlaceholderConfigurer"&gt; &lt;property name="locations"&gt; &lt;list&gt; &lt;value&gt;classpath:sys.properties&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name="ignoreUnresolvablePlaceholders" value="true"/&gt; &lt;!-- 这里可以配置一些属性 --&gt;&lt;/bean&gt; 也来一个 java configuration 版本吧：12345678@Beanpublic PropertySourcesPlaceholderConfigurer properties() &#123; PropertySourcesPlaceholderConfigurer pspc = new PropertySourcesPlaceholderConfigurer(); Resource[] resources = new ClassPathResource[]&#123;new ClassPathResource("sys.properties")&#125;; pspc.setLocations(resources); pspc.setIgnoreUnresolvablePlaceholders(true); return pspc;&#125; Spring Boot 相关Spring Boot 真的是好东西，开箱即用的感觉实在是太好了。这里简单介绍下相关的内容。 快速生成一个 Spring Boot 项目：https://start.spring.io/ application.properties我们每个项目都默认有一个 application.properties 文件，这个配置文件不需要像前面说的那样进行注册，Spring Boot 会帮我们自动注册。 当然，也许你想换个名字也是可以的，在启动的时候指定你的文件名字就可以了： java -Dspring.config.location=classpath:sys.properties -jar app.jar application-{env}.properties为了给不同的环境指定不同的配置，我们会用到这个。 比如测试环境和生产环境的数据库连接信息就不一样。 所以，在 application.properties 的基础上，我们还需要新建 application-dev.properties 和 application-prd.properties，用于配置环境相关的信息，然后启动的时候指定环境。 java -Dspring.profiles.active=prd -jar app.jar 结果就是，application.properties 和 application-prd.properties 两个文件中的配置都会注册进去，如果有重复的 key，application-prd.properties 文件中的优先级较高。 @ConfigurationProperties这个注解是 Spring Boot 中才有的。 即使大家不使用这个注解，大家也可能会在开源项目中看到这个，这里简单介绍下。 来一个例子直观一些。按照之前说的，在配置文件中填入下面的信息，你可以选择写入 application.properties 也可以用第一节介绍的方法。123javadoop.database.url=jdbc:mysql:javadoop.database.username=adminjavadoop.database.password=admin123456 java 文件：12345678@Configuration@ConfigurationProperties(prefix = "javadoop.database")public class DataBase &#123; String url; String username; String password; // getters and setters&#125; 这样，就在 Spring 的容器中就自动注册了一个类型为 DataBase 的 bean 了，而且属性都已经 set 好了。 在启动过程中动态修改属性值这个我觉得都不需要太多介绍，用 Spring Boot 的应该基本上都知道。 属性配置有个覆盖顺序，也就是当出现相同的 key 的时候，以哪里的值为准。 启动参数 &gt; application-{env}.properties &gt; application.properties 启动参数动态设置属性： java -Djavadoop.database.password=admin4321 -jar app.jar另外，还可以利用系统环境变量设置属性，还可以指定随机数等等，确实很灵活，不过没什么用，就不介绍了。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MySQL的or/in/union与索引优化]]></title>
    <url>%2F2017%2F12%2F25%2F2017-12-25-mysql%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 总是在灾难发生后，才想起容灾的重要性。总是在吃过亏后，才记得曾经有人提醒过。 一，核心军规 不在数据库做计算，cpu计算务必移至业务层 控制单表数据量，单表记录控制在千万级 控制列数量，字段数控制在20以内 平衡范式与冗余，为提高效率可以牺牲范式设计，冗余数据 拒绝3B(big)，大sql，大事务，大批量 二，字段类军规 用好数值类型tinyint(1Byte)smallint(2Byte)mediumint(3Byte)int(4Byte)bigint(8Byte)bad case：int(1)/int(11) 有些字符转化为数字用int而不是char(15)存储ip优先使用enum或set例如：sex enum (‘F’, ‘M’) 避免使用NULL字段NULL字段很难查询优化NULL字段的索引需要额外空间NULL字段的复合索引无效bad case：age int default nullgood case：age int not null default 0 不在数据库里存图片 三，索引类军规 谨慎合理使用索引改善查询、减慢更新索引一定不是越多越好（能不加就不加，要加的一定得加）覆盖记录条数过多不适合建索引，例如“性别” 字符字段必须建前缀索引 不在索引做列运算bad case：select id where age +1 = 10; innodb主键合理使用自增列主键建立聚簇索引主键不应该被修改字符串不应该做主键如果不指定主键，innodb会使用唯一且非空值索引代替 不用外键，请由程序保证约束 四，sql类军规 sql语句尽可能简单一条sql只能在一个cpu运算大语句拆小语句，减少锁时间一条大sql可以堵死整个库简单的事务 事务时间尽可能短bad case：上传图片事务避免使用触发器，用户自定义函数，请由程序取而代之 不用select *消耗cpu，io，内存，带宽这种程序不具有扩展性 OR改写为IN() OR改写为UNION (画外音：最新的mysql内核已经进行了相关优化) imit高效分页limit越大，效率越低select id from t limit 10000, 10;应该改为 =&gt;select id from t where id &gt; 10000 limit 10; 使用union all替代union，union有去重开销 尽量不用连接join 务必请使用“同类型”进行比较，否则可能全表扫面 打散批量更新 使用新能分析工具show profile;mysqlsla;mysqldumpslow;explain;show slow log;show processlist;show query_response_time(percona)]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单机环境下优雅地使用事件驱动进行代码解耦]]></title>
    <url>%2F2017%2F12%2F20%2F2017-12-20-guava-eventbus%2F</url>
    <content type="text"><![CDATA[虽然现在的各种应用都是集群部署，单机部署的应用越来越少了，但是不可否认的是，市场上还是存在许多单机应用的。本文要介绍的是 Guava 中的 EventBus 的使用。 EventBus 处理的事情类似观察者模式，基于事件驱动，观察者们监听自己感兴趣的特定事件，进行相应的处理。 本文想要介绍的内容是，在 Spring 环境中优雅地使用 Guava 包中的 EventBus，对我们的代码进行一定程度的解耦。当然，本文不介绍 EventBus 的原理，我所说的优雅也只是我觉得优雅，也许读者有更漂亮的代码，欢迎在评论区留言。 Step 0：添加 Guava 依赖12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;22.0&lt;/version&gt;&lt;/dependency&gt; 作为 java 程序员，如果你还没有使用过 Google Guava，请从现在开始将它加到你的每一个项目中。 Step 1：定义一个注解用于标记 listener1234567/** * 用于标记 listener */@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface EventBusListener &#123;&#125; Step 2：定义注册中心1234567891011121314151617181920212223242526272829303132333435363738package com.javadoop.eventbus;import java.util.List;import java.util.concurrent.Executors;import javax.annotation.PostConstruct;import org.springframework.stereotype.Component;import com.google.common.eventbus.AsyncEventBus;import com.google.common.eventbus.EventBus;import com.hongjiev.javadoop.util.SpringContextUtils;@Componentpublic class EventBusCenter &#123; // 管理同步事件 private EventBus syncEventBus = new EventBus(); // 管理异步事件 private AsyncEventBus asyncEventBus = new AsyncEventBus(Executors.newCachedThreadPool()); public void postSync(Object event) &#123; syncEventBus.post(event); &#125; public void postAsync(Object event) &#123; asyncEventBus.post(event); &#125; @PostConstruct public void init() &#123; // 获取所有带有 @EventBusListener 的 bean，将他们注册为监听者 List&lt;Object&gt; listeners = SpringContextUtils.getBeansWithAnnotation(EventBusListener.class); for (Object listener : listeners) &#123; asyncEventBus.register(listener); syncEventBus.register(listener); &#125; &#125;&#125; Step 3：定义各种事件举个例子，我们定义一个订单创建事件：1234567891011package com.javadoop.eventbus.event;public class OrderCreatedEvent &#123; private long orderId; private long userId; public OrderCreatedEvent(long orderId, long userId) &#123; this.setOrderId(orderId); this.setUserId(userId); &#125; // getter、setter&#125; Step 4：定义事件监听器首先，类上面需要加我们之前定义的注解：@EventBusListener，然后监听方法需要加注解 @Subscribe，方法参数为具体事件。1234567891011121314151617181920212223242526package com.javadoop.eventbus.listener;import org.springframework.stereotype.Component;import com.google.common.eventbus.Subscribe;import com.javadoop.eventbus.EventBusListener;import com.javadoop.eventbus.event.OrderCreatedEvent;@Component@EventBusListenerpublic class OrderChangeListener &#123; @Subscribe public void created(OrderCreatedEvent event) &#123; long orderId = event.getOrderId(); long userId = event.getUserId(); // 订单创建成功后的各种操作，如发短信、发邮件等等。 // 注意，事件可以被订阅多次，也就是说可以有很多方法监听 OrderCreatedEvent 事件， // 所以没必要在一个方法中处理发短信、发邮件、更新库存等 &#125; @Subscribe public void change(OrderChangeEvent event) &#123; // 处理订单变化后的修改 // 如发送提醒、更新物流等 &#125;&#125; Step 5：发送事件12345678910111213@Servicepublic class OrderService &#123; @Autowired private EventBusCenter eventBusCenter; public void createOrder() &#123; // 处理创建订单 // ... // 发送异步事件 eventBusCenter.postAsync(new OrderCreatedEvent(1L, 1L)); &#125;&#125; 总结EventBus 的好处在于，它将发生事件的代码和事件处理的代码进行了解耦。 比如系统中很多地方都会修改订单，用户可以自己修改、客服也可以修改、甚至可能是团购没成团系统进行的订单修改，所有这些触发订单修改的地方都要发短信、发邮件，假设以后还要增加其他操作，那么需要修改的地方就比较多。 而如果采用事件驱动的话，只要这些地方抛出事件就可以了，后续的维护是比较简单的。 而且，EventBus 支持同步事件和异步事件，可以满足我们不同场景下的需求。比如发短信，系统完全没必要等在那边，完全是可以异步做的。 附录：SpringContextUtils上面的代码使用到了 SpringContextUtils，我想大部分的 Spring 应用都会写这么一个工具类来从 Spring 容器中获取 Bean，用于一些不方便采用注入的地方。1234567891011121314151617181920212223242526272829303132333435363738@Componentpublic class SpringContextUtils implements BeanFactoryPostProcessor &#123; private static ConfigurableListableBeanFactory beanFactory; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory configurableListableBeanFactory) throws BeansException &#123; SpringContextUtils.beanFactory = configurableListableBeanFactory; &#125; public static &lt;T&gt; T getBean(String name) throws BeansException &#123; return (T) beanFactory.getBean(name); &#125; public static &lt;T&gt; T getBean(Class&lt;T&gt; clz) throws BeansException &#123; T result = beanFactory.getBean(clz); return result; &#125; public static &lt;T&gt; List&lt;T&gt; getBeansOfType(Class&lt;T&gt; type) &#123; return beanFactory.getBeansOfType(type).entrySet().stream().map(entry-&gt;entry.getValue()).collect(Collectors.toList()); &#125; // 上面的例子用到了这个 public static List&lt;Object&gt; getBeansWithAnnotation(Class&lt;? extends Annotation&gt; annotationType) &#123; Map&lt;String, Object&gt; beansWithAnnotation = beanFactory.getBeansWithAnnotation(annotationType); // java 8 的写法，将 map 的 value 收集起来到一个 list 中 return beansWithAnnotation.entrySet().stream().map(entry-&gt;entry.getValue()).collect(Collectors.toList()); // java 7 List&lt;Object&gt; result = new ArrayList&lt;&gt;(); for (Map.Entry&lt;String, Object&gt; entry : beansWithAnnotation.entrySet()) &#123; result.add(entry.getValue()); &#125; return result; &#125;&#125;]]></content>
      <categories>
        <category>Guava</category>
      </categories>
      <tags>
        <tag>Guava</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring IOC 容器源码分析]]></title>
    <url>%2F2017%2F11%2F15%2F2017-11-15-spring-ioc%2F</url>
    <content type="text"><![CDATA[Spring 最重要的概念是 IOC 和 AOP，本篇文章其实就是要带领大家来分析下 Spring 的 IOC 容器。既然大家平时都要用到 Spring，怎么可以不好好了解 Spring 呢？阅读本文并不能让你成为 Spring 专家，不过一定有助于大家理解 Spring 的很多概念，帮助大家排查应用中和 Spring 相关的一些问题。 阅读建议：读者至少需要知道怎么配置 Spring，了解 Spring 中的各种概念，少部分内容我还假设读者使用过 SpringMVC。本文要说的 IOC 总体来说有两处地方最重要，一个是创建 Bean 容器，一个是初始化 Bean，如果读者觉得一次性看完本文压力有点大，那么可以按这个思路分两次消化。读者不一定对 Spring 容器的源码感兴趣，也许附录部分介绍的知识对读者有些许作用。 我采用的源码版本是 4.3.11.RELEASE，算是 5.0.x 前比较新的版本了。为了降低难度，本文所说的所有的内容都是基于 xml 的配置的方式，实际使用已经很少人这么做了，至少不是纯 xml 配置，不过从理解源码的角度来看用这种方式来说无疑是最合适的。如果读者对注解方式的源码感兴趣，也许等我有时间的时候可以写篇文章介绍介绍。 我希望能将此文写成一篇 Spring IOC 源码分析的好文章，希望通过本文可以让读者不惧怕阅读 Spring 源码。 为了保持文章的严谨性，如果读者发现我哪里说错了请一定不吝指出，非常希望可以听到读者的声音。 引言先看下最基本的启动 Spring 容器的例子：123public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext("classpath:applicationfile.xml");&#125; 以上代码就可以利用配置文件来启动一个Spring 容器了，请使用 maven 的小伙伴直接在 dependencies 中加上以下依赖即可，我比较反对那些不知道要添加什么依赖，然后把 Spring 的所有相关的东西都加进来的方式。 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.3.11.RELEASE&lt;/version&gt;&lt;/dependency&gt; spring-context 会自动将 spring-core、spring-beans、spring-aop、spring-expression 这几个基础 jar 包带进来。 多说一句，很多开发者入门就直接接触的 SpringMVC，对 Spring 其实不是很了解，Spring 是渐进式的工具，并不具有很强的侵入性，它的模块也划分得很合理，即使你的应用不是 web 应用，或者之前完全没有使用到 Spring，而你就想用 Spring 的依赖注入这个功能，其实完全是可以的，它的引入不会对其他的组件产生冲突。 废话说完，我们继续。ApplicationContext context = new ClassPathXmlApplicationContext(...) 其实很好理解，从名字上就可以猜出一二，就是在 ClassPath 中寻找 xml 配置文件，根据 xml 文件内容来构建 ApplicationContext。当然，除了 ClassPathXmlApplicationContext 以外，我们也还有其他构建 ApplicationContext 的方案可供选择，我们先来看看大体的继承结构是怎么样的： 读者可以大致看一下类名，源码分析的时候不至于找不着看哪个类，因为 Spring 为了适应各种使用场景，提供的各个接口都可能有很多的实现类。对于我们来说，就是揪着一个完整的分支看完。当然，读本文的时候读者也不必太担心，每个代码块分析的时候，我都会告诉读者我们在说哪个类第几行。 我们可以看到，ClassPathXmlApplicationContext 兜兜转转了好久才到 ApplicationContext 接口，同样的，我们也可以使用绿颜色的 FileSystemXmlApplicationContext 和 AnnotationConfigApplicationContext 这两个类。 FileSystemXmlApplicationContext的构造函数需要一个 xml 配置文件在系统中的路径，其他和 ClassPathXmlApplicationContext 基本上一样。 AnnotationConfigApplicationContext是基于注解来使用的，它不需要配置文件，采用 java 配置类和各种注解来配置，是比较简单的方式，也是大势所趋吧。 不过本文旨在帮助大家理解整个构建流程，所以决定使用ClassPathXmlApplicationContext进行分析。 我们先来一个简单的例子来看看怎么实例化 ApplicationContext。 首先，定义一个接口：123public interface MessageService &#123; String getMessage();&#125; 定义接口实现类：123456public class MessageServiceImpl implements MessageService &#123; public String getMessage() &#123; return "hello world"; &#125;&#125; 接下来，我们在 resources 目录新建一个配置文件，文件名随意，通常叫 application.xml 或 application-xxx.xml就可以了：1234567&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;beans xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.springframework.org/schema/beans" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd" default-autowire="byName"&gt; &lt;bean id="messageService" class="com.javadoop.example.MessageServiceImpl"/&gt;&lt;/beans&gt; 这样，我们就可以跑起来了：12345678910111213public class App &#123; public static void main(String[] args) &#123; // 用我们的配置文件来启动一个 ApplicationContext ApplicationContext context = new ClassPathXmlApplicationContext("classpath:application.xml"); System.out.println("context 启动成功"); // 从 context 中取出我们的 Bean，而不是用 new MessageServiceImpl() 这种方式 MessageService messageService = context.getBean(MessageService.class); // 这句将输出: hello world System.out.println(messageService.getMessage()); &#125;&#125; 以上例子很简单，不过也够引出本文的主题了，就是怎么样通过配置文件来启动 Spring 的 ApplicationContext？也就是我们今天要分析的 IOC 的核心了。ApplicationContext 启动过程中，会负责创建实例 Bean，往各个 Bean 中注入依赖等。 BeanFactory 简介初学者可别以为我之前说那么多和 BeanFactory 无关，前面说的 ApplicationContext 其实就是一个 BeanFactory。我们来看下和 BeanFactory 接口相关的主要的继承结构： 我想，大家看完这个图以后，可能就不是很开心了。ApplicationContext 往下的继承结构前面一张图说过了，这里就不重复了。这张图呢，背下来肯定是不需要的，有几个重点和大家说明下就好。 ApplicationContext继承了ListableBeanFactory，这个 Listable 的意思就是，通过这个接口，我们可以获取多个 Bean，最顶层 BeanFactory 接口的方法都是获取单个 Bean 的。 ApplicationContext 继承了HierarchicalBeanFactory，Hierarchical 单词本身已经能说明问题了，也就是说我们可以在应用中起多个 BeanFactory，然后可以将各个 BeanFactory 设置为父子关系。 AutowireCapableBeanFactory这个名字中的 Autowire 大家都非常熟悉，它就是用来自动装配 Bean 用的，但是仔细看上图，ApplicationContext 并没有继承它，不过不用担心，不使用继承，不代表不可以使用组合，如果你看到 ApplicationContext 接口定义中的最后一个方法 getAutowireCapableBeanFactory()就知道了。 ConfigurableListableBeanFactory也是一个特殊的接口，看图，特殊之处在于它继承了第二层所有的三个接口，而 ApplicationContext 没有。这点之后会用到。 请先不用花时间在其他的接口和类上，先理解我说的这几点就可以了。然后，请读者打开编辑器，翻一下 BeanFactory、ListableBeanFactory、HierarchicalBeanFactory、AutowireCapableBeanFactory、ApplicationContext 这几个接口的代码，大概看一下各个接口中的方法，大家心里要有底，限于篇幅，我就不贴代码介绍了。 启动过程分析下面将会是冗长的代码分析，请读者先喝个水。记住，一定要在电脑中打开源码，不然纯看是很累的。 第一步，我们肯定要从 ClassPathXmlApplicationContext 的构造方法说起。1234567891011121314151617181920public class ClassPathXmlApplicationContext extends AbstractXmlApplicationContext &#123; private Resource[] configResources; // 如果已经有 ApplicationContext 并需要配置成父子关系，那么调用这个构造方法 public ClassPathXmlApplicationContext(ApplicationContext parent) &#123; super(parent); &#125; ... public ClassPathXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException &#123; super(parent); // 根据提供的路径，处理成配置文件数组(以分号、逗号、空格、tab、换行符分割) setConfigLocations(configLocations); if (refresh) &#123; refresh(); // 核心方法 &#125; &#125; ...&#125; 接下来，就是 refresh()，这里简单说下为什么是 refresh()，而不是 init() 这种名字的方法。因为 ApplicationContext 建立起来以后，其实我们是可以通过调用 refresh() 这个方法重建的，这样会将原来的 ApplicationContext 销毁，然后再重新执行一次初始化操作。 往下看，refresh() 方法里面调用了那么多方法，就知道肯定不简单了，请读者先看个大概，细节之后会详细说。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778@Overridepublic void refresh() throws BeansException, IllegalStateException &#123; // 来个锁，不然 refresh() 还没结束，你又来个启动或销毁容器的操作，那不就乱套了嘛 synchronized (this.startupShutdownMonitor) &#123; // 准备工作，记录下容器的启动时间、标记“已启动”状态、处理配置文件中的占位符 prepareRefresh(); // 这步比较关键，这步完成后，配置文件就会解析成一个个 Bean 定义，注册到 BeanFactory 中， // 当然，这里说的 Bean 还没有初始化，只是配置信息都提取出来了， // 注册也只是将这些信息都保存到了注册中心(说到底核心是一个 beanName-&gt; beanDefinition 的 map) ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 设置 BeanFactory 的类加载器，添加几个 BeanPostProcessor，手动注册几个特殊的 bean // 这块待会会展开说 prepareBeanFactory(beanFactory); try &#123; // 【这里需要知道 BeanFactoryPostProcessor 这个知识点，Bean 如果实现了此接口， // 那么在容器初始化以后，Spring 会负责调用里面的 postProcessBeanFactory 方法。】 // 这里是提供给子类的扩展点，到这里的时候，所有的 Bean 都加载、注册完成了，但是都还没有初始化 // 具体的子类可以在这步的时候添加一些特殊的 BeanFactoryPostProcessor 的实现类或做点什么事 postProcessBeanFactory(beanFactory); // 调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory(factory) 方法 invokeBeanFactoryPostProcessors(beanFactory); // 注册 BeanPostProcessor 的实现类，注意看和 BeanFactoryPostProcessor 的区别 // 此接口两个方法: postProcessBeforeInitialization 和 postProcessAfterInitialization // 两个方法分别在 Bean 初始化之前和初始化之后得到执行。注意，到这里 Bean 还没初始化 registerBeanPostProcessors(beanFactory); // 初始化当前 ApplicationContext 的 MessageSource，国际化这里就不展开说了，不然没完没了了 initMessageSource(); // 初始化当前 ApplicationContext 的事件广播器，这里也不展开了 initApplicationEventMulticaster(); // 从方法名就可以知道，典型的模板方法(钩子方法)， // 具体的子类可以在这里初始化一些特殊的 Bean（在初始化 singleton beans 之前） onRefresh(); // 注册事件监听器，监听器需要实现 ApplicationListener 接口。这也不是我们的重点，过 registerListeners(); // 重点，重点，重点 // 初始化所有的 singleton beans //（lazy-init 的除外） finishBeanFactoryInitialization(beanFactory); // 最后，广播事件，ApplicationContext 初始化完成 finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn("Exception encountered during context initialization - " + "cancelling refresh attempt: " + ex); &#125; // Destroy already created singletons to avoid dangling resources. // 销毁已经初始化的 singleton 的 Beans，以免有些 bean 会一直占用资源 destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // 把异常往外抛 throw ex; &#125; finally &#123; // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125;&#125; 下面，我们开始一步步来肢解这个 refresh() 方法。 创建 Bean 容器前的准备工作这个比较简单，直接看代码中的几个注释即可。12345678910111213141516171819protected void prepareRefresh() &#123; // 记录启动时间， // 将 active 属性设置为 true，closed 属性设置为 false，它们都是 AtomicBoolean 类型 this.startupDate = System.currentTimeMillis(); this.closed.set(false); this.active.set(true); if (logger.isInfoEnabled()) &#123; logger.info("Refreshing " + this); &#125; // Initialize any placeholder property sources in the context environment initPropertySources(); // 校验 xml 配置文件 getEnvironment().validateRequiredProperties(); this.earlyApplicationEvents = new LinkedHashSet&lt;ApplicationEvent&gt;();&#125; 创建 Bean 容器，加载并注册 Bean注意，这个方法是全文最重要的部分之一，这里将会初始化 BeanFactory、加载 Bean、注册 Bean 等等。 当然，这步结束后，Bean 并没有完成初始化。 // AbstractApplicationContext.java1234567891011protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; // 关闭旧的 BeanFactory (如果有)，创建新的 BeanFactory，加载 Bean 定义、注册 Bean 等等 refreshBeanFactory(); // 返回刚刚创建的 BeanFactory ConfigurableListableBeanFactory beanFactory = getBeanFactory(); if (logger.isDebugEnabled()) &#123; logger.debug("Bean factory for " + getDisplayName() + ": " + beanFactory); &#125; return beanFactory;&#125; // AbstractRefreshableApplicationContext.java 1201234567891011121314151617181920212223242526272829@Overrideprotected final void refreshBeanFactory() throws BeansException &#123; // 如果 ApplicationContext 中已经加载过 BeanFactory 了，销毁所有 Bean，关闭 BeanFactory // 注意，应用中 BeanFactory 本来就是可以多个的，这里可不是说应用全局是否有 BeanFactory，而是当前 // ApplicationContext 是否有 BeanFactory if (hasBeanFactory()) &#123; destroyBeans(); closeBeanFactory(); &#125; try &#123; // 初始化一个 DefaultListableBeanFactory，为什么用这个，我们马上说。 DefaultListableBeanFactory beanFactory = createBeanFactory(); // 用于 BeanFactory 的序列化，我想不部分人应该都用不到 beanFactory.setSerializationId(getId()); // 下面这两个方法很重要，别跟丢了，具体细节之后说 // 设置 BeanFactory 的两个配置属性：是否允许 Bean 覆盖、是否允许循环引用 customizeBeanFactory(beanFactory); // 加载 Bean 到 BeanFactory 中 loadBeanDefinitions(beanFactory); synchronized (this.beanFactoryMonitor) &#123; this.beanFactory = beanFactory; &#125; &#125; catch (IOException ex) &#123; throw new ApplicationContextException("I/O error parsing bean definition source for " + getDisplayName(), ex); &#125;&#125; 看到这里的时候，我觉得读者就应该站在高处看 ApplicationContext 了，ApplicationContext 继承自 BeanFactory，但是它不应该被理解为 BeanFactory 的实现类，而是说其内部持有一个实例化的 BeanFactory（DefaultListableBeanFactory）。以后所有的 BeanFactory 相关的操作其实是给这个实例来处理的。 我们说说为什么选择实例化 DefaultListableBeanFactory ？前面我们说了有个很重要的接口 ConfigurableListableBeanFactory，它实现了 BeanFactory 下面一层的所有三个接口，我把之前的继承图再拿过来大家再仔细看一下： 我们可以看到 ConfigurableListableBeanFactory 只有一个实现类 DefaultListableBeanFactory，而且实现类 DefaultListableBeanFactory 还通过实现右边的 AbstractAutowireCapableBeanFactory 通吃了右路。所以结论就是，最底下这个家伙 DefaultListableBeanFactory 基本上是最牛的 BeanFactory 了，这也是为什么这边会使用这个类来实例化的原因。 在继续往下之前，我们需要先了解 BeanDefinition。我们说 BeanFactory 是 Bean 容器，那么 Bean 又是什么呢？ 这里的 BeanDefinition 就是我们所说的 Spring 的 Bean，我们自己定义的各个 Bean 其实会转换成一个个 BeanDefinition 存在于 Spring 的 BeanFactory 中。 所以，如果有人问你 Bean 是什么的时候，你要知道 Bean 在代码层面上是 BeanDefinition 的实例。 BeanDefinition 中保存了我们的 Bean 信息，比如这个 Bean 指向的是哪个类、是否是单例的、是否懒加载、这个 Bean 依赖了哪些 Bean 等等。 BeanDefinition 接口定义我们来看下 BeanDefinition 的接口定义：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public interface BeanDefinition extends AttributeAccessor, BeanMetadataElement &#123; // 我们可以看到，默认只提供 sington 和 prototype 两种， // 很多读者都知道还有 request, session, globalSession, application, websocket 这几种， // 不过，它们属于基于 web 的扩展。 String SCOPE_SINGLETON = ConfigurableBeanFactory.SCOPE_SINGLETON; String SCOPE_PROTOTYPE = ConfigurableBeanFactory.SCOPE_PROTOTYPE; // 比较不重要，直接跳过吧 int ROLE_APPLICATION = 0; int ROLE_SUPPORT = 1; int ROLE_INFRASTRUCTURE = 2; // 设置父 Bean，这里涉及到 bean 继承，不是 java 继承。请参见附录介绍 void setParentName(String parentName); // 获取父 Bean String getParentName(); // 设置 Bean 的类名称 void setBeanClassName(String beanClassName); // 获取 Bean 的类名称 String getBeanClassName(); // 设置 bean 的 scope void setScope(String scope); String getScope(); // 设置是否懒加载 void setLazyInit(boolean lazyInit); boolean isLazyInit(); // 设置该 Bean 依赖的所有的 Bean，注意，这里的依赖不是指属性依赖(如 @Autowire 标记的)， // 是 depends-on="" 属性设置的值。 void setDependsOn(String... dependsOn); // 返回该 Bean 的所有依赖 String[] getDependsOn(); // 设置该 Bean 是否可以注入到其他 Bean 中，只对根据类型注入有效， // 如果根据名称注入，即使这边设置了 false，也是可以的 void setAutowireCandidate(boolean autowireCandidate); // 该 Bean 是否可以注入到其他 Bean 中 boolean isAutowireCandidate(); // 主要的。同一接口的多个实现，如果不指定名字的话，Spring 会优先选择设置 primary 为 true 的 bean void setPrimary(boolean primary); // 是否是 primary 的 boolean isPrimary(); // 如果该 Bean 采用工厂方法生成，指定工厂名称。对工厂不熟悉的读者，请参加附录 void setFactoryBeanName(String factoryBeanName); // 获取工厂名称 String getFactoryBeanName(); // 指定工厂类中的 工厂方法名称 void setFactoryMethodName(String factoryMethodName); // 获取工厂类中的 工厂方法名称 String getFactoryMethodName(); // 构造器参数 ConstructorArgumentValues getConstructorArgumentValues(); // Bean 中的属性值，后面给 bean 注入属性值的时候会说到 MutablePropertyValues getPropertyValues(); // 是否 singleton boolean isSingleton(); // 是否 prototype boolean isPrototype(); // 如果这个 Bean 原生是抽象类，那么不能实例化 boolean isAbstract(); int getRole(); String getDescription(); String getResourceDescription(); BeanDefinition getOriginatingBeanDefinition();&#125; 这个 BeanDefinition 其实已经包含很多的信息了，暂时不清楚所有的方法对应什么东西没关系，希望看完本文后读者可以彻底搞清楚里面的所有东西。 这里接口虽然那么多，但是没有类似 getInstance() 这种方法来获取我们定义的类的实例，真正的我们定义的类生成的实例到哪里去了呢？别着急，这个要很后面才能讲到。 有了 BeanDefinition 的概念以后，我们再往下看 refreshBeanFactory() 方法中的剩余部分：12customizeBeanFactory(beanFactory);loadBeanDefinitions(beanFactory); 虽然只有两个方法，但路还很长啊。。。 customizeBeanFactory customizeBeanFactory(beanFactory) 比较简单，就是配置是否允许 BeanDefinition 覆盖、是否允许循环引用。12345678910protected void customizeBeanFactory(DefaultListableBeanFactory beanFactory) &#123; if (this.allowBeanDefinitionOverriding != null) &#123; // 是否允许 Bean 定义覆盖 beanFactory.setAllowBeanDefinitionOverriding(this.allowBeanDefinitionOverriding); &#125; if (this.allowCircularReferences != null) &#123; // 是否允许 Bean 间的循环依赖 beanFactory.setAllowCircularReferences(this.allowCircularReferences); &#125;&#125; BeanDefinition 的覆盖问题大家也许会碰到，就是在配置文件中定义 bean 时使用了相同的 id 或 name，默认情况下，allowBeanDefinitionOverriding属性为 null，如果在同一配置文件中重复了，会抛错，但是如果不是同一配置文件中，会发生覆盖。 循环引用也很好理解：A 依赖 B，而 B 依赖 A。或 A 依赖 B，B 依赖 C，而 C 依赖 A。 默认情况下，Spring 允许循环依赖，当然如果你在 A 的构造方法中依赖 B，在 B 的构造方法中依赖 A 是不行的。 至于这两个属性怎么配置？我在附录中进行了介绍，尤其对于覆盖问题，很多人都希望禁止出现 Bean 覆盖，可是 Spring 默认是不同文件的时候可以覆盖的。 之后的源码中还会出现这两个属性，读者有个印象就可以了。 加载 Bean: loadBeanDefinitions 接下来是最重要的loadBeanDefinitions(beanFactory)方法了，这个方法将根据配置，加载各个 Bean，然后放到 BeanFactory 中。 读取配置的操作在 XmlBeanDefinitionReader 中，其负责加载配置、解析。 // AbstractXmlApplicationContext.java 80123456789101112131415161718/** 我们可以看到，此方法将通过一个 XmlBeanDefinitionReader 实例来加载各个 Bean。*/@Overrideprotected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; // 给这个 BeanFactory 实例化一个 XmlBeanDefinitionReader XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); // Configure the bean definition reader with this context's // resource loading environment. beanDefinitionReader.setEnvironment(this.getEnvironment()); beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); // 初始化 BeanDefinitionReader，其实这个是提供给子类覆写的， // 我看了一下，没有类覆写这个方法，我们姑且当做不重要吧 initBeanDefinitionReader(beanDefinitionReader); // 重点来了，继续往下 loadBeanDefinitions(beanDefinitionReader);&#125; 现在还在这个类中，接下来用刚刚初始化的 Reader 开始来加载 xml 配置，这块代码读者可以选择性跳过，不是很重要。也就是说，下面这个代码块，读者可以很轻松地略过。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137// AbstractXmlApplicationContext.java 120protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException &#123; Resource[] configResources = getConfigResources(); if (configResources != null) &#123; // 往下看 reader.loadBeanDefinitions(configResources); &#125; String[] configLocations = getConfigLocations(); if (configLocations != null) &#123; reader.loadBeanDefinitions(configLocations); &#125;&#125;// 上面虽然有两个分支，不过第二个分支很快通过解析路径转换为 Resource 以后也会进到这里@Overridepublic int loadBeanDefinitions(Resource... resources) throws BeanDefinitionStoreException &#123; Assert.notNull(resources, "Resource array must not be null"); int counter = 0; // 注意这里是个 for 循环，也就是每个文件是一个 resource for (Resource resource : resources) &#123; // 继续往下看 counter += loadBeanDefinitions(resource); &#125; return counter;&#125;// XmlBeanDefinitionReader 303@Overridepublic int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException &#123; return loadBeanDefinitions(new EncodedResource(resource));&#125;// XmlBeanDefinitionReader 314public int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException &#123; Assert.notNull(encodedResource, "EncodedResource must not be null"); if (logger.isInfoEnabled()) &#123; logger.info("Loading XML bean definitions from " + encodedResource.getResource()); &#125; // 用一个 ThreadLocal 来存放所有的配置文件资源 Set&lt;EncodedResource&gt; currentResources = this.resourcesCurrentlyBeingLoaded.get(); if (currentResources == null) &#123; currentResources = new HashSet&lt;EncodedResource&gt;(4); this.resourcesCurrentlyBeingLoaded.set(currentResources); &#125; if (!currentResources.add(encodedResource)) &#123; throw new BeanDefinitionStoreException( "Detected cyclic loading of " + encodedResource + " - check your import definitions!"); &#125; try &#123; InputStream inputStream = encodedResource.getResource().getInputStream(); try &#123; InputSource inputSource = new InputSource(inputStream); if (encodedResource.getEncoding() != null) &#123; inputSource.setEncoding(encodedResource.getEncoding()); &#125; // 核心部分 return doLoadBeanDefinitions(inputSource, encodedResource.getResource()); &#125; finally &#123; inputStream.close(); &#125; &#125; catch (IOException ex) &#123; throw new BeanDefinitionStoreException( "IOException parsing XML document from " + encodedResource.getResource(), ex); &#125; finally &#123; currentResources.remove(encodedResource); if (currentResources.isEmpty()) &#123; this.resourcesCurrentlyBeingLoaded.remove(); &#125; &#125;&#125;// 还在这个文件中，第 388 行protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) throws BeanDefinitionStoreException &#123; try &#123; // 这里就不看了 Document doc = doLoadDocument(inputSource, resource); // 继续 return registerBeanDefinitions(doc, resource); &#125; catch (...&#125;// 还在这个文件中，第 505 行// 返回从当前配置文件加载了多少数量的 Beanpublic int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException &#123; BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader(); int countBefore = getRegistry().getBeanDefinitionCount(); documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); return getRegistry().getBeanDefinitionCount() - countBefore;&#125;// DefaultBeanDefinitionDocumentReader 90@Overridepublic void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) &#123; this.readerContext = readerContext; logger.debug("Loading bean definitions"); Element root = doc.getDocumentElement(); doRegisterBeanDefinitions(root);&#125;经过漫长的链路，一个配置文件终于转换为一颗 DOM 树了，注意，这里指的是其中一个配置文件，不是所有的，读者可以看到上面有个 for 循环的。下面从根节点开始解析：```java// DefaultBeanDefinitionDocumentReader 116protected void doRegisterBeanDefinitions(Element root) &#123; // 我们看名字就知道，BeanDefinitionParserDelegate 必定是一个重要的类，它负责解析 Bean 定义， // 这里为什么要定义一个 parent? 看到后面就知道了，是递归问题， // 因为 &lt;beans /&gt; 内部是可以定义 &lt;beans /&gt; 的，所以这个方法的 root 其实不一定就是 xml 的根节点，也可以是嵌套在里面的 &lt;beans /&gt; 节点，从源码分析的角度，我们当做根节点就好了 BeanDefinitionParserDelegate parent = this.delegate; this.delegate = createDelegate(getReaderContext(), root, parent); if (this.delegate.isDefaultNamespace(root)) &#123; // 这块说的是根节点 &lt;beans ... profile="dev" /&gt; 中的 profile 是否是当前环境需要的， // 如果当前环境配置的 profile 不包含此 profile，那就直接 return 了，不对此 &lt;beans /&gt; 解析 // 不熟悉 profile 为何物，不熟悉怎么配置 profile 读者的请移步附录区 String profileSpec = root.getAttribute(PROFILE_ATTRIBUTE); if (StringUtils.hasText(profileSpec)) &#123; String[] specifiedProfiles = StringUtils.tokenizeToStringArray( profileSpec, BeanDefinitionParserDelegate.MULTI_VALUE_ATTRIBUTE_DELIMITERS); if (!getReaderContext().getEnvironment().acceptsProfiles(specifiedProfiles)) &#123; if (logger.isInfoEnabled()) &#123; logger.info("Skipped XML bean definition file due to specified profiles [" + profileSpec + "] not matching: " + getReaderContext().getResource()); &#125; return; &#125; &#125; &#125; preProcessXml(root); // 钩子 parseBeanDefinitions(root, this.delegate); postProcessXml(root); // 钩子 this.delegate = parent;&#125; preProcessXml(root) 和 postProcessXml(root) 是给子类用的钩子方法，鉴于没有被使用到，也不是我们的重点，我们直接跳过。 这里涉及到了 profile 的问题，对于不了解的读者，我在附录中对 profile 做了简单的解释，读者可以参考一下。 接下来，看核心解析方法 parseBeanDefinitions(root, this.delegate) :12345678910111213141516171819202122// default namespace 涉及到的就四个标签 &lt;import /&gt;、&lt;alias /&gt;、&lt;bean /&gt; 和 &lt;beans /&gt;，// 其他的属于 custom 的protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123; if (delegate.isDefaultNamespace(root)) &#123; NodeList nl = root.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (node instanceof Element) &#123; Element ele = (Element) node; if (delegate.isDefaultNamespace(ele)) &#123; parseDefaultElement(ele, delegate); &#125; else &#123; delegate.parseCustomElement(ele); &#125; &#125; &#125; &#125; else &#123; delegate.parseCustomElement(root); &#125;&#125; 从上面的代码，我们可以看到，对于每个配置来说，分别进入到 parseDefaultElement(ele, delegate); 和 delegate.parseCustomElement(ele);这两个分支了。 parseDefaultElement(ele, delegate) 代表解析的节点是&lt; import /&gt;、&lt; alias /&gt;、&lt; bean /&gt;、&lt; beans /&gt; 这几个。 这里的四个标签之所以是 default 的，是因为它们是处于这个 namespace 下定义的： http://www.springframework.org/schema/beans 又到初学者科普时间，不熟悉 namespace 的读者请看下面贴出来的 xml，这里的第二行 xmlns 就是咯。123456&lt;beans xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.springframework.org/schema/beans" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd" default-autowire="byName"&gt; 而对于其他的标签，将进入到delegate.parseCustomElement(element) 这个分支。如我们经常会使用到的&lt; mvc /&gt;、&lt; task /&gt;、&lt; context /&gt;、&lt; aop /&gt;等。 这些属于扩展，如果需要使用上面这些 ”非 default“ 标签，那么上面的 xml 头部的地方也要引入相应的 namespace 和 .xsd 文件的路径，如下所示。同时代码中需要提供相应的 parser 来解析，如 MvcNamespaceHandler、TaskNamespaceHandler、ContextNamespaceHandler、AopNamespaceHandler 等。 假如读者想分析 的实现原理，就应该到 ContextNamespaceHandler 中找答案。12345678910111213&lt;beans xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.springframework.org/schema/beans" xmlns:context="http://www.springframework.org/schema/context" xmlns:mvc="http://www.springframework.org/schema/mvc" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd " default-autowire="byName"&gt; 回过神来，看看处理 default 标签的方法：12345678910111213141516171819private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) &#123; if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) &#123; // 处理 &lt;import /&gt; 标签 importBeanDefinitionResource(ele); &#125; else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) &#123; // 处理 &lt;alias /&gt; 标签定义 // &lt;alias name="fromName" alias="toName"/&gt; processAliasRegistration(ele); &#125; else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) &#123; // 处理 &lt;bean /&gt; 标签定义，这也算是我们的重点吧 processBeanDefinition(ele, delegate); &#125; else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) &#123; // 如果碰到的是嵌套的 &lt;beans /&gt; 标签，需要递归 doRegisterBeanDefinitions(ele); &#125;&#125; 如果每个标签都说，那我不吐血，你们都要吐血了。我们挑我们的重点 标签出来说。 processBeanDefinition 下面是 processBeanDefinition 解析 标签：12345678910111213141516171819202122// DefaultBeanDefinitionDocumentReader 298protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; // 将 &lt;bean /&gt; 节点中的信息提取出来，然后封装到一个 BeanDefinitionHolder 中，细节往下看 BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); // 下面的几行先不要看，跳过先，跳过先，跳过先，后面会继续说的 if (bdHolder != null) &#123; bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; // Register the final decorated instance. BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException ex) &#123; getReaderContext().error("Failed to register bean definition with name '" + bdHolder.getBeanName() + "'", ele, ex); &#125; // Send registration event. getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125;&#125; 继续往下看怎么解析之前，我们先看下 标签中可以定义哪些属性： Property 解析 class 类的全限定名 name 可指定 id、name(用逗号、分号、空格分隔) scope 作用域 constructor arguments 指定构造参数 properties 设置属性的值 autowiring mode no(默认值)、byName、byType、 constructor lazy-initialization mode 是否懒加载(如果被非懒加载的bean依赖了那么其实也就不能懒加载了) initialization method bean 属性设置完成后，会调用这个方法 destruction method bean 销毁后的回调方法 上面表格中的内容我想大家都非常熟悉吧，如果不熟悉，那就是你不够了解 Spring 的配置了。 简单地说就是像下面这样子：123456789101112131415&lt;bean id="exampleBean" name="name1, name2, name3" class="com.javadoop.ExampleBean" scope="singleton" lazy-init="true" init-method="init" destroy-method="cleanup"&gt; &lt;!-- 可以用下面三种形式指定构造参数 --&gt; &lt;constructor-arg type="int" value="7500000"/&gt; &lt;constructor-arg name="years" value="7500000"/&gt; &lt;constructor-arg index="0" value="7500000"/&gt; &lt;!-- property 的几种情况 --&gt; &lt;property name="beanOne"&gt; &lt;ref bean="anotherExampleBean"/&gt; &lt;/property&gt; &lt;property name="beanTwo" ref="yetAnotherBean"/&gt; &lt;property name="integerProperty" value="1"/&gt;&lt;/bean&gt; 当然，除了上面举例出来的这些，还有&lt; factory-bean&gt;、&lt; factory-method&gt;、&lt; lockup-method /&gt;、&lt; replaced-method /&gt;、&lt; meta /&gt;、&lt; qualifier /&gt; 这几个，大家是不是熟悉呢？ 有了以上这些知识以后，我们再继续往里看怎么解析 bean 元素，是怎么转换到BeanDefinitionHolder的。 // BeanDefinitionParserDelegate 428123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public BeanDefinitionHolder parseBeanDefinitionElement(Element ele) &#123; return parseBeanDefinitionElement(ele, null);&#125;public BeanDefinitionHolder parseBeanDefinitionElement(Element ele, BeanDefinition containingBean) &#123; String id = ele.getAttribute(ID_ATTRIBUTE); String nameAttr = ele.getAttribute(NAME_ATTRIBUTE); List&lt;String&gt; aliases = new ArrayList&lt;String&gt;(); // 将 name 属性的定义按照 ”逗号、分号、空格“ 切分，形成一个别名列表数组， // 当然，如果你不定义的话，就是空的了 // 我在附录中简单介绍了一下 id 和 name 的配置，大家可以看一眼，有个20秒就可以了 if (StringUtils.hasLength(nameAttr)) &#123; String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, MULTI_VALUE_ATTRIBUTE_DELIMITERS); aliases.addAll(Arrays.asList(nameArr)); &#125; String beanName = id; // 如果没有指定id, 那么用别名列表的第一个名字作为beanName if (!StringUtils.hasText(beanName) &amp;&amp; !aliases.isEmpty()) &#123; beanName = aliases.remove(0); if (logger.isDebugEnabled()) &#123; logger.debug("No XML 'id' specified - using '" + beanName + "' as bean name and " + aliases + " as aliases"); &#125; &#125; if (containingBean == null) &#123; checkNameUniqueness(beanName, aliases, ele); &#125; // 根据 &lt;bean ...&gt;...&lt;/bean&gt; 中的配置创建 BeanDefinition，然后把配置中的信息都设置到实例中, // 细节后面再说 AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean); // 到这里，整个 &lt;bean /&gt; 标签就算解析结束了，一个 BeanDefinition 就形成了。 if (beanDefinition != null) &#123; // 如果都没有设置 id 和 name，那么此时的 beanName 就会为 null，进入下面这块代码产生 // 如果读者不感兴趣的话，我觉得不需要关心这块代码，对本文源码分析来说，这些东西不重要 if (!StringUtils.hasText(beanName)) &#123; try &#123; if (containingBean != null) &#123;// 按照我们的思路，这里 containingBean 是 null 的 beanName = BeanDefinitionReaderUtils.generateBeanName( beanDefinition, this.readerContext.getRegistry(), true); &#125; else &#123; // 如果我们不定义 id 和 name，那么我们引言里的那个例子： // 1. beanName 为：com.javadoop.example.MessageServiceImpl#0 // 2. beanClassName 为：com.javadoop.example.MessageServiceImpl beanName = this.readerContext.generateBeanName(beanDefinition); String beanClassName = beanDefinition.getBeanClassName(); if (beanClassName != null &amp;&amp; beanName.startsWith(beanClassName) &amp;&amp; beanName.length() &gt; beanClassName.length() &amp;&amp; !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) &#123; // 把 beanClassName 设置为 Bean 的别名 aliases.add(beanClassName); &#125; &#125; if (logger.isDebugEnabled()) &#123; logger.debug("Neither XML 'id' nor 'name' specified - " + "using generated bean name [" + beanName + "]"); &#125; &#125; catch (Exception ex) &#123; error(ex.getMessage(), ele); return null; &#125; &#125; String[] aliasesArray = StringUtils.toStringArray(aliases); // 返回 BeanDefinitionHolder return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray); &#125; return null;&#125; 看看怎么根据配置创建 BeanDefinition：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public AbstractBeanDefinition parseBeanDefinitionElement( Element ele, String beanName, BeanDefinition containingBean) &#123; this.parseState.push(new BeanEntry(beanName)); String className = null; if (ele.hasAttribute(CLASS_ATTRIBUTE)) &#123; className = ele.getAttribute(CLASS_ATTRIBUTE).trim(); &#125; try &#123; String parent = null; if (ele.hasAttribute(PARENT_ATTRIBUTE)) &#123; parent = ele.getAttribute(PARENT_ATTRIBUTE); &#125; // 创建 BeanDefinition，然后设置类信息而已，很简单，就不贴代码了 AbstractBeanDefinition bd = createBeanDefinition(className, parent); // 设置 BeanDefinition 的一堆属性，这些属性定义在 AbstractBeanDefinition 中 parseBeanDefinitionAttributes(ele, beanName, containingBean, bd); bd.setDescription(DomUtils.getChildElementValueByTagName(ele, DESCRIPTION_ELEMENT)); /** * 下面的一堆是解析 &lt;bean&gt;......&lt;/bean&gt; 内部的子元素， * 解析出来以后的信息都放到 bd 的属性中 */ // 解析 &lt;meta /&gt; parseMetaElements(ele, bd); // 解析 &lt;lookup-method /&gt; parseLookupOverrideSubElements(ele, bd.getMethodOverrides()); // 解析 &lt;replaced-method /&gt; parseReplacedMethodSubElements(ele, bd.getMethodOverrides()); // 解析 &lt;constructor-arg /&gt; parseConstructorArgElements(ele, bd); // 解析 &lt;property /&gt; parsePropertyElements(ele, bd); // 解析 &lt;qualifier /&gt; parseQualifierElements(ele, bd); bd.setResource(this.readerContext.getResource()); bd.setSource(extractSource(ele)); return bd; &#125; catch (ClassNotFoundException ex) &#123; error("Bean class [" + className + "] not found", ele, ex); &#125; catch (NoClassDefFoundError err) &#123; error("Class that bean class [" + className + "] depends on not found", ele, err); &#125; catch (Throwable ex) &#123; error("Unexpected failure during bean definition parsing", ele, ex); &#125; finally &#123; this.parseState.pop(); &#125; return null;&#125; 到这里，我们已经完成了根据&lt;bean /&gt; 配置创建了一个 BeanDefinitionHolder 实例。注意，是一个。 我们回到解析&lt;bean /&gt;的入口方法:1234567891011121314151617181920// DefaultBeanDefinitionDocumentReader 298protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; // 将 &lt;bean /&gt; 节点转换为 BeanDefinitionHolder，就是上面说的一堆 BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) &#123; // 如果有自定义属性的话，进行相应的解析，先忽略 bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; // 我们把这步叫做 注册Bean 吧 BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException ex) &#123; getReaderContext().error("Failed to register bean definition with name '" + bdHolder.getBeanName() + "'", ele, ex); &#125; // 注册完成后，发送事件，本文不展开说这个 getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125;&#125; 大家再仔细看一下这块吧，我们后面就不回来说这个了。这里已经根据一个&lt;bean /&gt; 标签产生了一个 BeanDefinitionHolder 的实例，这个实例里面也就是一个 BeanDefinition 的实例和它的 beanName、aliases 这三个信息，注意，我们的关注点始终在 BeanDefinition 上：12345678public class BeanDefinitionHolder implements BeanMetadataElement &#123; private final BeanDefinition beanDefinition; private final String beanName; private final String[] aliases;... 然后我们准备注册这个 BeanDefinition，最后，把这个注册事件发送出去。 下面，我们开始说注册 Bean 吧。 注册 Bean // BeanDefinitionReaderUtils 143123456789101112131415161718public static void registerBeanDefinition( BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry) throws BeanDefinitionStoreException &#123; String beanName = definitionHolder.getBeanName(); // 注册这个 Bean registry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition()); // 如果还有别名的话，也要根据别名统统注册一遍，不然根据别名就找不到 Bean 了，这我们就不开心了 String[] aliases = definitionHolder.getAliases(); if (aliases != null) &#123; for (String alias : aliases) &#123; // alias -&gt; beanName 保存它们的别名信息，这个很简单，用一个 map 保存一下就可以了， // 获取的时候，会先将 alias 转换为 beanName，然后再查找 registry.registerAlias(beanName, alias); &#125; &#125;&#125; 别名注册的放一边，毕竟它很简单，我们看看怎么注册 Bean。 // DefaultListableBeanFactory 793123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081@Overridepublic void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException &#123; Assert.hasText(beanName, "Bean name must not be empty"); Assert.notNull(beanDefinition, "BeanDefinition must not be null"); if (beanDefinition instanceof AbstractBeanDefinition) &#123; try &#123; ((AbstractBeanDefinition) beanDefinition).validate(); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanDefinitionStoreException(...); &#125; &#125; // old? 还记得 “允许 bean 覆盖” 这个配置吗？allowBeanDefinitionOverriding BeanDefinition oldBeanDefinition; // 之后会看到，所有的 Bean 注册后会放入这个 beanDefinitionMap 中 oldBeanDefinition = this.beanDefinitionMap.get(beanName); // 处理重复名称的 Bean 定义的情况 if (oldBeanDefinition != null) &#123; if (!isAllowBeanDefinitionOverriding()) &#123; // 如果不允许覆盖的话，抛异常 throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription()... &#125; else if (oldBeanDefinition.getRole() &lt; beanDefinition.getRole()) &#123; // log...用框架定义的 Bean 覆盖用户自定义的 Bean &#125; else if (!beanDefinition.equals(oldBeanDefinition)) &#123; // log...用新的 Bean 覆盖旧的 Bean &#125; else &#123; // log...用同等的 Bean 覆盖旧的 Bean，这里指的是 equals 方法返回 true 的 Bean &#125; // 覆盖 this.beanDefinitionMap.put(beanName, beanDefinition); &#125; else &#123; // 判断是否已经有其他的 Bean 开始初始化了. // 注意，"注册Bean" 这个动作结束，Bean 依然还没有初始化，我们后面会有大篇幅说初始化过程， // 在 Spring 容器启动的最后，会 预初始化 所有的 singleton beans if (hasBeanCreationStarted()) &#123; // Cannot modify startup-time collection elements anymore (for stable iteration) synchronized (this.beanDefinitionMap) &#123; this.beanDefinitionMap.put(beanName, beanDefinition); List&lt;String&gt; updatedDefinitions = new ArrayList&lt;String&gt;(this.beanDefinitionNames.size() + 1); updatedDefinitions.addAll(this.beanDefinitionNames); updatedDefinitions.add(beanName); this.beanDefinitionNames = updatedDefinitions; if (this.manualSingletonNames.contains(beanName)) &#123; Set&lt;String&gt; updatedSingletons = new LinkedHashSet&lt;String&gt;(this.manualSingletonNames); updatedSingletons.remove(beanName); this.manualSingletonNames = updatedSingletons; &#125; &#125; &#125; else &#123; // 最正常的应该是进到这里。 // 将 BeanDefinition 放到这个 map 中，这个 map 保存了所有的 BeanDefinition this.beanDefinitionMap.put(beanName, beanDefinition); // 这是个 ArrayList，所以会按照 bean 配置的顺序保存每一个注册的 Bean 的名字 this.beanDefinitionNames.add(beanName); // 这是个 LinkedHashSet，代表的是手动注册的 singleton bean， // 注意这里是 remove 方法，到这里的 Bean 当然不是手动注册的 // 手动指的是通过调用以下方法注册的 bean ： // registerSingleton(String beanName, Object singletonObject) // 这不是重点，解释只是为了不让大家疑惑。Spring 会在后面"手动"注册一些 Bean，如 "environment"、"systemProperties" 等 bean this.manualSingletonNames.remove(beanName); &#125; // 这个不重要，在预初始化的时候会用到，不必管它。 this.frozenBeanDefinitionNames = null; &#125; if (oldBeanDefinition != null || containsSingleton(beanName)) &#123; resetBeanDefinition(beanName); &#125;&#125; 总结一下，到这里已经初始化了 Bean 容器，&lt;bean /&gt;配置也相应的转换为了一个个 BeanDefinition，然后注册了各个 BeanDefinition 到注册中心，并且发送了注册事件。 Bean 容器实例化完成后说到这里，我们回到refresh() 方法，我重新贴了一遍代码，看看我们说到哪了。是的，我们才说完 obtainFreshBeanFactory() 方法。 考虑到篇幅，这里开始大幅缩减掉没必要详细介绍的部分，大家直接看下面的代码中的注释就好了。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778@Overridepublic void refresh() throws BeansException, IllegalStateException &#123; // 来个锁，不然 refresh() 还没结束，你又来个启动或销毁容器的操作，那不就乱套了嘛 synchronized (this.startupShutdownMonitor) &#123; // 准备工作，记录下容器的启动时间、标记“已启动”状态、处理配置文件中的占位符 prepareRefresh(); // 这步比较关键，这步完成后，配置文件就会解析成一个个 Bean 定义，注册到 BeanFactory 中， // 当然，这里说的 Bean 还没有初始化，只是配置信息都提取出来了， // 注册也只是将这些信息都保存到了注册中心(说到底核心是一个 beanName-&gt; beanDefinition 的 map) ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 设置 BeanFactory 的类加载器，添加几个 BeanPostProcessor，手动注册几个特殊的 bean // 这块待会会展开说 prepareBeanFactory(beanFactory); try &#123; // 【这里需要知道 BeanFactoryPostProcessor 这个知识点，Bean 如果实现了此接口， // 那么在容器初始化以后，Spring 会负责调用里面的 postProcessBeanFactory 方法。】 // 这里是提供给子类的扩展点，到这里的时候，所有的 Bean 都加载、注册完成了，但是都还没有初始化 // 具体的子类可以在这步的时候添加一些特殊的 BeanFactoryPostProcessor 的实现类或做点什么事 postProcessBeanFactory(beanFactory); // 调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory(factory) 方法 invokeBeanFactoryPostProcessors(beanFactory); // 注册 BeanPostProcessor 的实现类，注意看和 BeanFactoryPostProcessor 的区别 // 此接口两个方法: postProcessBeforeInitialization 和 postProcessAfterInitialization // 两个方法分别在 Bean 初始化之前和初始化之后得到执行。注意，到这里 Bean 还没初始化 registerBeanPostProcessors(beanFactory); // 初始化当前 ApplicationContext 的 MessageSource，国际化这里就不展开说了，不然没完没了了 initMessageSource(); // 初始化当前 ApplicationContext 的事件广播器，这里也不展开了 initApplicationEventMulticaster(); // 从方法名就可以知道，典型的模板方法(钩子方法)， // 具体的子类可以在这里初始化一些特殊的 Bean（在初始化 singleton beans 之前） onRefresh(); // 注册事件监听器，监听器需要实现 ApplicationListener 接口。这也不是我们的重点，过 registerListeners(); // 重点，重点，重点 // 初始化所有的 singleton beans //（lazy-init 的除外） finishBeanFactoryInitialization(beanFactory); // 最后，广播事件，ApplicationContext 初始化完成 finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn("Exception encountered during context initialization - " + "cancelling refresh attempt: " + ex); &#125; // Destroy already created singletons to avoid dangling resources. // 销毁已经初始化的 singleton 的 Beans，以免有些 bean 会一直占用资源 destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // 把异常往外抛 throw ex; &#125; finally &#123; // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125;&#125; 准备 Bean 容器: prepareBeanFactory这里简单介绍下 prepareBeanFactory(factory) 方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/** * Configure the factory's standard context characteristics, * such as the context's ClassLoader and post-processors. * @param beanFactory the BeanFactory to configure */protected void prepareBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123; // 设置 BeanFactory 的类加载器，我们知道 BeanFactory 需要加载类，也就需要类加载器， // 这里设置为当前 ApplicationContext 的类加载器 beanFactory.setBeanClassLoader(getClassLoader()); // 设置 BeanExpressionResolver beanFactory.setBeanExpressionResolver(new StandardBeanExpressionResolver(beanFactory.getBeanClassLoader())); // beanFactory.addPropertyEditorRegistrar(new ResourceEditorRegistrar(this, getEnvironment())); // 添加一个 BeanPostProcessor，这个 processor 比较简单， // 实现了 Aware 接口的几个特殊的 beans 在初始化的时候，这个 processor 负责回调 beanFactory.addBeanPostProcessor(new ApplicationContextAwareProcessor(this)); // 下面几行的意思就是，如果某个 bean 依赖于以下几个接口的实现类，在自动装配的时候忽略它们， // Spring 会通过其他方式来处理这些依赖。 beanFactory.ignoreDependencyInterface(EnvironmentAware.class); beanFactory.ignoreDependencyInterface(EmbeddedValueResolverAware.class); beanFactory.ignoreDependencyInterface(ResourceLoaderAware.class); beanFactory.ignoreDependencyInterface(ApplicationEventPublisherAware.class); beanFactory.ignoreDependencyInterface(MessageSourceAware.class); beanFactory.ignoreDependencyInterface(ApplicationContextAware.class); /** * 下面几行就是为特殊的几个 bean 赋值，如果有 bean 依赖了以下几个，会注入这边相应的值， * 之前我们说过，"当前 ApplicationContext 持有一个 BeanFactory"，这里解释了第一行 * ApplicationContext 继承了 ResourceLoader、ApplicationEventPublisher、MessageSource * 所以对于这几个，可以赋值为 this，注意 this 是一个 ApplicationContext * 那这里怎么没看到为 MessageSource 赋值呢？那是因为 MessageSource 被注册成为了一个普通的 bean */ beanFactory.registerResolvableDependency(BeanFactory.class, beanFactory); beanFactory.registerResolvableDependency(ResourceLoader.class, this); beanFactory.registerResolvableDependency(ApplicationEventPublisher.class, this); beanFactory.registerResolvableDependency(ApplicationContext.class, this); // 这个 BeanPostProcessor 也很简单，在 bean 实例化后，如果是 ApplicationListener 的子类， // 那么将其添加到 listener 列表中，可以理解成：注册事件监听器 beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(this)); // Detect a LoadTimeWeaver and prepare for weaving, if found. // 这里涉及到特殊的 bean，名为：loadTimeWeaver，这不是我们的重点，忽略它 if (beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) &#123; beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); // Set a temporary ClassLoader for type matching. beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); &#125; /** * 从下面几行代码我们可以知道，Spring 往往很 "智能" 就是因为它会帮我们默认注册一些有用的 bean， * 我们也可以选择覆盖 */ // 如果没有定义 "environment" 这个 bean，那么 Spring 会 "手动" 注册一个 if (!beanFactory.containsLocalBean(ENVIRONMENT_BEAN_NAME)) &#123; beanFactory.registerSingleton(ENVIRONMENT_BEAN_NAME, getEnvironment()); &#125; // 如果没有定义 "systemProperties" 这个 bean，那么 Spring 会 "手动" 注册一个 if (!beanFactory.containsLocalBean(SYSTEM_PROPERTIES_BEAN_NAME)) &#123; beanFactory.registerSingleton(SYSTEM_PROPERTIES_BEAN_NAME, getEnvironment().getSystemProperties()); &#125; // 如果没有定义 "systemEnvironment" 这个 bean，那么 Spring 会 "手动" 注册一个 if (!beanFactory.containsLocalBean(SYSTEM_ENVIRONMENT_BEAN_NAME)) &#123; beanFactory.registerSingleton(SYSTEM_ENVIRONMENT_BEAN_NAME, getEnvironment().getSystemEnvironment()); &#125;&#125; 在上面这块代码中，Spring 对一些特殊的 bean 进行了处理，读者如果暂时还不能消化它们也没有关系，慢慢往下看。 初始化所有的 singleton beans我们的重点当然是finishBeanFactoryInitialization(beanFactory);这个巨头了，这里会负责初始化所有的 singleton beans。 注意，后面的描述中，我都会使用初始化或预初始化来代表这个阶段。主要是 Spring 需要在这个阶段完成所有的 singleton beans 的实例化。 我们来总结一下，到目前为止，应该说 BeanFactory 已经创建完成，并且所有的实现了 BeanFactoryPostProcessor 接口的 Bean 都已经初始化并且其中的 postProcessBeanFactory(factory) 方法已经得到执行了。所有实现了 BeanPostProcessor 接口的 Bean 也都完成了初始化。 剩下的就是初始化其他还没被初始化的 singleton beans 了，我们知道它们是单例的，如果没有设置懒加载，那么 Spring 会在接下来初始化所有的 singleton beans。 // AbstractApplicationContext.java 8341234567891011121314151617181920212223242526272829303132333435363738394041// 初始化剩余的 singleton beansprotected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123; // 首先，初始化名字为 conversionService 的 Bean。本着送佛送到西的精神，我在附录中简单介绍了一下 ConversionService，因为这实在太实用了 // 什么，看代码这里没有初始化 Bean 啊！ // 注意了，初始化的动作包装在 beanFactory.getBean(...) 中，这里先不说细节，先往下看吧 if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp; beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) &#123; beanFactory.setConversionService( beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)); &#125; // Register a default embedded value resolver if no bean post-processor // (such as a PropertyPlaceholderConfigurer bean) registered any before: // at this point, primarily for resolution in annotation attribute values. if (!beanFactory.hasEmbeddedValueResolver()) &#123; beanFactory.addEmbeddedValueResolver(new StringValueResolver() &#123; @Override public String resolveStringValue(String strVal) &#123; return getEnvironment().resolvePlaceholders(strVal); &#125; &#125;); &#125; // 先初始化 LoadTimeWeaverAware 类型的 Bean // 一般用于织入第三方模块，在 class 文件载入 JVM 的时候动态织入，这里不展开说 String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false); for (String weaverAwareName : weaverAwareNames) &#123; getBean(weaverAwareName); &#125; // Stop using the temporary ClassLoader for type matching. beanFactory.setTempClassLoader(null); // 没什么别的目的，因为到这一步的时候，Spring 已经开始预初始化 singleton beans 了， // 肯定不希望这个时候还出现 bean 定义解析、加载、注册。 beanFactory.freezeConfiguration(); // 开始初始化剩下的 beanFactory.preInstantiateSingletons();&#125; 从上面最后一行往里看，我们又回到 DefaultListableBeanFactory 这个类了，这个类大家应该都不陌生了吧。 preInstantiateSingletons // DefaultListableBeanFactory 728123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869@Overridepublic void preInstantiateSingletons() throws BeansException &#123; if (this.logger.isDebugEnabled()) &#123; this.logger.debug("Pre-instantiating singletons in " + this); &#125; List&lt;String&gt; beanNames = new ArrayList&lt;String&gt;(this.beanDefinitionNames); // 触发所有的非懒加载的 singleton beans 的初始化操作 for (String beanName : beanNames) &#123; // 合并父 Bean 中的配置，注意 &lt;bean id="" class="" parent="" /&gt; 中的 parent，用的不多吧， // 考虑到这可能会影响大家的理解，我在附录中解释了一下 "Bean 继承"，请移步 RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); // 非抽象、非懒加载的 singletons。如果配置了 'abstract = true'，那是不需要初始化的 if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123; // 处理 FactoryBean(读者如果不熟悉 FactoryBean，请移步附录区了解) if (isFactoryBean(beanName)) &#123; // FactoryBean 的话，在 beanName 前面加上 ‘&amp;’ 符号。再调用 getBean，getBean 方法别急 final FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) getBean(FACTORY_BEAN_PREFIX + beanName); // 判断当前 FactoryBean 是否是 SmartFactoryBean 的实现，此处忽略，直接跳过 boolean isEagerInit; if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean) &#123; isEagerInit = AccessController.doPrivileged(new PrivilegedAction&lt;Boolean&gt;() &#123; @Override public Boolean run() &#123; return ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit(); &#125; &#125;, getAccessControlContext()); &#125; else &#123; isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit()); &#125; if (isEagerInit) &#123; getBean(beanName); &#125; &#125; else &#123; // 对于普通的 Bean，只要调用 getBean(beanName) 这个方法就可以进行初始化了 getBean(beanName); &#125; &#125; &#125; // 到这里说明所有的非懒加载的 singleton beans 已经完成了初始化 // 如果我们定义的 bean 是实现了 SmartInitializingSingleton 接口的，那么在这里得到回调，忽略 for (String beanName : beanNames) &#123; Object singletonInstance = getSingleton(beanName); if (singletonInstance instanceof SmartInitializingSingleton) &#123; final SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; if (System.getSecurityManager() != null) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; @Override public Object run() &#123; smartSingleton.afterSingletonsInstantiated(); return null; &#125; &#125;, getAccessControlContext()); &#125; else &#123; smartSingleton.afterSingletonsInstantiated(); &#125; &#125; &#125;&#125; 接下来，我们就进入到 getBean(beanName) 方法了，这个方法我们经常用来从 BeanFactory 中获取一个 Bean，而初始化的过程也封装到了这个方法里。 getBean 在继续前进之前，读者应该具备 FactoryBean 的知识，如果读者还不熟悉，请移步附录部分了解 FactoryBean。 // AbstractBeanFactory 196123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175@Overridepublic Object getBean(String name) throws BeansException &#123; return doGetBean(name, null, null, false);&#125;// 我们在剖析初始化 Bean 的过程，但是 getBean 方法我们经常是用来从容器中获取 Bean 用的，注意切换思路，// 已经初始化过了就从容器中直接返回，否则就先初始化再返回@SuppressWarnings("unchecked")protected &lt;T&gt; T doGetBean( final String name, final Class&lt;T&gt; requiredType, final Object[] args, boolean typeCheckOnly) throws BeansException &#123; // 获取一个 “正统的” beanName，处理两种情况，一个是前面说的 FactoryBean(前面带 ‘&amp;’)， // 一个是别名问题，因为这个方法是 getBean，获取 Bean 用的，你要是传一个别名进来，是完全可以的 final String beanName = transformedBeanName(name); // 注意跟着这个，这个是返回值 Object bean; // 检查下是不是已经创建过了 Object sharedInstance = getSingleton(beanName); // 这里说下 args 呗，虽然看上去一点不重要。前面我们一路进来的时候都是 getBean(beanName)， // 所以 args 其实是 null 的，但是如果 args 不为空的时候，那么意味着调用方不是希望获取 Bean，而是创建 Bean if (sharedInstance != null &amp;&amp; args == null) &#123; if (logger.isDebugEnabled()) &#123; if (isSingletonCurrentlyInCreation(beanName)) &#123; logger.debug("..."); &#125; else &#123; logger.debug("Returning cached instance of singleton bean '" + beanName + "'"); &#125; &#125; // 下面这个方法：如果是普通 Bean 的话，直接返回 sharedInstance， // 如果是 FactoryBean 的话，返回它创建的那个实例对象 // (FactoryBean 知识，读者若不清楚请移步附录) bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); &#125; else &#123; if (isPrototypeCurrentlyInCreation(beanName)) &#123; // 当前线程已经创建过了此 beanName 的 prototype 类型的 bean，那么抛异常 throw new BeanCurrentlyInCreationException(beanName); &#125; // 检查一下这个 BeanDefinition 在容器中是否存在 BeanFactory parentBeanFactory = getParentBeanFactory(); if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) &#123; // 如果当前容器不存在这个 BeanDefinition，试试父容器中有没有 String nameToLookup = originalBeanName(name); if (args != null) &#123; // 返回父容器的查询结果 return (T) parentBeanFactory.getBean(nameToLookup, args); &#125; else &#123; // No args -&gt; delegate to standard getBean method. return parentBeanFactory.getBean(nameToLookup, requiredType); &#125; &#125; if (!typeCheckOnly) &#123; // typeCheckOnly 为 false，将当前 beanName 放入一个 alreadyCreated 的 Set 集合中。 markBeanAsCreated(beanName); &#125; /* * 稍稍总结一下： * 到这里的话，要准备创建 Bean 了，对于 singleton 的 Bean 来说，容器中还没创建过此 Bean； * 对于 prototype 的 Bean 来说，本来就是要创建一个新的 Bean。 */ try &#123; final RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); checkMergedBeanDefinition(mbd, beanName, args); // 先初始化依赖的所有 Bean，这个很好理解。 // 注意，这里的依赖指的是 depends-on 中定义的依赖 String[] dependsOn = mbd.getDependsOn(); if (dependsOn != null) &#123; for (String dep : dependsOn) &#123; // 检查是不是有循环依赖，这里的循环依赖和我们前面说的循环依赖又不一样，这里肯定是不允许出现的，不然要乱套了，读者想一下就知道了 if (isDependent(beanName, dep)) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, "Circular depends-on relationship between '" + beanName + "' and '" + dep + "'"); &#125; // 注册一下依赖关系 registerDependentBean(dep, beanName); // 先初始化被依赖项 getBean(dep); &#125; &#125; // 创建 singleton 的实例 if (mbd.isSingleton()) &#123; sharedInstance = getSingleton(beanName, new ObjectFactory&lt;Object&gt;() &#123; @Override public Object getObject() throws BeansException &#123; try &#123; // 执行创建 Bean，详情后面再说 return createBean(beanName, mbd, args); &#125; catch (BeansException ex) &#123; destroySingleton(beanName); throw ex; &#125; &#125; &#125;); bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); &#125; // 创建 prototype 的实例 else if (mbd.isPrototype()) &#123; // It's a prototype -&gt; create a new instance. Object prototypeInstance = null; try &#123; beforePrototypeCreation(beanName); // 执行创建 Bean prototypeInstance = createBean(beanName, mbd, args); &#125; finally &#123; afterPrototypeCreation(beanName); &#125; bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd); &#125; // 如果不是 singleton 和 prototype 的话，需要委托给相应的实现类来处理 else &#123; String scopeName = mbd.getScope(); final Scope scope = this.scopes.get(scopeName); if (scope == null) &#123; throw new IllegalStateException("No Scope registered for scope name '" + scopeName + "'"); &#125; try &#123; Object scopedInstance = scope.get(beanName, new ObjectFactory&lt;Object&gt;() &#123; @Override public Object getObject() throws BeansException &#123; beforePrototypeCreation(beanName); try &#123; // 执行创建 Bean return createBean(beanName, mbd, args); &#125; finally &#123; afterPrototypeCreation(beanName); &#125; &#125; &#125;); bean = getObjectForBeanInstance(scopedInstance, name, beanName, mbd); &#125; catch (IllegalStateException ex) &#123; throw new BeanCreationException(beanName, "Scope '" + scopeName + "' is not active for the current thread; consider " + "defining a scoped proxy for this bean if you intend to refer to it from a singleton", ex); &#125; &#125; &#125; catch (BeansException ex) &#123; cleanupAfterBeanCreationFailure(beanName); throw ex; &#125; &#125; // 最后，检查一下类型对不对，不对的话就抛异常，对的话就返回了 if (requiredType != null &amp;&amp; bean != null &amp;&amp; !requiredType.isInstance(bean)) &#123; try &#123; return getTypeConverter().convertIfNecessary(bean, requiredType); &#125; catch (TypeMismatchException ex) &#123; if (logger.isDebugEnabled()) &#123; logger.debug("Failed to convert bean '" + name + "' to required type '" + ClassUtils.getQualifiedName(requiredType) + "'", ex); &#125; throw new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass()); &#125; &#125; return (T) bean;&#125; 大家应该也猜到了，接下来当然是分析 createBean 方法：1protected abstract Object createBean(String beanName, RootBeanDefinition mbd, Object[] args) throws BeanCreationException; 第三个参数 args 数组代表创建实例需要的参数，不就是给构造方法用的参数，或者是工厂 Bean 的参数嘛，不过要注意，在我们的初始化阶段，args 是 null。 这回我们要到一个新的类了 AbstractAutowireCapableBeanFactory，看类名，AutowireCapable？类名是不是也说明了点问题了。 主要是为了以下场景，采用 @Autowired 注解注入属性值：12345678public class MessageServiceImpl implements MessageService &#123; @Autowired private UserService userService; public String getMessage() &#123; return userService.getMessage(); &#125;&#125; 1&lt;bean id="messageService" class="com.javadoop.example.MessageServiceImpl" /&gt; 好了，读者要知道这么回事就可以了，继续向前。 // AbstractAutowireCapableBeanFactory 44712345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Central method of this class: creates a bean instance, * populates the bean instance, applies post-processors, etc. * @see #doCreateBean */@Overrideprotected Object createBean(String beanName, RootBeanDefinition mbd, Object[] args) throws BeanCreationException &#123; if (logger.isDebugEnabled()) &#123; logger.debug("Creating instance of bean '" + beanName + "'"); &#125; RootBeanDefinition mbdToUse = mbd; // 确保 BeanDefinition 中的 Class 被加载 Class&lt;?&gt; resolvedClass = resolveBeanClass(mbd, beanName); if (resolvedClass != null &amp;&amp; !mbd.hasBeanClass() &amp;&amp; mbd.getBeanClassName() != null) &#123; mbdToUse = new RootBeanDefinition(mbd); mbdToUse.setBeanClass(resolvedClass); &#125; // 准备方法覆写，这里又涉及到一个概念：MethodOverrides，它来自于 bean 定义中的 &lt;lookup-method /&gt; // 和 &lt;replaced-method /&gt;，如果读者感兴趣，回到 bean 解析的地方看看对这两个标签的解析。 // 我在附录中也对这两个标签的相关知识点进行了介绍，读者可以移步去看看 try &#123; mbdToUse.prepareMethodOverrides(); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanDefinitionStoreException(mbdToUse.getResourceDescription(), beanName, "Validation of method overrides failed", ex); &#125; try &#123; // 让 BeanPostProcessor 在这一步有机会返回代理，而不是 bean 实例， // 要彻底了解清楚这个，需要去看 InstantiationAwareBeanPostProcessor 接口，这里就不展开说了 Object bean = resolveBeforeInstantiation(beanName, mbdToUse); if (bean != null) &#123; return bean; &#125; &#125; catch (Throwable ex) &#123; throw new BeanCreationException(mbdToUse.getResourceDescription(), beanName, "BeanPostProcessor before instantiation of bean failed", ex); &#125; // 重头戏，创建 bean Object beanInstance = doCreateBean(beanName, mbdToUse, args); if (logger.isDebugEnabled()) &#123; logger.debug("Finished creating instance of bean '" + beanName + "'"); &#125; return beanInstance;&#125; 创建 Bean 往里看 doCreateBean 这个方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125/** * Actually create the specified bean. Pre-creation processing has already happened * at this point, e.g. checking &#123;@code postProcessBeforeInstantiation&#125; callbacks. * &lt;p&gt;Differentiates between default bean instantiation, use of a * factory method, and autowiring a constructor. * @param beanName the name of the bean * @param mbd the merged bean definition for the bean * @param args explicit arguments to use for constructor or factory method invocation * @return a new instance of the bean * @throws BeanCreationException if the bean could not be created * @see #instantiateBean * @see #instantiateUsingFactoryMethod * @see #autowireConstructor */protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final Object[] args) throws BeanCreationException &#123; // Instantiate the bean. BeanWrapper instanceWrapper = null; if (mbd.isSingleton()) &#123; instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); &#125; if (instanceWrapper == null) &#123; // 说明不是 FactoryBean，这里实例化 Bean，这里非常关键，细节之后再说 instanceWrapper = createBeanInstance(beanName, mbd, args); &#125; // 这个就是 Bean 里面的 我们定义的类 的实例，很多地方我描述成 "bean 实例" final Object bean = (instanceWrapper != null ? instanceWrapper.getWrappedInstance() : null); // 类型 Class&lt;?&gt; beanType = (instanceWrapper != null ? instanceWrapper.getWrappedClass() : null); mbd.resolvedTargetType = beanType; // 建议跳过吧，涉及接口：MergedBeanDefinitionPostProcessor synchronized (mbd.postProcessingLock) &#123; if (!mbd.postProcessed) &#123; try &#123; // MergedBeanDefinitionPostProcessor，这个我真不展开说了，直接跳过吧，很少用的 applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, "Post-processing of merged bean definition failed", ex); &#125; mbd.postProcessed = true; &#125; &#125; // Eagerly cache singletons to be able to resolve circular references // even when triggered by lifecycle interfaces like BeanFactoryAware. // 下面这块代码是为了解决循环依赖的问题，以后有时间，我再对循环依赖这个问题进行解析吧 boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) &#123; if (logger.isDebugEnabled()) &#123; logger.debug("Eagerly caching bean '" + beanName + "' to allow for resolving potential circular references"); &#125; addSingletonFactory(beanName, new ObjectFactory&lt;Object&gt;() &#123; @Override public Object getObject() throws BeansException &#123; return getEarlyBeanReference(beanName, mbd, bean); &#125; &#125;); &#125; // Initialize the bean instance. Object exposedObject = bean; try &#123; // 这一步也是非常关键的，这一步负责属性装配，因为前面的实例只是实例化了，并没有设值，这里就是设值 populateBean(beanName, mbd, instanceWrapper); if (exposedObject != null) &#123; // 还记得 init-method 吗？还有 InitializingBean 接口？还有 BeanPostProcessor 接口？ // 这里就是处理 bean 初始化完成后的各种回调 exposedObject = initializeBean(beanName, exposedObject, mbd); &#125; &#125; catch (Throwable ex) &#123; if (ex instanceof BeanCreationException &amp;&amp; beanName.equals(((BeanCreationException) ex).getBeanName())) &#123; throw (BeanCreationException) ex; &#125; else &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, "Initialization of bean failed", ex); &#125; &#125; if (earlySingletonExposure) &#123; // Object earlySingletonReference = getSingleton(beanName, false); if (earlySingletonReference != null) &#123; if (exposedObject == bean) &#123; exposedObject = earlySingletonReference; &#125; else if (!this.allowRawInjectionDespiteWrapping &amp;&amp; hasDependentBean(beanName)) &#123; String[] dependentBeans = getDependentBeans(beanName); Set&lt;String&gt; actualDependentBeans = new LinkedHashSet&lt;String&gt;(dependentBeans.length); for (String dependentBean : dependentBeans) &#123; if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) &#123; actualDependentBeans.add(dependentBean); &#125; &#125; if (!actualDependentBeans.isEmpty()) &#123; throw new BeanCurrentlyInCreationException(beanName, "Bean with name '" + beanName + "' has been injected into other beans [" + StringUtils.collectionToCommaDelimitedString(actualDependentBeans) + "] in its raw version as part of a circular reference, but has eventually been " + "wrapped. This means that said other beans do not use the final version of the " + "bean. This is often the result of over-eager type matching - consider using " + "'getBeanNamesOfType' with the 'allowEagerInit' flag turned off, for example."); &#125; &#125; &#125; &#125; // Register bean as disposable. try &#123; registerDisposableBeanIfNecessary(beanName, bean, mbd); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, "Invalid destruction signature", ex); &#125; return exposedObject;&#125; 到这里，我们已经分析完了 doCreateBean 方法，总的来说，我们已经说完了整个初始化流程。 接下来我们挑 doCreateBean 中的三个细节出来说说。一个是创建 Bean 实例的 createBeanInstance 方法，一个是依赖注入的 populateBean 方法，还有就是回调方法 initializeBean。 注意了，接下来的这三个方法要认真说那也是极其复杂的，很多地方我就点到为止了，感兴趣的读者可以自己往里看，最好就是碰到不懂的，自己写代码去调试它。 创建 Bean 实例 我们先看看 createBeanInstance 方法。需要说明的是，这个方法如果每个分支都分析下去，必然也是极其复杂冗长的，我们挑重点说。此方法的目的就是实例化我们指定的类。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950protected BeanWrapper createBeanInstance(String beanName, RootBeanDefinition mbd, Object[] args) &#123; // 确保已经加载了此 class Class&lt;?&gt; beanClass = resolveBeanClass(mbd, beanName); // 校验一下这个类的访问权限 if (beanClass != null &amp;&amp; !Modifier.isPublic(beanClass.getModifiers()) &amp;&amp; !mbd.isNonPublicAccessAllowed()) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, "Bean class isn't public, and non-public access not allowed: " + beanClass.getName()); &#125; if (mbd.getFactoryMethodName() != null) &#123; // 采用工厂方法实例化，不熟悉这个概念的读者请看附录，注意，不是 FactoryBean return instantiateUsingFactoryMethod(beanName, mbd, args); &#125; // 如果不是第一次创建，比如第二次创建 prototype bean。 // 这种情况下，我们可以从第一次创建知道，采用无参构造函数，还是构造函数依赖注入 来完成实例化 boolean resolved = false; boolean autowireNecessary = false; if (args == null) &#123; synchronized (mbd.constructorArgumentLock) &#123; if (mbd.resolvedConstructorOrFactoryMethod != null) &#123; resolved = true; autowireNecessary = mbd.constructorArgumentsResolved; &#125; &#125; &#125; if (resolved) &#123; if (autowireNecessary) &#123; // 构造函数依赖注入 return autowireConstructor(beanName, mbd, null, null); &#125; else &#123; // 无参构造函数 return instantiateBean(beanName, mbd); &#125; &#125; // 判断是否采用有参构造函数 Constructor&lt;?&gt;[] ctors = determineConstructorsFromBeanPostProcessors(beanClass, beanName); if (ctors != null || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_CONSTRUCTOR || mbd.hasConstructorArgumentValues() || !ObjectUtils.isEmpty(args)) &#123; // 构造函数依赖注入 return autowireConstructor(beanName, mbd, ctors, args); &#125; // 调用无参构造函数 return instantiateBean(beanName, mbd);&#125; 挑个简单的无参构造函数构造实例来看看：123456789101112131415161718192021222324252627protected BeanWrapper instantiateBean(final String beanName, final RootBeanDefinition mbd) &#123; try &#123; Object beanInstance; final BeanFactory parent = this; if (System.getSecurityManager() != null) &#123; beanInstance = AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; @Override public Object run() &#123; return getInstantiationStrategy().instantiate(mbd, beanName, parent); &#125; &#125;, getAccessControlContext()); &#125; else &#123; // 实例化 beanInstance = getInstantiationStrategy().instantiate(mbd, beanName, parent); &#125; // 包装一下，返回 BeanWrapper bw = new BeanWrapperImpl(beanInstance); initBeanWrapper(bw); return bw; &#125; catch (Throwable ex) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, "Instantiation of bean failed", ex); &#125;&#125; 我们可以看到，关键的地方在于：1beanInstance = getInstantiationStrategy().instantiate(mbd, beanName, parent); 这里会进行实际的实例化过程，我们进去看看: // SimpleInstantiationStrategy 591234567891011121314151617181920212223242526272829303132333435363738394041@Overridepublic Object instantiate(RootBeanDefinition bd, String beanName, BeanFactory owner) &#123; // 如果不存在方法覆写，那就使用 java 反射进行实例化，否则使用 CGLIB, // 方法覆写 请参见附录"方法注入"中对 lookup-method 和 replaced-method 的介绍 if (bd.getMethodOverrides().isEmpty()) &#123; Constructor&lt;?&gt; constructorToUse; synchronized (bd.constructorArgumentLock) &#123; constructorToUse = (Constructor&lt;?&gt;) bd.resolvedConstructorOrFactoryMethod; if (constructorToUse == null) &#123; final Class&lt;?&gt; clazz = bd.getBeanClass(); if (clazz.isInterface()) &#123; throw new BeanInstantiationException(clazz, "Specified class is an interface"); &#125; try &#123; if (System.getSecurityManager() != null) &#123; constructorToUse = AccessController.doPrivileged(new PrivilegedExceptionAction&lt;Constructor&lt;?&gt;&gt;() &#123; @Override public Constructor&lt;?&gt; run() throws Exception &#123; return clazz.getDeclaredConstructor((Class[]) null); &#125; &#125;); &#125; else &#123; constructorToUse = clazz.getDeclaredConstructor((Class[]) null); &#125; bd.resolvedConstructorOrFactoryMethod = constructorToUse; &#125; catch (Throwable ex) &#123; throw new BeanInstantiationException(clazz, "No default constructor found", ex); &#125; &#125; &#125; // 利用构造方法进行实例化 return BeanUtils.instantiateClass(constructorToUse); &#125; else &#123; // 存在方法覆写，利用 CGLIB 来完成实例化，需要依赖于 CGLIB 生成子类，这里就不展开了 return instantiateWithMethodInjection(bd, beanName, owner); &#125;&#125; 到这里，我们就算实例化完成了。我们开始说怎么进行属性注入。 bean 属性注入 // AbstractAutowireCapableBeanFactory 1203123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778protected void populateBean(String beanName, RootBeanDefinition mbd, BeanWrapper bw) &#123; // bean 实例的所有属性都在这里了 PropertyValues pvs = mbd.getPropertyValues(); if (bw == null) &#123; if (!pvs.isEmpty()) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, "Cannot apply property values to null instance"); &#125; else &#123; // Skip property population phase for null instance. return; &#125; &#125; // 到这步的时候，bean 实例化完成（通过工厂方法或构造方法），但是还没开始属性设值， // InstantiationAwareBeanPostProcessor 的实现类可以在这里对 bean 进行状态修改， // 我也没找到有实际的使用，所以我们暂且忽略这块吧 boolean continueWithPropertyPopulation = true; if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; // 如果返回 false，代表不需要进行后续的属性设值，也不需要再经过其他的 BeanPostProcessor 的处理 if (!ibp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName)) &#123; continueWithPropertyPopulation = false; break; &#125; &#125; &#125; &#125; if (!continueWithPropertyPopulation) &#123; return; &#125; if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; MutablePropertyValues newPvs = new MutablePropertyValues(pvs); // 通过名字找到所有属性值，如果是 bean 依赖，先初始化依赖的 bean。记录依赖关系 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME) &#123; autowireByName(beanName, mbd, bw, newPvs); &#125; // 通过类型装配。复杂一些 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; autowireByType(beanName, mbd, bw, newPvs); &#125; pvs = newPvs; &#125; boolean hasInstAwareBpps = hasInstantiationAwareBeanPostProcessors(); boolean needsDepCheck = (mbd.getDependencyCheck() != RootBeanDefinition.DEPENDENCY_CHECK_NONE); if (hasInstAwareBpps || needsDepCheck) &#123; PropertyDescriptor[] filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); if (hasInstAwareBpps) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; // 这里有个非常有用的 BeanPostProcessor 进到这里: AutowiredAnnotationBeanPostProcessor // 对采用 @Autowired、@Value 注解的依赖进行设值，这里的内容也是非常丰富的，不过本文不会展开说了，感兴趣的读者请自行研究 pvs = ibp.postProcessPropertyValues(pvs, filteredPds, bw.getWrappedInstance(), beanName); if (pvs == null) &#123; return; &#125; &#125; &#125; &#125; if (needsDepCheck) &#123; checkDependencies(beanName, mbd, filteredPds, pvs); &#125; &#125; // 设置 bean 实例的属性值 applyPropertyValues(beanName, mbd, bw, pvs);&#125; initializeBean 属性注入完成后，这一步其实就是处理各种回调了。 protected Object initializeBean(final String beanName, final Object bean, RootBeanDefinition mbd) { if (System.getSecurityManager() != null) { AccessController.doPrivileged(new PrivilegedAction() { @Override public Object run() { invokeAwareMethods(beanName, bean); return null; } }, getAccessControlContext()); } else { // 如果 bean 实现了 BeanNameAware、BeanClassLoaderAware 或 BeanFactoryAware 接口，回调 invokeAwareMethods(beanName, bean); } Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) { // BeanPostProcessor 的 postProcessBeforeInitialization 回调 wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); } try { // 处理 bean 中定义的 init-method， // 或者如果 bean 实现了 InitializingBean 接口，调用 afterPropertiesSet() 方法 invokeInitMethods(beanName, wrappedBean, mbd); } catch (Throwable ex) { throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, “Invocation of init method failed”, ex); } if (mbd == null || !mbd.isSynthetic()) { // BeanPostProcessor 的 postProcessAfterInitialization 回调 wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); } return wrappedBean;}大家发现没有，BeanPostProcessor 的两个回调都发生在这边，只不过中间处理了 init-method，是不是和读者原来的认知有点不一样了？ 附录id 和 name每个 Bean 在 Spring 容器中都有一个唯一的名字（beanName）和 0 个或多个别名（aliases）。 我们从 Spring 容器中获取 Bean 的时候，可以根据 beanName，也可以通过别名。1beanFactory.getBean("beanName or alias"); 在配置 的过程中，我们可以配置 id 和 name，看几个例子就知道是怎么回事了。1234567891011# beanName 为 messageService，别名有 3 个，分别为 m1、m2、m3&lt;bean id="messageService" name="m1, m2, m3" class="com.javadoop.example.MessageServiceImpl"&gt;# beanName 为 m1，别名有 2 个，分别为 m2、m3&lt;bean name="m1, m2, m3" class="com.javadoop.example.MessageServiceImpl" /&gt;# beanName 为：com.javadoop.example.MessageServiceImpl#0，别名为： com.javadoop.example.MessageServiceImpl&lt;bean class="com.javadoop.example.MessageServiceImpl"&gt;# beanName 为 messageService，没有别名。&lt;bean id="messageService" class="com.javadoop.example.MessageServiceImpl"&gt; 配置是否允许 Bean 覆盖、是否允许循环依赖我们说过，默认情况下，allowBeanDefinitionOverriding 属性为 null。**如果在同一配置文件中 Bean id 或 name 重复了，会抛错，但是如果不是同一配置文件中，会发生覆盖。 可是有些时候我们希望在系统启动的过程中就严格杜绝发生 Bean 覆盖，因为万一出现这种情况，会增加我们排查问题的成本。 循环依赖说的是 A 依赖 B，而 B 又依赖 A。或者是 A 依赖 B，B 依赖 C，而 C 却依赖 A。默认 allowCircularReferences 也是 null。 它们两个属性是一起出现的，必然可以在同一个地方一起进行配置。 添加这两个属性的作者 Juergen Hoeller 在这个 jira 的讨论中说明了怎么配置这两个属性。1234567891011121314151617public class NoBeanOverridingContextLoader extends ContextLoader &#123; @Override protected void customizeContext(ServletContext servletContext, ConfigurableWebApplicationContext applicationContext) &#123; super.customizeContext(servletContext, applicationContext); AbstractRefreshableApplicationContext arac = (AbstractRefreshableApplicationContext) applicationContext; arac.setAllowBeanDefinitionOverriding(false); &#125;&#125;public class MyContextLoaderListener extends org.springframework.web.context.ContextLoaderListener &#123; @Override protected ContextLoader createContextLoader() &#123; return new NoBeanOverridingContextLoader(); &#125;&#125; 123&lt;listener&gt; &lt;listener-class&gt;com.javadoop.MyContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; 如果以上方式不能满足你的需求，请参考这个链接：解决spring中不同配置文件中存在name或者id相同的bean可能引起的问题。 profile我们可以把不同环境的配置分别配置到单独的文件中，举个例子：1234567891011&lt;beans profile="development" xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:jdbc="http://www.springframework.org/schema/jdbc" xsi:schemaLocation="..."&gt; &lt;jdbc:embedded-database id="dataSource"&gt; &lt;jdbc:script location="classpath:com/bank/config/sql/schema.sql"/&gt; &lt;jdbc:script location="classpath:com/bank/config/sql/test-data.sql"/&gt; &lt;/jdbc:embedded-database&gt;&lt;/beans&gt; 12345678&lt;beans profile="production" xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:jee="http://www.springframework.org/schema/jee" xsi:schemaLocation="..."&gt; &lt;jee:jndi-lookup id="dataSource" jndi-name="java:comp/env/jdbc/datasource"/&gt;&lt;/beans&gt; 应该不必做过多解释了吧，看每个文件第一行的 profile=””。 当然，我们也可以在一个配置文件中使用：1234567891011121314151617&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:jdbc="http://www.springframework.org/schema/jdbc" xmlns:jee="http://www.springframework.org/schema/jee" xsi:schemaLocation="..."&gt; &lt;beans profile="development"&gt; &lt;jdbc:embedded-database id="dataSource"&gt; &lt;jdbc:script location="classpath:com/bank/config/sql/schema.sql"/&gt; &lt;jdbc:script location="classpath:com/bank/config/sql/test-data.sql"/&gt; &lt;/jdbc:embedded-database&gt; &lt;/beans&gt; &lt;beans profile="production"&gt; &lt;jee:jndi-lookup id="dataSource" jndi-name="java:comp/env/jdbc/datasource"/&gt; &lt;/beans&gt;&lt;/beans&gt; 理解起来也很简单吧。 接下来的问题是，怎么使用特定的 profile 呢？Spring 在启动的过程中，会去寻找 “spring.profiles.active” 的属性值，根据这个属性值来的。那怎么配置这个值呢？ Spring 会在这几个地方寻找spring.profiles.active的属性值：操作系统环境变量、JVM 系统变量、web.xml 中定义的参数、JNDI。 最简单的方式莫过于在程序启动的时候指定：1-Dspring.profiles.active=&quot;profile1,profile2&quot; profile 可以激活多个。当然，我们也可以通过代码的形式从 Environment 中设置 profile：1234AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext();ctx.getEnvironment().setActiveProfiles("development");ctx.register(SomeConfig.class, StandaloneDataConfig.class, JndiDataConfig.class);ctx.refresh(); // 重启 如果是 Spring Boot 的话更简单，我们一般会创建 application.properties、application-dev.properties、application-prod.properties 等文件，其中 application.properties 配置各个环境通用的配置，application-{profile}.properties 中配置特定环境的配置，然后在启动的时候指定 profile：1java -Dspring.profiles.active=prod -jar JavaDoop.jar 如果是单元测试中使用的话，在测试类中使用 @ActiveProfiles 指定，这里就不展开了。 工厂模式生成 Bean请读者注意 factory-bean 和 FactoryBean 的区别。这节说的是前者，是说静态工厂或实例工厂，而后者是 Spring 中的特殊接口，代表一类特殊的 Bean，附录的下面一节会介绍 FactoryBean。 设计模式里，工厂方法模式分静态工厂和实例工厂，我们分别看看 Spring 中怎么配置这两个，来个代码示例就什么都清楚了。 静态工厂：12345678910111213&lt;bean id="clientService" class="examples.ClientService" factory-method="createInstance"/&gt;public class ClientService &#123; private static ClientService clientService = new ClientService(); private ClientService() &#123;&#125; // 静态方法 public static ClientService createInstance() &#123; return clientService; &#125;&#125; 实例工厂：1234567891011121314151617181920212223242526&lt;bean id="serviceLocator" class="examples.DefaultServiceLocator"&gt; &lt;!-- inject any dependencies required by this locator bean --&gt;&lt;/bean&gt;&lt;bean id="clientService" factory-bean="serviceLocator" factory-method="createClientServiceInstance"/&gt;&lt;bean id="accountService" factory-bean="serviceLocator" factory-method="createAccountServiceInstance"/&gt;public class DefaultServiceLocator &#123; private static ClientService clientService = new ClientServiceImpl(); private static AccountService accountService = new AccountServiceImpl(); public ClientService createClientServiceInstance() &#123; return clientService; &#125; public AccountService createAccountServiceInstance() &#123; return accountService; &#125;&#125; FactoryBeanFactoryBean 适用于 Bean 的创建过程比较复杂的场景，比如数据库连接池的创建。 123456789public interface FactoryBean&lt;T&gt; &#123; T getObject() throws Exception; Class&lt;T&gt; getObjectType(); boolean isSingleton();&#125;public class Person &#123; private Car car ; private void setCar(Car car)&#123; this.car = car; &#125; &#125; 我们假设现在需要创建一个 Person 的 Bean，首先我们需要一个 Car 的实例，我们这里假设 Car 的实例创建很麻烦，那么我们可以把创建 Car 的复杂过程包装起来：123456789101112131415161718192021public class MyCarFactoryBean implements FactoryBean&lt;Car&gt;&#123; private String make; private int year ; public void setMake(String m)&#123; this.make =m ; &#125; public void setYear(int y)&#123; this.year = y; &#125; public Car getObject()&#123; // 这里我们假设 Car 的实例化过程非常复杂，反正就不是几行代码可以写完的那种 CarBuilder cb = CarBuilder.car(); if(year!=0) cb.setYear(this.year); if(StringUtils.hasText(this.make)) cb.setMake( this.make ); return cb.factory(); &#125; public Class&lt;Car&gt; getObjectType() &#123; return Car.class ; &#125; public boolean isSingleton() &#123; return false; &#125;&#125; 我们看看装配的时候是怎么配置的：12345678&lt;bean class = "com.javadoop.MyCarFactoryBean" id = "car"&gt; &lt;property name = "make" value ="Honda"/&gt; &lt;property name = "year" value ="1984"/&gt;&lt;/bean&gt;&lt;bean class = "com.javadoop.Person" id = "josh"&gt; &lt;property name = "car" ref = "car"/&gt;&lt;/bean&gt; 看到不一样了吗？id 为 “car” 的 bean 其实指定的是一个 FactoryBean，不过配置的时候，我们直接让配置 Person 的 Bean 直接依赖于这个 FactoryBean 就可以了。中间的过程 Spring 已经封装好了。 说到这里，我们再来点干货。我们知道，现在还用 xml 配置 Bean 依赖的越来越少了，更多时候，我们可能会采用 java config 的方式来配置，这里有什么不一样呢？12345678910111213141516171819@Configurationpublic class CarConfiguration &#123; @Bean public MyCarFactoryBean carFactoryBean()&#123; MyCarFactoryBean cfb = new MyCarFactoryBean(); cfb.setMake("Honda"); cfb.setYear(1984); return cfb; &#125; @Bean public Person aPerson()&#123; Person person = new Person(); // 注意这里的不同 person.setCar(carFactoryBean().getObject()); return person; &#125;&#125; 这个时候，其实我们的思路也很简单，把 MyCarFactoryBean 看成是一个简单的 Bean 就可以了，不必理会什么 FactoryBean，它是不是 FactoryBean 和我们没关系。 初始化 Bean 的回调有以下四种方案：12345678910111213141516&lt;bean id="exampleInitBean" class="examples.ExampleBean" init-method="init"/&gt;public class AnotherExampleBean implements InitializingBean &#123; public void afterPropertiesSet() &#123; // do some initialization work &#125;&#125;@Bean(initMethod = "init")public Foo foo() &#123; return new Foo();&#125;@PostConstructpublic void init() &#123;&#125; 销毁 Bean 的回调12345678910111213141516&lt;bean id="exampleInitBean" class="examples.ExampleBean" destroy-method="cleanup"/&gt;public class AnotherExampleBean implements DisposableBean &#123; public void destroy() &#123; // do some destruction work (like releasing pooled connections) &#125;&#125;@Bean(destroyMethod = "cleanup")public Bar bar() &#123; return new Bar();&#125;@PreDestroypublic void cleanup() &#123;&#125; ConversionService既然文中说到了这个，顺便提一下好了。 最有用的场景就是，它用来将前端传过来的参数和后端的 controller 方法上的参数进行绑定的时候用。 像前端传过来的字符串、整数要转换为后端的 String、Integer 很容易，但是如果 controller 方法需要的是一个枚举值，或者是 Date 这些非基础类型（含基础类型包装类）值的时候，我们就可以考虑采用 ConversionService 来进行转换。12345678&lt;bean id="conversionService" class="org.springframework.context.support.ConversionServiceFactoryBean"&gt; &lt;property name="converters"&gt; &lt;list&gt; &lt;bean class="com.javadoop.learning.utils.StringToEnumConverterFactory"/&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; ConversionService 接口很简单，所以要自定义一个 convert 的话也很简单。 下面再说一个实现这种转换很简单的方式，那就是实现 Converter 接口。 来看一个很简单的例子，这样比什么都管用。1234567891011public class StringToDateConverter implements Converter&lt;String, Date&gt; &#123; @Override public Date convert(String source) &#123; try &#123; return DateUtils.parseDate(source, "yyyy-MM-dd", "yyyy-MM-dd HH:mm:ss", "yyyy-MM-dd HH:mm", "HH:mm:ss", "HH:mm"); &#125; catch (ParseException e) &#123; return null; &#125; &#125;&#125; 只要注册这个 Bean 就可以了。这样，前端往后端传的时间描述字符串就很容易绑定成 Date 类型了，不需要其他任何操作。 Bean 继承在初始化 Bean 的地方，我们说过了这个：1RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); 这里涉及到的就是 &lt;bean parent=&quot;&quot; /&gt; 中的 parent 属性，我们来看看 Spring 中是用这个来干什么的。 首先，我们要明白，这里的继承和 java 语法中的继承没有任何关系，不过思路是相通的。child bean 会继承 parent bean 的所有配置，也可以覆盖一些配置，当然也可以新增额外的配置。 Spring 中提供了继承自 AbstractBeanDefinition 的 ChildBeanDefinition 来表示 child bean。 看如下一个例子:12345678910&lt;bean id="inheritedTestBean" abstract="true" class="org.springframework.beans.TestBean"&gt; &lt;property name="name" value="parent"/&gt; &lt;property name="age" value="1"/&gt;&lt;/bean&gt;&lt;bean id="inheritsWithDifferentClass" class="org.springframework.beans.DerivedTestBean" parent="inheritedTestBean" init-method="initialize"&gt; &lt;property name="name" value="override"/&gt;&lt;/bean&gt; parent bean 设置了 abstract=”true” 所以它不会被实例化，child bean 继承了 parent bean 的两个属性，但是对 name 属性进行了覆写。 child bean 会继承 scope、构造器参数值、属性值、init-method、destroy-method 等等。 当然，我不是说 parent bean 中的 abstract = true 在这里是必须的，只是说如果加上了以后 Spring 在实例化 singleton beans 的时候会忽略这个 bean。 比如下面这个极端 parent bean，它没有指定 class，所以毫无疑问，这个 bean 的作用就是用来充当模板用的 parent bean，此处就必须加上 abstract = true。1234&lt;bean id="inheritedTestBeanWithoutClass" abstract="true"&gt; &lt;property name="name" value="parent"/&gt; &lt;property name="age" value="1"/&gt;&lt;/bean&gt; 方法注入一般来说，我们的应用中大多数的 Bean 都是 singleton 的。singleton 依赖 singleton，或者 prototype 依赖 prototype 都很好解决，直接设置属性依赖就可以了。 但是，如果是 singleton 依赖 prototype 呢？这个时候不能用属性依赖，因为如果用属性依赖的话，我们每次其实拿到的还是第一次初始化时候的 bean。 一种解决方案就是不要用属性依赖，每次获取依赖的 bean 的时候从 BeanFactory 中取。这个也是大家最常用的方式了吧。怎么取，我就不介绍了，大部分 Spring 项目大家都会定义那么个工具类的。 BeanPostProcessor应该说 BeanPostProcessor 概念在 Spring 中也是比较重要的。我们看下接口定义： 1234567public interface BeanPostProcessor &#123; Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException; Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException;&#125; 看这个接口中的两个方法名字我们大体上可以猜测 bean 在初始化之前会执行 postProcessBeforeInitialization 这个方法，初始化完成之后会执行 postProcessAfterInitialization 这个方法。但是，这么理解是非常片面的。 首先，我们要明白，除了我们自己定义的 BeanPostProcessor 实现外，Spring 容器在启动时自动给我们也加了几个。如在获取 BeanFactory 的 obtainFactory() 方法结束后的 prepareBeanFactory(factory)，大家仔细看会发现，Spring 往容器中添加了这两个 BeanPostProcessor：ApplicationContextAwareProcessor、ApplicationListenerDetector。 我们回到这个接口本身，读者请看第一个方法，这个方法接受的第一个参数是 bean 实例，第二个参数是 bean 的名字，重点在返回值将会作为新的 bean 实例，所以，没事的话这里不能随便返回个 null。 那意味着什么呢？我们很容易想到的就是，我们这里可以对一些我们想要修饰的 bean 实例做一些事情。但是对于 Spring 框架来说，它会决定是不是要在这个方法中返回 bean 实例的代理，这样就有更大的想象空间了。 最后，我们说说如果我们自己定义一个 bean 实现 BeanPostProcessor 的话，它的执行时机是什么时候？ 如果仔细看了代码分析的话，其实很容易知道了，在 bean 实例化完成、属性注入完成之后，会执行回调方法，具体请参见类 AbstractAutowireCapableBeanFactory#initBean 方法。 首先会回调几个实现了 Aware 接口的 bean，然后就开始回调 BeanPostProcessor 的 postProcessBeforeInitialization 方法，之后是回调 init-method，然后再回调 BeanPostProcessor 的 postProcessAfterInitialization 方法。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MySQL的or/in/union与索引优化]]></title>
    <url>%2F2017%2F11%2F07%2F2017-07-15-mysq-union%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 假设订单业务表结构为：order(oid, date, uid, status, money, time, …)其中： oid，订单ID，主键 date，下单日期，有普通索引，管理后台经常按照date查询 uid，用户ID，有普通索引，用户查询自己订单 status，订单状态，有普通索引，管理后台经常按照status查询 money/time，订单金额/时间，被查询字段，无索引 … 假设订单有三种状态：0已下单，1已支付，2已完成业务需求，查询未完成的订单，哪个SQL更快呢？ select * from order where status!=2 select * from order where status=0 or status=1 select * from order where status IN (0,1) select from order where status=0union allselect from order where status=1 结论：方案1最慢，方案2，3，4都能命中索引 但是… 一：union all 肯定是能够命中索引的123select * from order where status=0union allselect * from order where status=1 说明：直接告诉MySQL怎么做，MySQL耗费的CPU最少程序员并不经常这么写SQL(union all) 二：简单的in能够命中索引1select * from order where status in (0,1) 说明：让MySQL思考，查询优化耗费的cpu比union all多，但可以忽略不计程序员最常这么写SQL(in)，这个例子，最建议这么写 三：对于or，新版的MySQL能够命中索引1select * from order where status=0 or status=1 说明：让MySQL思考，查询优化耗费的cpu比in多，别把负担交给MySQL不建议程序员频繁用or，不是所有的or都命中索引对于老版本的MySQL，建议查询分析下 四、对于!=，负向查询肯定不能命中索引1select * from order where status!=2 说明：全表扫描，效率最低，所有方案中最慢禁止使用负向查询 五、其他方案1select * from order where status &lt; 2 这个具体的例子中，确实快，但是：这个例子只举了3个状态，实际业务不止这3个状态，并且状态的“值”正好满足偏序关系，万一是查其他状态呢，SQL不宜依赖于枚举的值，方案不通用这个SQL可读性差，可理解性差，可维护性差，强烈不推荐]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL的or/in/union与索引优化]]></title>
    <url>%2F2017%2F11%2F07%2F2017-11-07-mysql%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 假设订单业务表结构为：order(oid, date, uid, status, money, time, …)其中： oid，订单ID，主键 date，下单日期，有普通索引，管理后台经常按照date查询 uid，用户ID，有普通索引，用户查询自己订单 status，订单状态，有普通索引，管理后台经常按照status查询 money/time，订单金额/时间，被查询字段，无索引 … 假设订单有三种状态：0已下单，1已支付，2已完成业务需求，查询未完成的订单，哪个SQL更快呢？ select * from order where status!=2 select * from order where status=0 or status=1 select * from order where status IN (0,1) select from order where status=0union allselect from order where status=1 结论：方案1最慢，方案2，3，4都能命中索引 但是… 一：union all 肯定是能够命中索引的123select * from order where status=0union allselect * from order where status=1 说明：直接告诉MySQL怎么做，MySQL耗费的CPU最少程序员并不经常这么写SQL(union all) 二：简单的in能够命中索引1select * from order where status in (0,1) 说明：让MySQL思考，查询优化耗费的cpu比union all多，但可以忽略不计程序员最常这么写SQL(in)，这个例子，最建议这么写 三：对于or，新版的MySQL能够命中索引1select * from order where status=0 or status=1 说明：让MySQL思考，查询优化耗费的cpu比in多，别把负担交给MySQL不建议程序员频繁用or，不是所有的or都命中索引对于老版本的MySQL，建议查询分析下 四、对于!=，负向查询肯定不能命中索引1select * from order where status!=2 说明：全表扫描，效率最低，所有方案中最慢禁止使用负向查询 五、其他方案1select * from order where status &lt; 2 这个具体的例子中，确实快，但是：这个例子只举了3个状态，实际业务不止这3个状态，并且状态的“值”正好满足偏序关系，万一是查其他状态呢，SQL不宜依赖于枚举的值，方案不通用这个SQL可读性差，可理解性差，可维护性差，强烈不推荐]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分层架构，前后端分离有啥坏处？]]></title>
    <url>%2F2017%2F11%2F07%2F2017-11-07-separation-1%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 前文《互联网分层架构，为啥要前后端分离？》详细介绍了前后端分离的缘起，很多朋友在评论中留言，纷纷表示，要往前后端分离架构上转型。 任何脱离业务发展，业务特点的架构设计都是耍流氓，不是任何公司在任何阶段都适用“前后端分离”分层架构的，今天简单聊聊实施“前后端分离”需要考虑的一些要素，供大家参考。 一、SEO上的考虑如果是 PC 端的站点，需要考虑是否需要强支持 SEO，前后端分离的架构，很可能需要搜索引擎的 spider 执行完 js 才能得到完整的可收录的页面，而“执行 js ”并不是所有搜索引擎都支持的，此时势必影响站点的收录。 当然，如果是原生 APP ，后端 node.js 只返回 json 数据，或者单页应用 SPA （对百度来说就是一个页面），则不太需要考虑这方面的问题。 二、产品特性的考虑很多产品追求酷炫的前端效果，并且对前端兼容性要求很高，前端产品改版频率很高，那么前后端分离是有必要的。 否则，前后端分离只会带来更多系统架构的复杂性。 三、公司发展阶段的考虑公司发展的初级阶段，人比较少，对产品迭代速度的要求较高，此时更多的需要一些全栈的工程师，一个人开发从前到后全搞定。如果此时实施前后端分离，将引入“联调”一说，并且增加了沟通成本比，可能导致产品迭代的速度降低。 四、人员技能考虑传统 FE 与后端 Java/PHP 工程师的合作方式， FE 工程师不需要有很深的后端功底，一旦引入前后端分离， node.js 层的前端同学需要了解更多的后端知识体系，不排除有 FE 同学对后端技能的排斥，引发人员的不稳定。 结论：前后端分离不只是一个分层架构的技术决策，和SEO、产品特性、公司发展阶段、人员知识体系相关，千万不可一概而论。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构师之路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[互联网分层架构，为啥要前后端分离？]]></title>
    <url>%2F2017%2F11%2F06%2F2017-10-25-separation%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 通用业务服务化之后，系统的典型后端结构如上： web-server通过RPC接口，从通用业务服务获取数据 biz-service通过RPC接口，从多个基础数据service获取数据 基础数据service通过DAO，从独立db/cache获取数据 db/cache存储数据 随着时间的推移，系统架构并不会一成不变，业务越来越复杂，改版越来越多，此时web-server层虽然使用了MVC架构，但以下诸多痛点是否似曾相识？ 产品追求绚丽的效果，并对设备兼容性要求高，这些需求不断折磨着使用MVC的Java工程师们（本文以Java举例） 不管是PC，还是手机H5，还是APP，应用前端展现的变化频率远远大于后端逻辑的变化频率（感谢那些喜欢做改版的产品经理），改velocity模版并不是Java工程师喜欢和擅长的工作 此时，为了缓解这些问题，一般会成立单独的前端FE部门，来负责交互与展现的研发，其职责与后端Java工程师分离开，但痛点依然没有完全解决： 一点点展现的改动，需要Java工程师们重新编译，打包，上线，重启tomcat，效率极低 原先Java工程师负责所有MVC的研发工作，现在分为Java和FE两块，需要等前端和后端都完成研发，才能一起调试整体效果，不仅增加了沟通成本，任何一块出问题，都可能导致项目延期 更具体的，看一个这样的例子，最开始产品只有PC版本，此时其系统分层架构如下： 客户端，web-server，service，非常清晰。 随着业务的发展，产品需要新增Mobile版本，Mobile版本和PC版本大部分业务逻辑都一样，唯一的区别是屏幕比较小： 信息展现的条数会比较少，即调用service服务时，传入的参数会不一样 产品功能会比较少，大部分service的调用一样，少数service不需要调用 展现，交互会有所区别（Controller） 由于工期较紧，Mobile版本的web-server一般怎么来呢？ 没错，把PC版本的工程拷贝一份，然后再做小量的修改： service调用的参数有些变化 大部分service的调用一样，少数service的调用去掉 修改展现，交互相关的代码 业务继续发展，产品又需要新增APP版本，APP版本和Mobile版本业务逻辑完全相同，唯一的区别是： Mobile版本返回html格式的数据，APP版本返回json格式的数据，然后进行本地渲染 由于工期较紧，APP版本的web-server一般怎么来呢？ 没错，把Mobile版本的工程拷贝一份，然后再做小量的修改： 把拼装html数据的代码，修改为拼装json数据 这么迭代，演化，发展，架构会变成这个样子： 端，是PC，Mobile，APP web-server接入，是PC站，M站，APP站 服务层，通用的业务服务，以及基础数据服务 这个架构图中的依赖关系是不是看上去很别扭？ 端到web-server之间连接关系很清晰 web-server与service之间的连接关系变成了蜘蛛网 PC/H5/APP的web-server层大部分业务是相同的，只有少数的逻辑/展现/交互不一样： 一旦一个服务RPC接口有稍许变化，所有web-server系统都需要升级修改 web-server之间存在大量代码拷贝 一旦拷贝代码，出现一个bug，多个子系统都需要升级修改 如何让数据的获取更加高效快捷，如何让数据生产与数据展现解耦分离呢？前后端分离的分层抽象势在必行。 通过前后端分离分层抽象： 站点展示层，node.js，负责数据的展现与交互，由FE维护 站点数据层，web-server，负责业务逻辑与json数据接口的提供，由Java工程师维护 这样的好处是： 复杂的业务逻辑与数据生成，只有在站点数据层处写了一次，没有代码拷贝 底层service接口发生变化，只有站点数据层一处需要升级修改 底层service如果有bug，只有站点数据层一处需要升级修改 站点展现层可以根据产品的不同形态，传入不同的参数，调用不同的站点数据层接口 除此之外： 产品追求绚丽的效果，并对设备兼容性要求高，不再困扰Java工程师，由更专业的FE对接 一点点展现的改动，不再需要Java工程师们重新编译，打包，上线，重启tomcat 约定好json接口后，Java和FE分开开发，FE可以用mock的接口自测，不再等待一起联调 结论：当业务越来越复杂，端上的产品越来越多，展现层的变化越来越快越来越多，站点层存在大量代码拷贝，数据获取复杂性成为通用痛点的时候，就应该进行前后端分离分层抽象，简化数据获取过程，提高数据获取效率，向上游屏蔽底层的复杂性。 最后再强调两点： 是否需要前后端分离，和业务复杂性，以及业务发展阶段有关，不可一概而论 本文强调的前后端分离的思路，实际情况下有多种实现方式，文章并没有透彻展开实现细节]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构师之路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搞定所有的跨域请求问题: jsonp & CORS]]></title>
    <url>%2F2017%2F11%2F06%2F2017-11-06-cross-domain%2F</url>
    <content type="text"><![CDATA[原文链接: https://javadoop.com/post/cross-domain 网上各种跨域教程，各种实践，各种问答，除了简单的 jsonp 以外，很多说 CORS 的都是行不通的，老是缺那么一两个关键的配置。本文只想解决问题，所有的代码经过亲自实践。 本文解决跨域中的 get、post、data、cookie 等这些问题。 本文只会说 get 请求和 post 请求，读者请把 post 请求理解成除 get 请求外的所有其他请求方式。 JSONPjsonp 的原理很简单，利用了前端请求静态资源的时候不存在跨域问题这个思路。 但是只支持 get，只支持 get，只支持 get。 注意一点，既然这个方法叫 jsonp，后端数据一定要使用 json 数据，不能随便的搞个字符串什么的，不然你会觉得结果莫名其妙的。 前端 jQuery 写法12345678$.ajax(&#123; type: "get", url: baseUrl + "/jsonp/get", dataType: "jsonp", success: function(response) &#123; $("#response").val(JSON.stringify(response)); &#125;&#125;); dataType: “jsonp”。除了这个，其他配置和普通的请求是一样的。 后端 SpringMVC 配置如果你也使用 SpringMVC，那么配置一个 jsonp 的 Advice 就可以了，这样我们写的每一个 Controller 方法就完全不需要考虑客户端到底是不是 jsonp 请求了，Spring 会自动做相应的处理。1234567@ControllerAdvicepublic class JsonpAdvice extends AbstractJsonpResponseBodyAdvice &#123; public JsonpAdvice()&#123; // 这样如果请求中带 callback 参数，Spring 就知道这个是 jsonp 的请求了 super("callback"); &#125;&#125; 以上写法要求 SpringMVC 版本不低于 3.2，低于 3.2 的我只能说，你们该升级了。 后端非 SpringMVC 配置以前刚工作的时候，Struts2 还红遍天，几年的光景，SpringMVC 就基本统治下来了国内市场。 偷懒一下，这里贴个伪代码吧，在我们的方法返回前端之前调一下 wrap 方法：12345678public Object wrap(HttpServletRequest request)&#123; String callback = request.getParameter("callback"); if(StringUtils.isBlank(callback))&#123; return result; &#125; else &#123; return callback+"("+JSON.toJSONString(result)+")"; &#125;&#125; CORSCross-Origin Resource Sharing 毕竟 jsonp 只支持 get 请求，肯定不能满足我们的所有的请求需要，所以才需要搬出 CORS。 国内的 web 开发者还是比较苦逼的，用户死不升级浏览器，老板还死要开发者做兼容。 CORS 支持以下浏览器，目前来看，浏览器的问题已经越来越不重要了，连淘宝都不支持 IE7 了~~~ Chrome 3+ Firefox 3.5+ Opera 12+ Safari 4+ Internet Explorer 8+ 前端 jQuery 写法直接看代码吧：12345678910111213141516$.ajax(&#123; type: "POST", url: baseUrl + "/jsonp/post", dataType: 'json', crossDomain: true, xhrFields: &#123; withCredentials: true &#125;, data: &#123; name: "name_from_frontend" &#125;, success: function (response) &#123; console.log(response)// 返回的 json 数据 $("#response").val(JSON.stringify(response)); &#125;&#125;); dataType: “json”，这里是 json，不是 jsonp，不是 jsonp，不是 jsonp。 crossDomain: true，这里代表使用跨域请求 xhrFields: {withCredentials: true}，这样配置就可以把 cookie 带过去了，不然我们连 session 都没法维护，很多人都栽在这里。当然，如果你没有这个需求，也就不需要配置这个了。 后端 SpringMVC 配置对于大部分的 web 项目，一般都会有 mvc 相关的配置类，此类继承自 WebMvcConfigurerAdapter。如果你也使用 SpringMVC 4.2 以上的版本的话，直接像下面这样添加这个方法就可以了：12345678@Configurationpublic class WebConfig extends WebMvcConfigurerAdapter &#123; @Override public void addCorsMappings(CorsRegistry registry) &#123; registry.addMapping("/**/*").allowedOrigins("*"); &#125;&#125; 如果很不幸你的项目中 SpringMVC 版本低于 4.2，那么需要「曲线救国」一下：12345678910public class CrossDomainFilter extends OncePerRequestFilter &#123; @Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; response.addHeader("Access-Control-Allow-Origin", "*");// 如果提示 * 不行，请往下看 response.addHeader("Access-Control-Allow-Credentials", "true"); response.addHeader("Access-Control-Allow-Methods", "GET, POST, PUT, DELETE"); response.addHeader("Access-Control-Allow-Headers", "Content-Type"); filterChain.doFilter(request, response); &#125;&#125; 在 web.xml 中配置下 filter：12345678&lt;filter&gt; &lt;filter-name&gt;CrossDomainFilter&lt;/filter-name&gt; &lt;filter-class&gt;com.javadoop.filters.CrossDomainFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;CrossDomainFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 有很多项目用 shiro 的，也可以通过配置 shiro 过滤器的方式，这里就不介绍了。 注意了，我说的是很笼统的配置，对于大部分项目是可以这么笼统地配置的。文中类似 “*” 这种配置读者应该都能知道怎么配。 如果读者发现浏览器提示不能用 ‘*’ 符号，那读者可以在上面的 filter 中根据 request 对象拿到请求头中的 referer（request.getHeader(“referer”)），然后动态地设置 “Access-Control-Allow-Origin”： 12345678String referer = request.getHeader("referer");if (StringUtils.isNotBlank(referer)) &#123; URL url = new URL(referer); String origin = url.getProtocol() + "://" + url.getHost(); response.addHeader("Access-Control-Allow-Origin", origin);&#125; else &#123; response.addHeader("Access-Control-Allow-Origin", "*");&#125; 前端非 jQuery 写法jQuery 一招鲜吃遍天的日子是彻底不在了，这里就说说如果不使用 jQuery 的话，怎么解决 post 跨域的问题。大部分的 js 库都会提供相应的方案的，大家直接找相应的文档看看就知道怎么用了。 来一段原生 js 介绍下：1234567891011121314151617181920function createCORSRequest(method, url) &#123; var xhr = new XMLHttpRequest(); if ("withCredentials" in xhr) &#123; // 如果有 withCredentials 这个属性，那么可以肯定是 XMLHTTPRequest2 对象。看第三个参数 xhr.open(method, url, true); &#125; else if (typeof XDomainRequest != "undefined") &#123; // 此对象是 IE 用来跨域请求的 xhr = new XDomainRequest(); xhr.open(method, url); &#125; else &#123; // 如果是这样，很不幸，浏览器不支持 CORS xhr = null; &#125; return xhr;&#125;var xhr = createCORSRequest('GET', url);if (!xhr) &#123; throw new Error('CORS not supported');&#125; 其中，Chrome，Firefox，Opera，Safari 这些「程序员友好」的浏览器使用的是 XMLHTTPRequest2 对象。IE 使用的是 XDomainRequest。 我想，对于 95% 的读者来说，说到这里就够了，我就不往下说了，读者如果有需要补充的，请在评论区留言。]]></content>
      <categories>
        <category>HTTP</category>
      </categories>
      <tags>
        <tag>跨域请求</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单KEY业务，数据库水平切分架构实践]]></title>
    <url>%2F2017%2F11%2F02%2F2017-06-16-db-middleware1%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 本文将以“用户中心”为例，介绍“单KEY”类业务，随着数据量的逐步增大，数据库性能显著降低，数据库水平切分相关的架构实践： 如何来实施水平切分 水平切分后常见的问题 典型问题的优化思路及实践 一、用户中心用户中心是一个非常常见的业务，主要提供用户注册、登录、信息查询与修改的服务，其核心元数据为：User(uid, login_name, passwd, sex, age, nickname, …)其中： uid为用户ID，主键 login_name, passwd, sex, age, nickname, …等用户属性数据库设计上，一般来说在业务初期，单库单表就能够搞定这个需求，典型的架构设计为： user-center：用户中心服务，对调用者提供友好的RPC接口 user-db：对用户进行数据存储 二、用户中心水平切分方法当数据量越来越大时，需要对数据库进行水平切分，常见的水平切分算法有“范围法”和“哈希法”。 范围法，以用户中心的业务主键uid为划分依据，将数据水平切分到两个数据库实例上去： user-db1：存储0到1千万的uid数据 user-db2：存储1到2千万的uid数据 范围法的优点是： 切分策略简单，根据uid，按照范围，user- center很快能够定位到数据在哪个库上 扩容简单，如果容量不够，只要增加user-db3即可 范围法的不足是： uid必须要满足递增的特性 数据量不均，新增的user-db3，在初期的数据会比较少 请求量不均，一般来说，新注册的用户活跃度会比较高，故user-db2往往会比user-db1负载要高，导致服务器利用率不平衡 哈希法，也是以用户中心的业务主键uid为划分依据，将数据水平切分到两个数据库实例上去： user-db1：存储uid取模得1的uid数据 user-db2：存储uid取模得0的uid数据 哈希法的优点是： 切分策略简单，根据uid，按照hash，user-center很快能够定位到数据在哪个库上 数据量均衡，只要uid是均匀的，数据在各个库上的分布一定是均衡的 请求量均衡，只要uid是均匀的，负载在各个库上的分布一定是均衡的 哈希法的不足是： 扩容麻烦，如果容量不够，要增加一个库，重新hash可能会导致数据迁移，如何平滑的进行数据迁移，是一个需要解决的问题 三、用户中心水平切分后带来的问题使用uid来进行水平切分之后，整个用户中心的业务访问会遇到什么问题呢？ 对于uid属性上的查询可以直接路由到库，假设访问uid=124的数据，取模后能够直接定位db-user1： 对于非uid属性上的查询，例如login_name属性上的查询，就悲剧了： 假设访问login_name=shenjian的数据，由于不知道数据落在哪个库上，往往需要遍历所有库，当分库数量多起来，性能会显著降低。 四、用户中心非uid属性查询需求分析任何脱离业务的架构设计都是耍流氓，在进行架构讨论之前，先来对业务进行简要分析，看非uid属性上有哪些查询需求。 根据楼主这些年的架构经验，用户中心非uid属性上经常有两类业务需求：（1）用户侧，前台访问，最典型的有两类需求用户登录：通过login_name/phone/email查询用户的实体，1%请求属于这种类型用户信息查询：登录之后，通过uid来查询用户的实例，99%请求属这种类型 用户侧的查询基本上是单条记录的查询，访问量较大，服务需要高可用，并且对一致性的要求较高。 （2）运营侧，后台访问，根据产品、运营需求，访问模式各异，按照年龄、性别、头像、登陆时间、注册时间来进行查询。 运营侧的查询基本上是批量分页的查询，由于是内部系统，访问量很低，对可用性的要求不高，对一致性的要求也没这么严格。 这两类不同的业务需求，应该使用什么样的架构方案来解决呢？ 五、用户中心水平切分架构思路用户中心在数据量较大的情况下，使用uid进行水平切分，对于非uid属性上的查询需求，架构设计的核心思路为： 针对用户侧，应该采用“建立非uid属性到uid的映射关系”的架构方案 针对运营侧，应该采用“前台与后台分离”的架构方案 六、用户中心-用户侧最佳实践【索引表法】思路：uid能直接定位到库，login_name不能直接定位到库，如果通过login_name能查询到uid，问题解决 解决方案： 建立一个索引表记录login_name-&gt;uid的映射关系 用login_name来访问时，先通过索引表查询到uid，再定位相应的库 索引表属性较少，可以容纳非常多数据，一般不需要分库 如果数据量过大，可以通过login_name来分库 潜在不足：多一次数据库查询，性能下降一倍 【缓存映射法】思路：访问索引表性能较低，把映射关系放在缓存里性能更佳 解决方案： login_name查询先到cache中查询uid，再根据uid定位数据库 假设cache miss，采用扫全库法获取login_name对应的uid，放入cache login_name到uid的映射关系不会变化，映射关系一旦放入缓存，不会更改，无需淘汰，缓存命中率超高 如果数据量过大，可以通过login_name进行cache水平切分 潜在不足：多一次cache查询 【login_name生成uid】思路：不进行远程查询，由login_name直接得到uid 解决方案： 在用户注册时，设计函数login_name生成uid，uid=f(login_name)，按uid分库插入数据 用login_name来访问时，先通过函数计算出uid，即uid=f(login_name)再来一遍，由uid路由到对应库 潜在不足：该函数设计需要非常讲究技巧，有uid生成冲突风险 【login_name基因融入uid】思路：不能用login_name生成uid，可以从login_name抽取“基因”，融入uid中. 假设分8库，采用uid%8路由，潜台词是，uid的最后3个bit决定这条数据落在哪个库上，这3个bit就是所谓的“基因”。 解决方案： 在用户注册时，设计函数login_name生成3bit基因，login_name_gene=f(login_name)，如上图粉色部分同时，生成61bit的全局唯一id，作为用户的标识，如上图绿色部分 接着把3bit的login_name_gene也作为uid的一部分，如上图屎黄色部分 生成64bit的uid，由id和login_name_gene拼装而成，并按照uid分库插入数据 用login_name来访问时，先通过函数由login_name再次复原3bit基因，login_name_gene=f(login_name)，通过login_name_gene%8直接定位到库 七、用户中心-运营侧最佳实践前台用户侧，业务需求基本都是单行记录的访问，只要建立非uid属性 login_name / phone / email 到uid的映射关系，就能解决问题。 后台运营侧，业务需求各异，基本是批量分页的访问，这类访问计算量较大，返回数据量较大，比较消耗数据库性能。 如果此时前台业务和后台业务公用一批服务和一个数据库，有可能导致，由于后台的“少数几个请求”的“批量查询”的“低效”访问，导致数据库的cpu偶尔瞬时100%，影响前台正常用户的访问（例如，登录超时）。 而且，为了满足后台业务各类“奇形怪状”的需求，往往会在数据库上建立各种索引，这些索引占用大量内存，会使得用户侧前台业务uid/login_name上的查询性能与写入性能大幅度降低，处理时间增长。 对于这一类业务，应该采用“前台与后台分离”的架构方案： 用户侧前台业务需求架构依然不变，产品运营侧后台业务需求则抽取独立的web / service / db 来支持，解除系统之间的耦合，对于“业务复杂”“并发量低”“无需高可用”“能接受一定延时”的后台业务： 可以去掉service层，在运营后台web层通过dao直接访问db 不需要反向代理，不需要集群冗余 不需要访问实时库，可以通过MQ或者线下异步同步数据 在数据库非常大的情况下，可以使用更契合大量数据允许接受更高延时的“索引外置”或者“HIVE”的设计方案 八、总结将以“用户中心”为典型的“单KEY”类业务，水平切分的架构点，本文做了这样一些介绍。 水平切分方式： 范围法 哈希法 水平切分后碰到的问题： 通过uid属性查询能直接定位到库，通过非uid属性查询不能定位到库 非uid属性查询的典型业务： 用户侧，前台访问，单条记录的查询，访问量较大，服务需要高可用，并且对一致性的要求较高 运营侧，后台访问，根据产品、运营需求，访问模式各异，基本上是批量分页的查询，由于是内部系统，访问量很低，对可用性的要求不高，对一致性的要求也没这么严格 这两类业务的架构设计思路： 针对用户侧，应该采用“建立非uid属性到uid的映射关系”的架构方案 针对运营侧，应该采用“前台与后台分离”的架构方案 用户前台侧，“建立非uid属性到uid的映射关系”最佳实践： 索引表法：数据库中记录login_name-&gt;uid的映射关系 缓存映射法：缓存中记录login_name-&gt;uid的映射关系 login_name生成uid login_name基因融入uid 运营后台侧，“前台与后台分离”最佳实践： 前台、后台系统web/service/db分离解耦，避免后台低效查询引发前台查询抖动 可以采用数据冗余的设计方式 可以采用“外置索引”（例如ES搜索系统）或者“大数据处理”（例如HIVE）来满足后台变态的查询需求]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构师之路</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[究竟为什么要引入数据库中间件?]]></title>
    <url>%2F2017%2F11%2F02%2F2017-11-02-db-middleware%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 不少朋友经常会问我以下问题： 58到家有没有使用数据库中间件 使用了什么数据库中间件，是自研，还是第三方 怎么实现的，是基于客户端的中间件，还是基于服务端的中间件 使用中间件后，join/子查询/集函数/事务等问题是怎么解决的 …你是不是也有类似的疑问？ 然而，“究竟为什么要引入数据库中间件”却很少有人问及。 “架构师之路”文章思路，以解决“为什么”为优先，借着近期撰写互联网分层架构系列文章，讲一讲这个核心问题： 究竟为什么要引入数据库中间件 经过连续分层架构演进，DAO层，基础数据服务化，通用业务服务化，前后端分离之后，一个业务系统的后端结构如上： web-view层通过http接口，从web-data获取json数据（前后端分离） web-data层通过RPC接口，从biz-service获取数据（通用业务服务） biz-service层通过RPC接口，从base-service获取数据（基础数据服务） base-service层通过DAO，从db获取数据（DAO） db存储数据 随着时间的推移，数据量会越来越大，base-service通过DAO来访问db的性能会越来越低，需要开始考虑对db进行水平切分，一旦db进行水平切分，原来很多SQL可以支持的功能，就需要base-service层来进行特殊处理： 有些数据需要路由到特定的水平切分库 有些数据不确定落在哪一个水平切分库，就需要访问所有库 有些数据需要访问全局的库，拿到数据的全局视野，到service层进行额外处理 … 更具体的，对于前台高并发的业务，db水平切分后，有这么几类典型的业务场景及应对方案。特别强调一下，此处应对的是“前台”“高并发”“db水平切分”的场景，对于后台的需求，将通过前台与后台分离的架构处理，不在此处讨论。 一：partition key上的单行查询典型场景：通过uid查询user 场景特点： 通过patition key查询 每次只返回一行记录 解决方案：base-service层通过patition key来进行库路由 如上图： user-service底层user库，分库patition key是uid uid上的查询，user-service可以直接定位到库 二、非patition key上的单行查询典型场景：通过login_name查询user 场景特点： 通过非patition key查询 每次只返回一行记录 解决方案1：base-service层访问所有库 如上图： user-service通过login_name先查全库 结果集在user-service再合并，最终返回一条记录 解决方案2：base-service先查mapping库，再通过patition key路由 如上图： 新建mapping库，记录login_name到uid的映射关系 当有非 patition key的查询时，先通过login_name查询uid 再通过patition key进行路由，最终返回一条记录 解决方案3：基因法关于“基因法”解决非patition key上的查询需求详见《分库后，非patition key上访问的多种解决办法》。 三、patition key上的批量查询典型场景：用户列表uid上的IN查询 场景特点： 通过patition key查询 每次返回多行记录 解决方案1：base-service层访问所有库，结果集到base-service合并 解决方案2：base-service分析路由规则，按需访问如上图： base-service根据路由规则分析，判断出有些数据落在库1，有些数据落在库2 base-service按需访问相关库，而不是访问全库 base-service合并结果集，返回列表数据 四、非patition key上的夸库分页需求关于分库后，夸库分页的查询需求，详见《业界难题，夸库分页的四种方案》。 五、其他需求…本文写到这里，上述一、二、三、四、五其实都不是重点，base-service层通过各种各样的奇技淫巧，能够解决db水平切分后的数据访问问题，只不过： base-service层的复杂度提高了 数据的获取效率降低了 当需要进行db水平切分的base-service越来越多以后，此时分层架构会变成下面这个样子： 底层的复杂性会扩散到各个base-service，所有的base-service都要关注： patition key路由 非patition key查询，先mapping，再路由 先全库，再合并 先分析，再按需路由 夸库分页处理 … 这个架构图是不是看上去很别扭？如何让数据的获取更加高效快捷呢？数据库中间件的引入，势在必行。 这是“基于服务端”的数据库中间件架构图： base-service层，就像访问db一样，访问db-proxy，高效获取数据 所有底层的复杂性，都屏蔽在db-proxy这一层 这是“基于客户端”的数据库中间件架构图： base-service层，通过db-proxy.jar，高效获取数据 所有底层的复杂性，都屏蔽在db-proxy.jar这一层 结论：当数据库水平切分，base-service层获取db数据过于复杂，成为通用痛点的时候，就应该抽象出数据库中间件，简化数据获取过程，提高数据获取效率，向上游屏蔽底层的复杂性。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构师之路</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分层架构，是否需要业务服务层]]></title>
    <url>%2F2017%2F10%2F18%2F2017-10-18-architecture%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 《互联网分层架构的本质》简述了两个观点： 互联网分层架构的本质，是数据的移动 互联网分层架构演进的核心原则：是让上游更高效的获取与处理数据，让下游能屏蔽数据的获取细节 《分层架构：什么时候抽象DAO层，什么时候抽象数据服务层》中的观点是： 当手写代码从DB中获取数据，成为通用痛点的时候，就应该抽象出DAO层，简化数据获取过程，提高数据获取效率，向上游屏蔽底层的复杂性 当业务越来越复杂，垂直拆分的系统越来越多，数据库实施了水平切分，数据层实施了缓存加速之后，底层数据获取复杂性成为通用痛点的时候，就应该抽象出数据服务层，简化数据获取过程，提高数据获取效率，向上游屏蔽底层的复杂性 文本将要解答的问题是： 基础数据的访问需要服务化，业务层是否需要服务化 如果需要服务化，什么时候服务化 基础数据的访问服务化之后，一个业务系统的后端架构如上： web-server通过RPC接口，从基础数据service获取数据 基础数据service通过DAO，从db/cache获取数据 db/cache存储数据 随着时间的推移，系统架构并不会一成不变： 随着业务越来越复杂，业务会不断进行垂直拆分 随着数据越来越复杂，基础数据service也会越来越多 于是系统架构变成了上图这个样子，业务垂直拆分，有若干个基础数据服务： 垂直业务要通过多个RPC接口访问不同的基础数据service，service共有是服务化的特征 每个基础数据service访问自己的数据存储，数据私有也是服务化的特征 这个架构图中的依赖关系是不是看上去很别扭？ 基础数据service与存储层之前连接关系很清晰 业务web-server层与基础数据service层之间的连接关系错综复杂，变成了蜘蛛网 再举一个更具体的例子，58同城 列表页 web-server如何获取底层的数据？ 首先调用商业基础service，获取商业广告帖子数据，用于顶部置顶/精准的广告帖子展示 再调用搜索基础service，获取自然搜索帖子数据，用于中部自然搜索帖子展示 再调用推荐基础service，获取推荐帖子数据，用于底部推荐帖子展示 再调用用户基础service，获取用户数据，用于右侧用户信息展示 … 如果只有一个列表页这么写还行，但如果有招聘、房产、二手、二手车、黄页…等多个大部分是共性数据，少部分是个性数据的列表页，每次都这么获取数据，就略显低效了，有大量冗余、重复、每次必写的代码。 特别的，不同业务上游列表页都依赖于底层若干相同服务： 一旦一个服务RPC接口有稍许变化，所有上游的系统都需要升级修改 子系统之间很可能出现代码拷贝 一旦拷贝代码，出现一个bug，多个子系统都需要升级修改 如何让数据的获取更加高效快捷呢？ 业务服务化，通用业务服务层的抽象势在必行。 通过抽象通用业务服务层，例如58同城“通用列表服务”： web-server层，可以通过RPC接口，像调用本地函数一样，调用通用业务service，一次性获取所有通用数据 通用业务service，也可以通过多次调用基础数据service提供的RPC接口，分别获取数据，底层数据获取的复杂性，全都屏蔽在了此处 是不是连接关系也看起来更清晰？ 这样的好处是： 复杂的从基础服务获取数据的代码，只有在通用业务service处写了一次，没有代码拷贝 底层基础数据service接口发生变化，只有通用业务service一处需要升级修改 如果有bug，不管是底层基础数据service的bug，还是通用业务service的bug，都只有一处需要升级修改 业务web-server获取数据更便捷，获取所有数据，只需一个RPC接口调用 结论： 当业务越来越复杂，垂直拆分的系统越来越多，基础数据服务越来越多，底层数据获取复杂性成为通用痛点的时候，就应该抽象出通用业务服务，简化数据获取过程，提高数据获取效率，向上游屏蔽底层的复杂性。 最后再强调两点： 是否需要抽象通用业务服务，和业务复杂性，以及业务发展阶段有关，不可一概而论 需要抽象什么通用业务服务，和具体业务相关 任何脱离业务的架构设计，都是耍流氓。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>服务化</tag>
        <tag>架构师之路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[互联网分层架构的本质]]></title>
    <url>%2F2017%2F10%2F11%2F2017-10-11-architecture%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 上图是一个典型的互联网分层架构： 客户端层：典型调用方是browser或者APP 站点应用层：实现核心业务逻辑，从下游获取数据，对上游返回html或者json 数据-缓存层：加速访问存储 数据-数据库层：固化数据存储 如果实施了服务化，这个分层架构图可能是这样： 中间多了一个服务层。 同一个层次的内部，例如端上的APP，以及web-server，也都有进行MVC分层： view层：展现 control层：逻辑 model层：数据 可以看到，每个工程师骨子里，都潜移默化的实施着分层架构。 那么，互联网分层架构的本质究竟是什么呢？ 如果我们仔细思考会发现，不管是跨进程的分层架构，还是进程内的MVC分层，都是一个“数据移动”，然后“被处理”和“被呈现”的过程，归根结底一句话：互联网分层架构，是一个数据移动，处理，呈现的过程，其中数据移动是整个过程的核心。 如上图所示：数据处理和呈现要CPU计算，CPU是固定不动的： db/service/web-server都部署在固定的集群上 端上，不管是browser还是APP，也有固定的CPU处理 数据是移动的： 跨进程移动：数据从数据库和缓存里，转移到service层，到web-server层，到client层 同进程移动：数据从model层，转移到control层，转移到view层 数据要移动，所以有两个东西很重要： 数据传输的格式 数据在各层次的形态 先看数据传输的格式，即协议很重要： service与db/cache之间，二进制协议/文本协议是数据传输的载体 web-server与service之间，RPC的二进制协议是数据传输的载体 client和web-server之间，http协议是数据传输的载体 再看数据在各层次的形态，以用户数据为例： db层，数据是以“行”为单位存在的row(uid, name, age) cache层，数据是以kv的形式存在的kv(uid -&gt; User) service层，会把row或者kv转化为对程序友好的User对象 web-server层，会把对程序友好的User对象转化为对http友好的json对象 client层：最终端上拿到的是json对象 结论：互联网分层架构的本质，是数据的移动。 为什么要说这个，这将会引出“分层架构演进”的核心原则与方法： 让上游更高效的获取与处理数据，复用 让下游能屏蔽数据的获取细节，封装 弄清楚这个原则与方法，再加上一些经验积累，就能回答网友经常在评论中提出的这些问题了： 是否需要引入DAO层，什么时机引入 是否需要服务化，什么时机服务化 是否需要抽取通用中台业务，什么时机抽取 是否需要前后端分离，什么时机分离 （网友们的这些提问，其实很难回答。在不了解业务发展阶段，业务规模，数据量并发量的情况下，妄下YES或NO的结论，本身就是不负责任的。）更具体的分层架构演进细节，下一篇和大家细究。 总结 互联网分层架构的本质，是数据的移动 互联网分层架构中，数据的传输格式（协议）与数据在各层次的形态很重要 互联网分层架构演进的核心原则与方法：封装与复用 思考哪一个系统的架构，不是“固定CPU，移动数据”，而是“固定数据，移动CPU”呢？]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构师之路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[互联网分层架构之-DAO与服务化]]></title>
    <url>%2F2017%2F10%2F11%2F2017-10-13-dao-and-service%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 互联网分层架构的本质，是数据的移动。 互联网分层架构演进的核心原则： 让上游更高效的获取与处理数据，复用 让下游能屏蔽数据的获取细节，封装 这些在上一篇《互联网分层架构的本质》中有详尽的描述，在实际系统架构演进过程中，如何利用这两个原则，对系统逐步进行分层抽象呢？咱们先从后端系统开始讲解。 本文主要解答两个问题： 后端架构，什么时候进行DAO层的抽象 后端架构，什么时候进行数据服务层的抽象 核心问题一：什么时候进行DAO层的抽象 一个业务系统最初的后端结构如上： web-server层从db层获取数据并进行加工处理 db层存储数据 此时，web-server层如何获取底层的数据呢？ web-server层获取数据的一段伪代码如上，不用纠结代码的细节，也不用纠结不同编程语言与不同数据库驱动的差异，其获取数据的过程大致为： 创建一个与数据库的连接，初始化资源 根据业务拼装一个SQL语句 通过连接执行SQL语句，并获得结果集 通过游标遍历结果集，取出每行数据，亦可从每行数据中取出属性数据 关闭数据库连接，回收资源 如果业务不复杂，这段代码写1次2次还可以，但如果业务越来越复杂，每次都这么获取数据，就略显低效了，有大量冗余、重复、每次必写的代码。 如何让数据的获取更加高效快捷呢？ 通过技术手段实现： 表与类的映射 属性与成员的映射 SQL与函数的映射 绝大部分公司正在用的ORM，DAO等技术，就是一种分层抽象，可以提高数据获取的效率，屏蔽连接，游标，结果集这些复杂性。 结论当手写代码从DB中获取数据，成为通用痛点的时候，就应该抽象出DAO层，简化数据获取过程，提高数据获取效率，向上游屏蔽底层的复杂性。 核心问题二：什么时候要进行数据服务层的抽象抽象出DAO层之后，系统架构并不会一成不变： 随着业务越来越复杂，业务系统会不断进行垂直拆分 随着数据量越来越大，数据库会进行水平切分 随着读并发的越来越大，会增加缓存降低数据库的压力 于是系统架构变成了这个样子： 业务系统垂直拆分，数据库水平切分，缓存这些都是常见的架构优化手段。 此时，web-server层如何获取底层的数据呢？根据楼主的经验，以用户数据为例，流程一般是这样的： 先查缓存：先用uid尝试从缓存获取数据，如果cache hit，数据获取成功，返回User实体，流程结束 确定路由：如果cache miss，先查询路由配置，确定uid落在哪个数据库实例的哪个库上 查询DB：通过DAO从对应库获取uid对应的数据实体User 插入缓存：将kv(uid, User)放入缓存，以便下次缓存查询数据能够命中缓存 如果业务不复杂，这段代码写1次2次还可以，但如果业务越来越复杂，每次都这么获取数据，就略显低效了，有大量冗余、重复、每次必写的代码。 特别的，业务垂直拆分成非常多的子系统之后： 一旦底层有稍许变化，所有上游的系统都需要升级修改 子系统之间很可能出现代码拷贝 一旦拷贝代码，出现一个bug，多个子系统都需要升级修改 不相信业务会垂直拆分成多个子系统？举两个例子： 58同城有招聘、房产、二手、二手车、黄页等5大头部业务，都需要访问用户数据 58到家有月嫂、保姆、丽人、速运、平台等多个业务，也都需要访问用户数据如果每个子系统都需要关注缓存，分库，读写分离的复杂性，调用层会疯掉的。 如何让数据的获取更加高效快捷呢？ 服务化，数据服务层的抽象势在必行。 通过抽象数据服务层： web-server层可以通过RPC接口，像调用本地函数一样调用远端的数据 数据服务层，只有这一处需要关注缓存，分库，读写分离这些复杂性服务化这里就不展开，更详细的可参考《互联网架构为什么要做服务化？》。 结论当业务越来越复杂，垂直拆分的系统越来越多，数据库实施了水平切分，数据层实施了缓存加速之后，底层数据获取复杂性成为通用痛点的时候，就应该抽象出数据服务层，简化数据获取过程，提高数据获取效率，向上游屏蔽底层的复杂性。 互联网分层架构是一个很有意思的问题，服务化的引入，并不是越早越好： 请求处理时间可能会增加 运维可能会更加复杂 定位问题可能会更加麻烦 千万别鲁莽的在“微服务”大流之下，草率的进行微服务改造，看似“高大上架构”的背后，隐藏着更多并未接触过的“大坑”。还是那句话，架构和业务的特点和阶段有关：一切脱离业务的架构设计，都是耍流氓。 这一篇先到这里，分层架构，还有很多内容要和大家聊： 后端架构，是否需要抽取中台业务，什么时机抽取 后端架构，是否需要前后端分离，什么时机分离 前端架构，如何进行分层实践 末了，再次强调下，互联网分层架构的本质，是数据的移动。 互联网分层架构演进的核心原则，是让上游更高效的获取与处理数据，让下游能屏蔽掉数据的复杂性获取细节。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>服务化</tag>
        <tag>架构师之路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度解读 java 线程池设计思想及源码实现]]></title>
    <url>%2F2017%2F09%2F05%2F2017-09-05-java-thread-pool%2F</url>
    <content type="text"><![CDATA[原文链接: https://javadoop.com/post/java-thread-pool 本文一大重点是源码解析，不过线程池设计思想以及作者实现过程中的一些巧妙用法是我想传达给读者的。本文还是会一行行关键代码进行分析，目的是为了让那些自己看源码不是很理解的同学可以得到参考。 线程池是非常重要的工具，如果你要成为一个好的工程师，还是得比较好地掌握这个知识。即使你为了谋生，也要知道，这基本上是面试必问的题目，而且面试官很容易从被面试者的回答中捕捉到被面试者的技术水平。 本文略长，建议在 pc 上阅读，边看文章边翻源码（Java7 和 Java8 都一样），建议想好好看的读者抽出至少 15 至 30 分钟的整块时间来阅读。当然，如果读者仅为面试准备，可以直接滑到最后的总结部分。 总览下图是 java 线程池几个相关类的继承结构： 先简单说说这个继承结构，Executor 位于最顶层，也是最简单的，就一个execute(Runnable runnable)接口方法定义。 ExecutorService也是接口，在Executor 接口的基础上添加了很多的接口方法，所以一般来说我们会使用这个接口。 然后再下来一层是AbstractExecutorService，从名字我们就知道，这是抽象类，这里实现了非常有用的一些方法供子类直接使用，之后我们再细说。 然后才到我们的重点部分ThreadPoolExecutor类，这个类提供了关于线程池所需的非常丰富的功能。 另外，我们还涉及到下图中的这些类： 同在并发包中的Executors类，类名中带字母 s，我们猜到这个是工具类，里面的方法都是静态方法，如以下我们最常用的用于生成ThreadPoolExecutor的实例的一些方法：12345678910public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125;public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 另外，由于线程池支持获取线程执行的结果，所以，引入了 Future 接口，RunnableFuture 继承自此接口，然后我们最需要关心的就是它的实现类FutureTask。到这里，记住这个概念，在线程池的使用过程中，我们是往线程池提交任务（task），使用过线程池的都知道，我们提交的每个任务是实现了 Runnable 接口的，其实就是先将 Runnable 的任务包装成 FutureTask，然后再提交到线程池。这样，读者才能比较容易记住 FutureTask 这个类名：它首先是一个任务（Task），然后具有 Future 接口的语义，即可以在将来（Future）得到执行的结果。 当然，线程池中的BlockingQueue也是非常重要的概念，如果线程数达到corePoolSize，我们的每个任务会提交到等待队列中，等待线程池中的线程来取任务并执行。这里的 BlockingQueue 通常我们使用其实现类 LinkedBlockingQueue、ArrayBlockingQueue 和 SynchronousQueue，每个实现类都有不同的特征，使用场景之后会慢慢分析。想要详细了解各个 BlockingQueue 的读者，可以参考我的前面的一篇对 BlockingQueue 的各个实现类进行详细分析的文章。 把事情说完整：除了上面说的这些类外，还有一个很重要的类，就是定时任务实现类 ScheduledThreadPoolExecutor，它继承自本文要重点讲解的 ThreadPoolExecutor，用于实现定时执行。不过本文不会介绍它的实现，我相信读者看完本文后可以比较容易地看懂它的源码。 以上就是本文要介绍的知识，废话不多说，开始进入正文。 Executor 接口1234567/* * @since 1.5 * @author Doug Lea */public interface Executor &#123; void execute(Runnable command);&#125; 我们可以看到 Executor 接口非常简单，就一个void execute(Runnable command)方法，代表提交一个任务。为了让大家理解 java 线程池的整个设计方案，我会按照 Doug Lea 的设计思路来多说一些相关的东西。 我们经常这样启动一个线程：123new Thread(new Runnable()&#123; // do something&#125;).start(); 用了线程池 Executor 后就可以像下面这么使用：123Executor executor = anExecutor;executor.execute(new RunnableTask1());executor.execute(new RunnableTask2()); 如果我们希望线程池同步执行每一个任务，我们可以这么实现这个接口：12345class DirectExecutor implements Executor &#123; public void execute(Runnable r) &#123; r.run();// 这里不是用的new Thread(r).start()，也就是说没有启动任何一个新的线程。 &#125;&#125; 我们希望每个任务提交进来后，直接启动一个新的线程来执行这个任务，我们可以这么实现：12345class ThreadPerTaskExecutor implements Executor &#123; public void execute(Runnable r) &#123; new Thread(r).start(); // 每个任务都用一个新的线程来执行 &#125;&#125; 我们再来看下怎么组合两个 Executor 来使用，下面这个实现是将所有的任务都加到一个 queue 中，然后从 queue 中取任务，交给真正的执行器执行，这里采用 synchronized 进行并发控制：123456789101112131415161718192021222324252627282930313233343536class SerialExecutor implements Executor &#123; // 任务队列 final Queue&lt;Runnable&gt; tasks = new ArrayDeque&lt;Runnable&gt;(); // 这个才是真正的执行器 final Executor executor; // 当前正在执行的任务 Runnable active; // 初始化的时候，指定执行器 SerialExecutor(Executor executor) &#123; this.executor = executor; &#125; // 添加任务到线程池: 将任务添加到任务队列，scheduleNext 触发执行器去任务队列取任务 public synchronized void execute(final Runnable r) &#123; tasks.offer(new Runnable() &#123; public void run() &#123; try &#123; r.run(); &#125; finally &#123; scheduleNext(); &#125; &#125; &#125;); if (active == null) &#123; scheduleNext(); &#125; &#125; protected synchronized void scheduleNext() &#123; if ((active = tasks.poll()) != null) &#123; // 具体的执行转给真正的执行器 executor executor.execute(active); &#125; &#125;&#125; 当然了，Executor 这个接口只有提交任务的功能，太简单了，我们想要更丰富的功能，比如我们想知道执行结果、我们想知道当前线程池有多少个线程活着、已经完成了多少任务等等，这些都是这个接口的不足的地方。接下来我们要介绍的是继承自 Executor 接口的ExecutorService接口，这个接口提供了比较丰富的功能，也是我们最常使用到的接口。 ExecutorService一般我们定义一个线程池的时候，往往都是使用这个接口：12ExecutorService executor = Executors.newFixedThreadPool(args...);ExecutorService executor = Executors.newCachedThreadPool(args...); 因为这个接口中定义的一系列方法大部分情况下已经可以满足我们的需要了。 那么我们简单初略地来看一下这个接口中都有哪些方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public interface ExecutorService extends Executor &#123; // 关闭线程池，已提交的任务继续执行，不接受继续提交新任务 void shutdown(); // 关闭线程池，尝试停止正在执行的所有任务，不接受继续提交新任务 // 它和前面的方法相比，加了一个单词“now”，区别在于它会去停止当前正在进行的任务 List&lt;Runnable&gt; shutdownNow(); // 线程池是否已关闭 boolean isShutdown(); // 如果调用了 shutdown() 或 shutdownNow() 方法后，所有任务结束了，那么返回true // 这个方法必须在调用shutdown或shutdownNow方法之后调用才会返回true boolean isTerminated(); // 等待所有任务完成，并设置超时时间 // 我们这么理解，实际应用中是，先调用 shutdown 或 shutdownNow， // 然后再调这个方法等待所有的线程真正地完成，返回值意味着有没有超时 boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; // 提交一个 Callable 任务 &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); // 提交一个 Runnable 任务，第二个参数将会放到 Future 中，作为返回值， // 因为 Runnable 的 run 方法本身并不返回任何东西 &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); // 提交一个 Runnable 任务 Future&lt;?&gt; submit(Runnable task); // 执行所有任务，返回 Future 类型的一个 list &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException; // 也是执行所有任务，但是这里设置了超时时间 &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException; // 只有其中的一个任务结束了，就可以返回，返回执行完的那个任务的结果 &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException; // 同上一个方法，只有其中的一个任务结束了，就可以返回，返回执行完的那个任务的结果， // 不过这个带超时，超过指定的时间，抛出 TimeoutException 异常 &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 这些方法都很好理解，一个简单的线程池主要就是这些功能，能提交任务，能获取结果，能关闭线程池，这也是为什么我们经常用这个接口的原因。 FutureTask在继续往下层介绍 ExecutorService 的实现类之前，我们先来说说相关的类 FutureTask。12Future -&gt; RunnableFuture -&gt; FutureTaskRunnable -&gt; RunnableFuture FutureTask 通过 RunnableFuture 间接实现了Future 和 Runnable 接口，所以每个 Runnable 通常都先包装成 FutureTask，然后调用 executor.execute(Runnable command) 将其提交给线程池 我们知道，Runnable 的 void run() 方法是没有返回值的，所以，通常，如果我们需要的话，会在 submit 中指定第二个参数作为返回值：1&lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); 其实到时候会通过这两个参数，将其包装成Callable。 Callable 也是因为线程池的需要，所以才有了这个接口。它和 Runnable 的区别在于 run() 没有返回值，而 Callable 的 call() 方法有返回值，同时，如果运行出现异常，call() 方法会抛出异常。1234public interface Callable&lt;V&gt; &#123; V call() throws Exception;&#125; 在这里，就不展开说 FutureTask 类了，因为本文篇幅本来就够大了，这里我们需要知道怎么用就行了。 下面，我们来看看ExecutorService的抽象实现AbstractExecutorService。 AbstractExecutorServiceAbstractExecutorService 抽象类派生自 ExecutorService 接口，然后在其基础上实现了几个实用的方法，这些方法提供给子类进行调用。 这个抽象类实现了invokeAny方法和invokeAll方法，这里的两个newTaskFor方法也比较有用，用于将任务包装成 FutureTask。定义于最上层接口 Executor中的 void execute(Runnable command) 由于不需要获取结果，不会进行 FutureTask 的包装。 需要获取结果（FutureTask），用 submit 方法，不需要获取结果，可以用 execute 方法。 下面，我将一行一行源码地来分析这个类，跟着源码来看看其实现吧： Tips: invokeAny 和 invokeAll 方法占了这整个类的绝大多数篇幅，读者可以选择适当跳过，因为它们可能在你的实践中使用的频次比较低，而且它们不带有承前启后的作用，不用担心会漏掉什么导致看不懂后面的代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263public abstract class AbstractExecutorService implements ExecutorService &#123; // RunnableFuture 是用于获取执行结果的，我们常用它的子类 FutureTask // 下面两个 newTaskFor 方法用于将我们的任务包装成 FutureTask 提交到线程池中执行 protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Runnable runnable, T value) &#123; return new FutureTask&lt;T&gt;(runnable, value); &#125; protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Callable&lt;T&gt; callable) &#123; return new FutureTask&lt;T&gt;(callable); &#125; // 提交任务 public Future&lt;?&gt; submit(Runnable task) &#123; if (task == null) throw new NullPointerException(); // 1. 将任务包装成 FutureTask RunnableFuture&lt;Void&gt; ftask = newTaskFor(task, null); // 2. 交给执行器执行，execute 方法由具体的子类来实现 // 前面也说了，FutureTask 间接实现了Runnable 接口。 execute(ftask); return ftask; &#125; public &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result) &#123; if (task == null) throw new NullPointerException(); // 1. 将任务包装成 FutureTask RunnableFuture&lt;T&gt; ftask = newTaskFor(task, result); // 2. 交给执行器执行 execute(ftask); return ftask; &#125; public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; if (task == null) throw new NullPointerException(); // 1. 将任务包装成 FutureTask RunnableFuture&lt;T&gt; ftask = newTaskFor(task); // 2. 交给执行器执行 execute(ftask); return ftask; &#125; // 此方法目的：将 tasks 集合中的任务提交到线程池执行，任意一个线程执行完后就可以结束了 // 第二个参数 timed 代表是否设置超时机制，超时时间为第三个参数， // 如果 timed 为 true，同时超时了还没有一个线程返回结果，那么抛出 TimeoutException 异常 private &lt;T&gt; T doInvokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, boolean timed, long nanos) throws InterruptedException, ExecutionException, TimeoutException &#123; if (tasks == null) throw new NullPointerException(); // 任务数 int ntasks = tasks.size(); if (ntasks == 0) throw new IllegalArgumentException(); // List&lt;Future&lt;T&gt;&gt; futures= new ArrayList&lt;Future&lt;T&gt;&gt;(ntasks); // ExecutorCompletionService 不是一个真正的执行器，参数 this 才是真正的执行器 // 它对执行器进行了包装，每个任务结束后，将结果保存到内部的一个 completionQueue 队列中 // 这也是为什么这个类的名字里面有个 Completion 的原因吧。 ExecutorCompletionService&lt;T&gt; ecs = new ExecutorCompletionService&lt;T&gt;(this); try &#123; // 用于保存异常信息，此方法如果没有得到任何有效的结果，那么我们可以抛出最后得到的一个异常 ExecutionException ee = null; long lastTime = timed ? System.nanoTime() : 0; Iterator&lt;? extends Callable&lt;T&gt;&gt; it = tasks.iterator(); // 首先先提交一个任务，后面的任务到下面的 for 循环一个个提交 futures.add(ecs.submit(it.next())); // 提交了一个任务，所以任务数量减 1 --ntasks; // 正在执行的任务数(提交的时候 +1，任务结束的时候 -1) int active = 1; for (;;) &#123; // ecs 上面说了，其内部有一个 completionQueue 用于保存执行完成的结果 // BlockingQueue 的 poll 方法不阻塞，返回 null 代表队列为空 Future&lt;T&gt; f = ecs.poll(); // 为 null，说明刚刚提交的第一个线程还没有执行完成 // 在前面先提交一个任务，加上这里做一次检查，也是为了提高性能 if (f == null) &#123; if (ntasks &gt; 0) &#123; --ntasks; futures.add(ecs.submit(it.next())); ++active; &#125; // 这里是 else if，不是 if。这里说明，没有任务了，同时 active 为 0 说明 // 任务都执行完成了。其实我也没理解为什么这里做一次 break？ // 因为我认为 active 为 0 的情况，必然从下面的 f.get() 返回了 // 2018-02-23 感谢读者 newmicro 的 comment， // 这里的 active == 0，说明所有的任务都执行失败，那么这里是 for 循环出口 else if (active == 0) break; // 这里也是 else if。这里说的是，没有任务了，但是设置了超时时间，这里检测是否超时 else if (timed) &#123; // 带等待的 poll 方法 f = ecs.poll(nanos, TimeUnit.NANOSECONDS); // 如果已经超时，抛出 TimeoutException 异常，这整个方法就结束了 if (f == null) throw new TimeoutException(); long now = System.nanoTime(); nanos -= now - lastTime; lastTime = now; &#125; // 这里是 else。说明，没有任务需要提交，但是池中的任务没有完成，还没有超时(如果设置了超时) // take() 方法会阻塞，直到有元素返回，说明有任务结束了 else f = ecs.take(); &#125; /* * 我感觉上面这一段并不是很好理解，这里简单说下。 * 1. 首先，这在一个 for 循环中，我们设想每一个任务都没那么快结束， * 那么，每一次都会进到第一个分支，进行提交任务，直到将所有的任务都提交了 * 2. 任务都提交完成后，如果设置了超时，那么 for 循环其实进入了“一直检测是否超时” 这件事情上 * 3. 如果没有设置超时机制，那么不必要检测超时，那就会阻塞在 ecs.take() 方法上， 等待获取第一个执行结果 * 4. 如果所有的任务都执行失败，也就是说 future 都返回了， 但是 f.get() 抛出异常，那么从 active == 0 分支出去(感谢 newmicro 提出) // 当然，这个需要看下面的 if 分支。 */ // 有任务结束了 if (f != null) &#123; --active; try &#123; // 返回执行结果，如果有异常，都包装成 ExecutionException return f.get(); &#125; catch (ExecutionException eex) &#123; ee = eex; &#125; catch (RuntimeException rex) &#123; ee = new ExecutionException(rex); &#125; &#125; &#125;// 注意看 for 循环的范围，一直到这里 if (ee == null) ee = new ExecutionException(); throw ee; &#125; finally &#123; // 方法退出之前，取消其他的任务 for (Future&lt;T&gt; f : futures) f.cancel(true); &#125; &#125; public &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException &#123; try &#123; return doInvokeAny(tasks, false, 0); &#125; catch (TimeoutException cannotHappen) &#123; assert false; return null; &#125; &#125; public &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException &#123; return doInvokeAny(tasks, true, unit.toNanos(timeout)); &#125; // 执行所有的任务，返回任务结果。 // 先不要看这个方法，我们先想想，其实我们自己提交任务到线程池，也是想要线程池执行所有的任务 // 只不过，我们是每次 submit 一个任务，这里以一个集合作为参数提交 public &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException &#123; if (tasks == null) throw new NullPointerException(); List&lt;Future&lt;T&gt;&gt; futures = new ArrayList&lt;Future&lt;T&gt;&gt;(tasks.size()); boolean done = false; try &#123; // 这个很简单 for (Callable&lt;T&gt; t : tasks) &#123; // 包装成 FutureTask RunnableFuture&lt;T&gt; f = newTaskFor(t); futures.add(f); // 提交任务 execute(f); &#125; for (Future&lt;T&gt; f : futures) &#123; if (!f.isDone()) &#123; try &#123; // 这是一个阻塞方法，直到获取到值，或抛出了异常 // 这里有个小细节，其实 get 方法签名上是会抛出 InterruptedException 的 // 可是这里没有进行处理，而是抛给外层去了。此异常发生于还没执行完的任务被取消了 f.get(); &#125; catch (CancellationException ignore) &#123; &#125; catch (ExecutionException ignore) &#123; &#125; &#125; &#125; done = true; // 这个方法返回，不像其他的场景，返回 List&lt;Future&gt;，其实执行结果还没出来 // 这个方法返回是真正的返回，任务都结束了 return futures; &#125; finally &#123; // 为什么要这个？就是上面说的有异常的情况 if (!done) for (Future&lt;T&gt; f : futures) f.cancel(true); &#125; &#125; // 带超时的 invokeAll，我们找不同吧 public &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException &#123; if (tasks == null || unit == null) throw new NullPointerException(); long nanos = unit.toNanos(timeout); List&lt;Future&lt;T&gt;&gt; futures = new ArrayList&lt;Future&lt;T&gt;&gt;(tasks.size()); boolean done = false; try &#123; for (Callable&lt;T&gt; t : tasks) futures.add(newTaskFor(t)); long lastTime = System.nanoTime(); Iterator&lt;Future&lt;T&gt;&gt; it = futures.iterator(); // 提交一个任务，检测一次是否超时 while (it.hasNext()) &#123; execute((Runnable)(it.next())); long now = System.nanoTime(); nanos -= now - lastTime; lastTime = now; // 超时 if (nanos &lt;= 0) return futures; &#125; for (Future&lt;T&gt; f : futures) &#123; if (!f.isDone()) &#123; if (nanos &lt;= 0) return futures; try &#123; // 调用带超时的 get 方法，这里的参数 nanos 是剩余的时间， // 因为上面其实已经用掉了一些时间了 f.get(nanos, TimeUnit.NANOSECONDS); &#125; catch (CancellationException ignore) &#123; &#125; catch (ExecutionException ignore) &#123; &#125; catch (TimeoutException toe) &#123; return futures; &#125; long now = System.nanoTime(); nanos = now - lastTime; lastTime = now; &#125; &#125; done = true; return futures; &#125; finally &#123; if (!done) for (Future&lt;T&gt; f : futures) f.cancel(true); &#125; &#125;&#125; 到这里，我们发现，这个抽象类包装了一些基本的方法，可是像 submit、invokeAny、invokeAll 等方法，它们都没有真正开启线程来执行任务，它们都只是在方法内部调用了execute方法，所以最重要的 execute(Runnable runnable) 方法还没出现，需要等具体执行器来实现这个最重要的部分，这里我们要说的就是ThreadPoolExecutor类了。 鉴于本文的篇幅，我觉得看到这里的读者应该已经不多了，快餐文化使然啊！我写的每篇文章都力求让读者可以通过我的一篇文章而记住所有的相关知识点，所以篇幅不免长了些。其实，工作了很多年的话，会有一个感觉，比如说线程池，即使看了 20 篇各种总结，也不如一篇长文实实在在讲解清楚每一个知识点，有点少即是多，多即是少的意味了。 ThreadPoolExecutorThreadPoolExecutor 是 JDK 中的线程池实现，这个类实现了一个线程池需要的各个方法，它实现了任务提交、线程管理、监控等等方法。 我们可以基于它来进行业务上的扩展，以实现我们需要的其他功能，比如实现定时任务的类 ScheduledThreadPoolExecutor就继承自 ThreadPoolExecutor。当然，这不是本文关注的重点，下面，还是赶紧进行源码分析吧。 首先，我们来看看线程池实现中的几个概念和处理流程。 我们先回顾下提交任务的几个方法：123456789101112131415161718public Future&lt;?&gt; submit(Runnable task) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;Void&gt; ftask = newTaskFor(task, null); execute(ftask); return ftask;&#125;public &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;T&gt; ftask = newTaskFor(task, result); execute(ftask); return ftask;&#125;public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;T&gt; ftask = newTaskFor(task); execute(ftask); return ftask;&#125; 一个最基本的概念是，submit 方法中，参数是 Runnable 类型（也有Callable 类型），这个参数不是用于 new Thread(runnable).start() 中的，此处的这个参数不是用于启动线程的，这里指的是任务，任务要做的事情是 run() 方法里面定义的或 Callable 中的 call() 方法里面定义的。 初学者往往会搞混这个，因为 Runnable 总是在各个地方出现，经常把一个 Runnable 包到另一个 Runnable 中。请把它想象成有个 Task 接口，这个接口里面有一个 run() 方法（我想作者只是不想因为这个再定义一个完全可以用 Runnable 来代替的接口，Callable 的出现，完全是因为 Runnable 不能满足需要）。 我们回过神来继续往下看，我画了一个简单的示意图来描述线程池中的一些主要的构件： 当然，上图没有考虑队列是否有界，提交任务时队列满了怎么办？什么情况下会创建新的线程？提交任务时线程池满了怎么办？空闲线程怎么关掉？这些问题下面我们会一一解决。 我们经常会使用 Executors 这个工具类来快速构造一个线程池，对于初学者而言，这种工具类是很有用的，开发者不需要关注太多的细节，只要知道自己需要一个线程池，仅仅提供必需的参数就可以了，其他参数都采用作者提供的默认值。12345678910public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125;public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 这里先不说有什么区别，它们最终都会导向这个构造方法：1234567891011121314151617181920212223public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); // 这几个参数都是必须要有的 if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; &#125; 基本上，上面的构造方法中列出了我们最需要关心的几个属性了，下面逐个介绍下构造方法中出现的这几个属性： corePoolSize 核心线程数，不要抠字眼，反正先记着有这么个属性就可以了。 maximumPoolSize ​最大线程数，线程池允许创建的最大线程数。 keepAliveTime 当线程数大于corePoolSize时，多余的空闲线程在终止之前等待新任务的最长时间。注意这个值并不会对所有线程起作用，如果线程池中的线程数少于等于核心线程数 corePoolSize，那么这些线程不会因为空闲太长时间而被关闭，当然，也可以通过调用 allowCoreThreadTimeOut(true)使核心线程数内的线程也可以被回收。 unit keepAliveTime的时间单位 workQueue 在执行任务之前用于暂存任务的队列,队列只能暂存通过execute（Runnable）提交的任务。workQueue 是BlockingQueue接口的某个实现（常使用 ArrayBlockingQueue 和 LinkedBlockingQueue） threadFactory 用于生成线程，一般我们可以用默认的就可以了。通常，我们可以通过它将我们的线程的名字设置得比较可读一些，如 Message-Thread-1， Message-Thread-2 类似这样。 handler： 当线程池已经满了，但是又有新的任务提交的时候，该采取什么策略由这个来指定。有几种方式可供选择，像抛出异常、直接拒绝然后返回等，也可以自己实现相应的接口实现自己的逻辑，这个之后再说。 除了上面几个属性外，我们再看看其他重要的属性。 Doug Lea 采用一个 32 位的整数来存放线程池的状态和当前池中的线程数，其中高 3 位用于存放线程池状态，低 29 位表示线程数（即使只有 29 位，也已经不小了，大概 5 亿多，现在还没有哪个机器能起这么多线程的吧）。我们知道，java 语言在整数编码上是统一的，都是采用补码的形式，下面是简单的移位操作和布尔操作，都是挺简单的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));// 这里 COUNT_BITS 设置为 29(32-3)，意味着前三位用于存放线程状态，后29位用于存放线程数// 很多初学者很喜欢在自己的代码中写很多 29 这种数字，或者某个特殊的字符串，然后分布在各个地方，这是非常糟糕的private static final int COUNT_BITS = Integer.SIZE - 3;// 000 11111111111111111111111111111// 这里得到的是 29 个 1，也就是说线程池的最大线程数是 2^29-1=536870911// 以我们现在计算机的实际情况，这个数量还是够用的private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// 我们说了，线程池的状态存放在高 3 位中// 运算结果为 111跟29个0：111 00000000000000000000000000000private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;// 000 00000000000000000000000000000private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;// 001 00000000000000000000000000000private static final int STOP = 1 &lt;&lt; COUNT_BITS;// 010 00000000000000000000000000000private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;// 011 00000000000000000000000000000private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;// 将整数 c 的低 29 位修改为 0，就得到了线程池的状态private static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; &#125;// 将整数 c 的高 3 为修改为 0，就得到了线程池中的线程数private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125;private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125;/* * Bit field accessors that don't require unpacking ctl. * These depend on the bit layout and on workerCount being never negative. */private static boolean runStateLessThan(int c, int s) &#123; return c &lt; s;&#125;private static boolean runStateAtLeast(int c, int s) &#123; return c &gt;= s;&#125;private static boolean isRunning(int c) &#123; return c &lt; SHUTDOWN;&#125; 上面就是对一个整数的简单的位操作，几个操作方法将会在后面的源码中一直出现，所以读者最好把方法名字和其代表的功能记住，看源码的时候也就不需要来来回回翻了。 在这里，介绍下线程池中的各个状态和状态变化的转换过程： RUNNING：这个没什么好说的，这是最正常的状态：接受新的任务，处理等待队列中的任务 SHUTDOWN：不接受新的任务提交，但是会继续处理等待队列中的任务 STOP：不接受新的任务提交，不再处理等待队列中的任务，中断正在执行任务的线程 TIDYING：所有的任务都销毁了，workCount 为 0。线程池的状态在转换为 TIDYING 状态时，会执行钩子方法 terminated() TERMINATED：terminated()方法结束后，线程池的状态就会变成这个 RUNNING 定义为 -1，SHUTDOWN 定义为 0，其他的都比 0 大，所以等于 0 的时候不能提交任务，大于 0 的话，连正在执行的任务也需要中断。 看了这几种状态的介绍，读者大体也可以猜到十之八九的状态转换了，各个状态的转换过程有以下几种： RUNNING -&gt; SHUTDOWN：当调用了 shutdown() 后，会发生这个状态转换，这也是最重要的 (RUNNING or SHUTDOWN) -&gt; STOP：当调用 shutdownNow() 后，会发生这个状态转换，这下要清楚 shutDown() 和 shutDownNow() 的区别了 SHUTDOWN -&gt; TIDYING：当任务队列和线程池都清空后，会由 SHUTDOWN 转换为 TIDYING STOP -&gt; TIDYING：当任务队列清空后，发生这个转换 TIDYING -&gt; TERMINATED：这个前面说了，当 terminated() 方法结束后 上面的几个记住核心的就可以了，尤其第一个和第二个。 另外，我们还要看看一个内部类 Worker，因为 Doug Lea 把线程池中的线程包装成了一个个 Worker，翻译成工人，就是线程池中做任务的线程。所以到这里，我们知道任务是 Runnable（内部叫 task 或 command），线程是 Worker。 Worker 这里又用到了抽象类 AbstractQueuedSynchronizer。题外话，AQS 在并发中真的是到处出现，而且非常容易使用，写少量的代码就能实现自己需要的同步方式（对 AQS 源码感兴趣的读者请参看我之前写的几篇文章）。1234567891011121314151617181920212223242526272829303132private final class Worker extends AbstractQueuedSynchronizer implements Runnable&#123; private static final long serialVersionUID = 6138294804551838833L; // 这个是真正的线程，任务靠你啦 final Thread thread; // 前面说了，这里的 Runnable 是任务。为什么叫 firstTask？因为在创建线程的时候，如果同时指定了 // 这个线程起来以后需要执行的第一个任务，那么第一个任务就是存放在这里的(线程可不止执行这一个任务) // 当然了，也可以为 null，这样线程起来了，自己到任务队列（BlockingQueue）中取任务（getTask 方法）就行了 Runnable firstTask; // 用于存放此线程完全的任务数，注意了，这里用了 volatile，保证可见性 volatile long completedTasks; // Worker 只有这一个构造方法，传入 firstTask，也可以传 null Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; // 调用 ThreadFactory 来创建一个新的线程 this.thread = getThreadFactory().newThread(this); &#125; // 这里调用了外部类的 runWorker 方法 public void run() &#123; runWorker(this); &#125; ...// 其他几个方法没什么好看的，就是用 AQS 操作，来获取这个线程的执行权，用了独占锁&#125; 前面虽然啰嗦，但是简单。有了上面的这些基础后，我们终于可以看看 ThreadPoolExecutor 的 execute 方法了，前面源码分析的时候也说了，各种方法都最终依赖于 execute 方法：12]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[URI设计原则]]></title>
    <url>%2F2017%2F08%2F25%2F2017-08-25-uri%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 咱们设计的REST API真的nice么？优雅型：http://api.exapmle.com/louvre/da-vinci/mona-lisa卢浮宫/达芬奇/蒙娜丽莎中庸型：http://58.com/bj/ershou/310976北京/二手频道/帖子ID谢特型：http://api.example.com/68dd0-a9d3-11e0-9f1c不知道什么鬼 本文将分享URI设计的一些原则。 1. URI的末尾不要添加“/”多一个斜杠，语义完全不同，究竟是目录，还是资源，还是不确定而多做一次301跳转？负面case：http://api.canvas.com/shapes/正面case：http://api.canvas.com/shapes 2. 使用“-”提高URI的可读性目的是使得URI便于理解，用“-”来连接单词正面case：http://api.example.com/blogs/my-first-post 3. 禁止在URL中使用“_”目的是提高可读性，“_”可能被文本查看器中的下划线特效遮蔽负面case：http://api.example.com/blogs/my_first_post 别争，看到效果就明白了 4. 禁止使用大写字母RFC 3986中规定URI区分大小写，但别用大写字母来为难程序员了，既不美观，又麻烦负面case：http://api.example.com/My-Folder/My-Doc正面case：http://api.example.com/my-folder/my-doc 5. 不要在URI中包含扩展名应鼓励REST API客户端使用HTTP提供的格式选择机制Accept request header正面case：http://58.com/bj/ershou/310976一个case：http://58.com/bj/ershou/310976x.shtml 6. 建议URI中的名称使用复数额，楼主不知道为何会有这么奇怪的建正面case：http://api.college.com/students/3248234/courses负面case：http://api.college.com/student/3248234/course 最后，给后端研发工程师一个建议：清晰优雅的 RESTful API是为调用者编写的，别无脑随意定义一些shit一样的URI给移动/前端工程师使用，小心生命有危险。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构师之路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解读 Java 并发队列 BlockingQueue]]></title>
    <url>%2F2017%2F08%2F22%2F2017-08-22-blockingqueue%2F</url>
    <content type="text"><![CDATA[原文链接:https://javadoop.com/post/java-concurrent-queue 最近得空，想写篇文章好好说说 java 线程池问题，我相信很多人都一知半解的，包括我自己在仔仔细细看源码之前，也有许多的不解，甚至有些地方我一直都没有理解到位。 说到线程池实现，那么就不得不涉及到各种 BlockingQueue 的实现，那么我想就 BlockingQueue 的问题和大家分享分享我了解的一些知识。 本文没有像之前分析 AQS 那样一行一行源码分析了，不过还是把其中最重要和最难理解的代码说了一遍，所以不免篇幅略长。本文涉及到比较多的 Doug Lea 对 BlockingQueue 的设计思想，希望有心的读者真的可以有一些收获，我觉得自己还是写了一些干货的。 本文直接参考 Doug Lea 写的 Java doc 和注释，这也是我们在学习 java 并发包时最好的材料了。希望大家能有所思、有所悟，学习 Doug Lea 的代码风格，并将其优雅、严谨的作风应用到我们写的每一行代码中。 BlockingQueue开篇先介绍下 BlockingQueue 这个接口的规则，后面再看其实现。 首先，最基本的来说， BlockingQueue 是一个先进先出的队列（Queue），为什么说是阻塞（Blocking）的呢？是因为 BlockingQueue 支持当获取队列元素但是队列为空时，会阻塞等待队列中有元素再返回；也支持添加元素时，如果队列已满，那么等到队列可以放入新元素时再放入。 BlockingQueue 是一个接口，继承自Queue，所以其实现类也可以作为 Queue 的实现来使用，而 Queue 又继承自 Collection 接口。 BlockingQueue 对插入操作、移除操作、获取元素操作提供了四种不同的方法用于不同的场景中使用： 抛出异常； 返回特殊值（null 或 true/false，取决于具体的操作）； 阻塞等待此操作，直到这个操作成功； 阻塞等待此操作，直到成功或者超时指定时间。总结如下： Throws exception Special value Blocks Times out Insert add(e) offer(e) put(e) offer(e, time, unit) Remove remove() poll() take() poll(time, unit) Examine element() peek() not applicable not applicable BlockingQueue 的各个实现都遵循了这些规则，当然我们也不用死记这个表格，知道有这么回事，然后写代码的时候根据自己的需要去看方法的注释来选取合适的方法即可。 对于 BlockingQueue，我们的关注点应该在 put(e)和 take() 这两个方法，因为这两个方法是带阻塞的。 BlockingQueue 不接受 null 值的插入，相应的方法在碰到 null 的插入时会抛出 NullPointerException 异常。null 值在这里通常用于作为特殊值返回（表格中的第三列），代表 poll 失败。所以，如果允许插入 null 值的话，那获取的时候，就不能很好地用 null 来判断到底是代表失败，还是获取的值就是 null 值。 一个 BlockingQueue 可能是有界的，如果在插入的时候，发现队列满了，那么 put 操作将会阻塞。通常，在这里我们说的无界队列也不是说真正的无界，而是它的容量是 Integer.MAX_VALUE（21亿多）。 BlockingQueue 是设计用来实现生产者-消费者队列的，当然，你也可以将它当做普通的 Collection 来用，前面说了，它实现了 java.util.Collection 接口。例如，我们可以用 remove(x) 来删除任意一个元素，但是，这类操作通常并不高效，所以尽量只在少数的场合使用，比如一条消息已经入队，但是需要做取消操作的时候。 BlockingQueue 的实现都是线程安全的，但是批量的集合操作如 addAll, containsAll, retainAll 和 removeAll 不一定是原子操作。如 addAll(c) 有可能在添加了一些元素后中途抛出异常，此时 BlockingQueue 中已经添加了部分元素，这个是允许的，取决于具体的实现。 BlockingQueue 不支持 close 或 shutdown 等关闭操作，因为开发者可能希望不会有新的元素添加进去，此特性取决于具体的实现，不做强制约束。 最后，BlockingQueue 在生产者-消费者的场景中，是支持多消费者和多生产者的，说的其实就是线程安全问题。 相信上面说的每一句都很清楚了，BlockingQueue 是一个比较简单的线程安全容器，下面我会分析其具体的在 JDK 中的实现，这里又到了 Doug Lea 表演时间了。 BlockingQueue 实现之 ArrayBlockingQueueArrayBlockingQueue 是 BlockingQueue 接口的有界队列实现类，底层采用数组来实现。 其并发控制采用可重入锁来控制，不管是插入操作还是读取操作，都需要获取到锁才能进行操作。 如果读者看过我之前写的《一行一行源码分析清楚 AbstractQueuedSynchronizer（二）》 的关于 Condition 的文章的话，那么你一定能很容易看懂 ArrayBlockingQueue 的源码，它采用一个 ReentrantLock 和相应的两个 Condition 来实现。 ArrayBlockingQueue 共有以下几个属性：12345678910111213// 用于存放元素的数组final Object[] items;// 下一次读取操作的位置int takeIndex;// 下一次写入操作的位置int putIndex;// 队列中的元素数量int count;// 以下几个就是控制并发用的同步器final ReentrantLock lock;private final Condition notEmpty;private final Condition notFull; 我们用个示意图来描述其同步机制： ArrayBlockingQueue 实现并发同步的原理就是，读操作和写操作都需要获取到 AQS 独占锁才能进行操作。如果队列为空，这个时候读操作的线程进入到读线程队列排队，等待写线程写入新的元素，然后唤醒读线程队列的第一个等待线程。如果队列已满，这个时候写操作的线程进入到写线程队列排队，等待读线程将队列元素移除腾出空间，然后唤醒写线程队列的第一个等待线程。 对于 ArrayBlockingQueue，我们可以在构造的时候指定以下三个参数： 队列容量，其限制了队列中最多允许的元素个数； 指定独占锁是公平锁还是非公平锁。非公平锁的吞吐量比较高，公平锁可以保证每次都是等待最久的线程获取到锁； 可以指定用一个集合来初始化，将此集合中的元素在构造方法期间就先添加到队列中。 更具体的源码我就不进行分析了，因为它就是 AbstractQueuedSynchronizer 中 Condition 的使用，感兴趣的读者请看我写的《一行一行源码分析清楚 AbstractQueuedSynchronizer（二）》，因为只要看懂了那篇文章，ArrayBlockingQueue 的代码就没有分析的必要了，当然，如果你完全不懂 Condition，那么基本上也就可以说看不懂 ArrayBlockingQueue 的源码了。 BlockingQueue 实现之 LinkedBlockingQueue底层基于单向链表实现的阻塞队列，可以当做无界队列也可以当做有界队列来使用。看构造方法：12345678910// 传说中的无界队列public LinkedBlockingQueue() &#123; this(Integer.MAX_VALUE);&#125;// 传说中的有界队列public LinkedBlockingQueue(int capacity) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node&lt;E&gt;(null);&#125; 我们看看这个类有哪些属性：1234567891011121314151617181920212223// 队列容量private final int capacity;// 队列中的元素数量private final AtomicInteger count = new AtomicInteger(0);// 队头private transient Node&lt;E&gt; head;// 队尾private transient Node&lt;E&gt; last;// take, poll, peek 等读操作的方法需要获取到这个锁private final ReentrantLock takeLock = new ReentrantLock();// 如果读操作的时候队列是空的，那么等待 notEmpty 条件private final Condition notEmpty = takeLock.newCondition();// put, offer 等写操作的方法需要获取到这个锁private final ReentrantLock putLock = new ReentrantLock();// 如果写操作的时候队列是满的，那么等待 notFull 条件private final Condition notFull = putLock.newCondition(); 这里用了两个锁，两个 Condition，简单介绍如下： takeLock 和 notEmpty 怎么搭配：如果要获取（take）一个元素，需要获取 takeLock 锁，但是获取了锁还不够，如果队列此时为空，还需要队列不为空（notEmpty）这个条件（Condition）。 putLock 需要和 notFull 搭配：如果要插入（put）一个元素，需要获取 putLock 锁，但是获取了锁还不够，如果队列此时已满，还需要队列不是满的（notFull）这个条件（Condition）。 首先，这里用一个示意图来看看 LinkedBlockingQueue 的并发读写控制，然后再开始分析源码： 看懂这个示意图，源码也就简单了，读操作是排好队的，写操作也是排好队的，唯一的并发问题在于一个写操作和一个读操作同时进行，只要控制好这个就可以了。 先上构造方法：12345public LinkedBlockingQueue(int capacity) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node&lt;E&gt;(null);&#125; 注意，这里会初始化一个空的头结点，那么第一个元素入队的时候，队列中就会有两个元素。读取元素时，也总是获取头节点后面的一个节点。count 的计数值不包括这个头节点。 我们来看下 put 方法是怎么将元素插入到队尾的：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public void put(E e) throws InterruptedException &#123; if (e == null) throw new NullPointerException(); // 如果你纠结这里为什么是 -1，可以看看 offer 方法。这就是个标识成功、失败的标志而已。 int c = -1; Node&lt;E&gt; node = new Node(e); final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; // 必须要获取到 putLock 才可以进行插入操作 putLock.lockInterruptibly(); try &#123; // 如果队列满，等待 notFull 的条件满足。 while (count.get() == capacity) &#123; notFull.await(); &#125; // 入队 enqueue(node); // count 原子加 1，c 还是加 1 前的值 c = count.getAndIncrement(); // 如果这个元素入队后，还有至少一个槽可以使用，调用 notFull.signal() 唤醒等待线程。 // 哪些线程会等待在 notFull 这个 Condition 上呢？ if (c + 1 &lt; capacity) notFull.signal(); &#125; finally &#123; // 入队后，释放掉 putLock putLock.unlock(); &#125; // 如果 c == 0，那么代表队列在这个元素入队前是空的（不包括head空节点）， // 那么所有的读线程都在等待 notEmpty 这个条件，等待唤醒，这里做一次唤醒操作 if (c == 0) signalNotEmpty();&#125;// 入队的代码非常简单，就是将 last 属性指向这个新元素，并且让原队尾的 next 指向这个元素// 这里入队没有并发问题，因为只有获取到 putLock 独占锁以后，才可以进行此操作private void enqueue(Node&lt;E&gt; node) &#123; // assert putLock.isHeldByCurrentThread(); // assert last.next == null; last = last.next = node;&#125;// 元素入队后，如果需要，调用这个方法唤醒读线程来读private void signalNotEmpty() &#123; final ReentrantLock takeLock = this.takeLock; takeLock.lock(); try &#123; notEmpty.signal(); &#125; finally &#123; takeLock.unlock(); &#125;&#125; 我们再看看 take 方法：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public E take() throws InterruptedException &#123; E x; int c = -1; final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; // 首先，需要获取到 takeLock 才能进行出队操作 takeLock.lockInterruptibly(); try &#123; // 如果队列为空，等待 notEmpty 这个条件满足再继续执行 while (count.get() == 0) &#123; notEmpty.await(); &#125; // 出队 x = dequeue(); // count 进行原子减 1 c = count.getAndDecrement(); // 如果这次出队后，队列中至少还有一个元素，那么调用 notEmpty.signal() 唤醒其他的读线程 if (c &gt; 1) notEmpty.signal(); &#125; finally &#123; // 出队后释放掉 takeLock takeLock.unlock(); &#125; // 如果 c == capacity，那么说明在这个 take 方法发生的时候，队列是满的 // 既然出队了一个，那么意味着队列不满了，唤醒写线程去写 if (c == capacity) signalNotFull(); return x;&#125;// 取队头，出队private E dequeue() &#123; // assert takeLock.isHeldByCurrentThread(); // assert head.item == null; // 之前说了，头结点是空的 Node&lt;E&gt; h = head; Node&lt;E&gt; first = h.next; h.next = h; // help GC // 设置这个为新的头结点 head = first; E x = first.item; first.item = null; return x;&#125;// 元素出队后，如果需要，调用这个方法唤醒写线程来写private void signalNotFull() &#123; final ReentrantLock putLock = this.putLock; putLock.lock(); try &#123; notFull.signal(); &#125; finally &#123; putLock.unlock(); &#125;&#125; 源码分析就到这里结束了吧，毕竟还是比较简单的源码，基本上只要读者认真点都看得懂。 BlockingQueue 实现之 SynchronousQueue它是一个特殊的队列，它的名字其实就蕴含了它的特征 - - 同步的队列。为什么说是同步的呢？这里说的并不是多线程的并发问题，而是因为当一个线程往队列中写入一个元素时，写入操作不会立即返回，需要等待另一个线程来将这个元素拿走；同理，当一个读线程做读操作的时候，同样需要一个相匹配的写线程的写操作。这里的 Synchronous 指的就是读线程和写线程需要同步，一个读线程匹配一个写线程。 我们比较少使用到 SynchronousQueue 这个类，不过它在线程池的实现类ScheduledThreadPoolExecutor中得到了应用，感兴趣的读者可以在看完这个后去看看相应的使用。 虽然上面我说了队列，但是 SynchronousQueue 的队列其实是虚的，其不提供任何空间（一个都没有）来存储元素。数据必须从某个写线程交给某个读线程，而不是写到某个队列中等待被消费。 你不能在 SynchronousQueue 中使用 peek 方法（在这里这个方法直接返回 null），peek 方法的语义是只读取不移除，显然，这个方法的语义是不符合 SynchronousQueue 的特征的。SynchronousQueue 也不能被迭代，因为根本就没有元素可以拿来迭代的。虽然 SynchronousQueue 间接地实现了 Collection 接口，但是如果你将其当做 Collection 来用的话，那么集合是空的。当然，这个类也是不允许传递 null 值的（并发包中的容器类好像都不支持插入 null 值，因为 null 值往往用作其他用途，比如用于方法的返回值代表操作失败）。 接下来，我们来看看具体的源码实现吧，它的源码不是很简单的那种，我们需要先搞清楚它的设计思想。 源码加注释大概有 1200 行，我们先看大框架：12345678910111213// 构造时，我们可以指定公平模式还是非公平模式，区别之后再说public SynchronousQueue(boolean fair) &#123; transferer = fair ? new TransferQueue() : new TransferStack();&#125;abstract static class Transferer &#123; // 从方法名上大概就知道，这个方法用于转移元素，从生产者手上转到消费者手上 // 也可以被动地，消费者调用这个方法来从生产者手上取元素 // 第一个参数 e 如果不是 null，代表场景为：将元素从生产者转移给消费者 // 如果是 null，代表消费者等待生产者提供元素，然后返回值就是相应的生产者提供的元素 // 第二个参数代表是否设置超时，如果设置超时，超时时间是第三个参数的值 // 返回值如果是 null，代表超时，或者中断。具体是哪个，可以通过检测中断状态得到。 abstract Object transfer(Object e, boolean timed, long nanos);&#125; Transferer 有两个内部实现类，是因为构造 SynchronousQueue 的时候，我们可以指定公平策略。公平模式意味着，所有的读写线程都遵守先来后到，FIFO 嘛，对应 TransferQueue。而非公平模式则对应 TransferStack。 我们先采用公平模式分析源码，然后再说说公平模式和非公平模式的区别。 接下来，我们看看 put 方法和 take 方法：12345678910111213141516// 写入值public void put(E o) throws InterruptedException &#123; if (o == null) throw new NullPointerException(); if (transferer.transfer(o, false, 0) == null) &#123; // 1 Thread.interrupted(); throw new InterruptedException(); &#125;&#125;// 读取值并移除public E take() throws InterruptedException &#123; Object e = transferer.transfer(null, false, 0); // 2 if (e != null) return (E)e; Thread.interrupted(); throw new InterruptedException();&#125; 我们看到，写操作 put(E o) 和读操作 take() 都是调用 Transferer.transfer(…) 方法，区别在于第一个参数是否为 null 值。 我们来看看 transfer 的设计思路，其基本算法如下： 当调用这个方法时，如果队列是空的，或者队列中的节点和当前的线程操作类型一致（如当前操作是 put 操作，而队列中的元素也都是写线程）。这种情况下，将当前线程加入到等待队列即可。 如果队列中有等待节点，而且与当前操作可以匹配（如队列中都是读操作线程，当前线程是写操作线程，反之亦然）。这种情况下，匹配等待队列的队头，出队，返回相应数据。 其实这里有个隐含的条件被满足了，队列如果不为空，肯定都是同种类型的节点，要么都是读操作，要么都是写操作。这个就要看到底是读线程积压了，还是写线程积压了。 我们可以假设出一个男女配对的场景：一个男的过来，如果一个人都没有，那么他需要等待；如果发现有一堆男的在等待，那么他需要排到队列后面；如果发现是一堆女的在排队，那么他直接牵走队头的那个女的。 既然这里说到了等待队列，我们先看看其实现，也就是 QNode: ```javastatic final class QNode { volatile QNode next; // 可以看出来，等待队列是单向链表 volatile Object item; // CAS’ed to or from null volatile Thread waiter; // 将线程对象保存在这里，用于挂起和唤醒 final boolean isData; // 用于判断是写线程节点(isData == true)，还是读线程节点 QNode(Object item, boolean isData) { this.item = item; this.isData = isData; } ……]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据库秒级平滑扩容架构方案]]></title>
    <url>%2F2017%2F08%2F19%2F2017-02-09-database%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 一、缘起（1）并发量大，流量大的互联网架构，一般来说，数据库上层都有一个服务层，服务层记录了“业务库名”与“数据库实例”的映射关系，通过数据库连接池向数据库路由sql语句以执行： 如上图：服务层配置用户库user对应的数据库实例物理位置为ip（其实是一个内网域名）。 （2）随着数据量的增大，数据要进行水平切分，分库后将数据分布到不同的数据库实例（甚至物理机器）上，以达到降低数据量，增强性能的扩容目的： 如上图：用户库user分布在两个实例上，ip0和ip1，服务层通过用户标识uid取模的方式进行寻库路由，模2余0的访问ip0上的user库，模2余1的访问ip1上的user库。 关于数据库水平切分，垂直切分的更多细节，详见《一分钟掌握数据库垂直拆分》 。 （3）互联网架构需要保证数据库高可用，常见的一种方式，使用双主同步+keepalived+虚ip的方式保证数据库的可用性： 如上图：两个相互同步的主库使用相同的虚ip。 如上图：当主库挂掉的时候，虚ip自动漂移到另一个主库，整个过程对调用方透明，通过这种方式保证数据库的高可用。 （4）综合上文的（2）和（3），线上实际的架构，既有水平切分，又有高可用保证，所以实际的数据库架构是这样的： 提问：如果数据量持续增大，分2个库性能扛不住了，该怎么办呢？回答：继续水平拆分，拆成更多的库，降低单库数据量，增加库主库实例（机器）数量，提高性能。 最终问题抛出：分成x个库后，随着数据量的增加，要增加到y个库，数据库扩容的过程中，能否平滑，持续对外提供服务，保证服务的可用性，是本文要讨论的问题。 二、停服务方案在讨论平滑方案之前，先简要说明下“x库拆y库”停服务的方案：（1）站点挂一个公告“为了为广大用户提供更好的服务，本站点/游戏将在今晚00:00-2:00之间升级，届时将不能登录，用户周知”（2）停服务（3）新建y个库，做好高可用（4）数据迁移，重新分布，写一个数据迁移程序，从x个库里导入到y个库里，路由规则由%x升级为%y（5）修改服务配置，原来x行配置升级为y行（6）重启服务，连接新库重新对外提供服务整个过程中，最耗时的是第四步数据迁移。 回滚方案：如果数据迁移失败，或者迁移后测试失败，则将配置改回x库，恢复服务，改天再挂公告。 方案优点：简单 方案缺点：（1）停服务，不高可用（2）技术同学压力大，所有工作要在规定时间内做完，根据经验，压力越大约容易出错（这一点很致命）（3）如果有问题第一时间没检查出来，启动了服务，运行一段时间后再发现有问题，难以回滚，需要回档，可能会丢失一部分数据 有没有更平滑的方案呢？ 三、秒级、平滑、帅气方案再次看一眼扩容前的架构，分两个库，假设每个库1亿数据量，如何平滑扩容，增加实例数，降低单库数据量呢？三个简单步骤搞定。 （1）修改配置 主要修改两处：a）数据库实例所在的机器做双虚ip，原来%2=0的库是虚ip0，现在增加一个虚ip00，%2=1的另一个库同理b）修改服务的配置（不管是在配置文件里，还是在配置中心），将2个库的数据库配置，改为4个库的数据库配置，修改的时候要注意旧库与辛苦的映射关系： %2=0的库，会变为%4=0与%4=2；%2=1的部分，会变为%4=1与%4=3；这样修改是为了保证，拆分后依然能够路由到正确的数据。 （2）reload配置，实例扩容 服务层reload配置，reload可能是这么几种方式：a）比较原始的，重启服务，读新的配置文件b）高级一点的，配置中心给服务发信号，重读配置文件，重新初始化数据库连接池 不管哪种方式，reload之后，数据库的实例扩容就完成了，原来是2个数据库实例提供服务，现在变为4个数据库实例提供服务，这个过程一般可以在秒级完成。 整个过程可以逐步重启，对服务的正确性和可用性完全没有影响：a）即使%2寻库和%4寻库同时存在，也不影响数据的正确性，因为此时仍然是双主数据同步的b）服务reload之前是不对外提供服务的，冗余的服务能够保证高可用 完成了实例的扩展，会发现每个数据库的数据量依然没有下降，所以第三个步骤还要做一些收尾工作。 （3）收尾工作，数据收缩 有这些一些收尾工作：a）把双虚ip修改回单虚ipb）解除旧的双主同步，让成对库的数据不再同步增加c）增加新的双主同步，保证高可用d）删除掉冗余数据，例如：ip0里%4=2的数据全部干掉，只为%4=0的数据提供服务啦 这样下来，每个库的数据量就降为原来的一半，数据收缩完成。 四、总结该帅气方案能够实现n库扩2n库的秒级、平滑扩容，增加数据库服务能力，降低单库一半的数据量，其核心原理是：成倍扩容，避免数据迁移。 迁移步骤：（1）修改配置（2）reload配置，实例扩容完成（3）删除冗余数据等收尾工作，数据量收缩完成]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线上服务CPU100%问题快速定位实战]]></title>
    <url>%2F2017%2F08%2F19%2F2017-08-19-cpu%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 CPU异常，如何快速定位哪个服务、哪个线程、哪段代码的问题？下面，我们进行了一次线上服务CPU问题排查实战演练。 功能问题，通过日志，单步调试相对比较好定位。性能问题，例如线上服务器CPU100%，如何找到相关服务，如何定位问题代码，更考验技术人的功底。58到家架构部，运维部，58速运技术部联合进行了一次线上服务CPU问题排查实战演练，同学们反馈有收获，特将实战演练的试题和答案公布出来，希望对大家也有帮助。 题目某服务器上部署了若干tomcat实例，即若干垂直切分的Java站点服务，以及若干Java微服务，突然收到运维的CPU异常告警。问：如何定位是哪个服务进程导致CPU过载，哪个线程导致CPU过载，哪段代码导致CPU过载? 步骤一、找到最耗CPU的进程工具：top方法： 执行top -c ，显示进程运行信息列表 键入P (大写p)，进程按照CPU使用率排序 如上图，最耗CPU的进程PID为10765 步骤二：找到最耗CPU的线程工具：top方法： top -Hp 10765 ，显示一个进程的线程运行信息列表 键入P (大写p)，线程按照CPU使用率排序图示： 如上图，进程10765内，最耗CPU的线程PID为10804 步骤三：将线程PID转化为16进制工具：printf方法：printf “%x\n” 10804图示： 如上图，10804对应的16进制是0x2a34，当然，这一步可以用计算器。 之所以要转化为16进制，是因为堆栈里，线程id是用16进制表示的。 步骤四：查看堆栈，找到线程在干嘛工具：pstack/jstack/grep方法：jstack 10765 | grep ‘0x2a34’ -C5 –color 打印进程堆栈 通过线程id，过滤得到线程堆栈图示： 如上图，找到了耗CPU高的线程对应的线程名称“AsyncLogger-1”，以及看到了该线程正在执行代码的堆栈。 希望对经常进行线上CPU问题排查的同学有帮助，如果有更好的实践，也欢迎分享。想要印象深刻，请大家务必线上实操练习哟。]]></content>
      <categories>
        <category>Java语言</category>
      </categories>
      <tags>
        <tag>OOM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息如何在网络上安全传输]]></title>
    <url>%2F2017%2F08%2F10%2F2017-08-10-message%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 和女/男票聊了一些私密的话，成天担心消息会不会被泄漏，始终不放心，看完此文，消息传输安全性的来龙去脉，终于略知一二了。 一、初级阶段：信息裸传 特点：在网络上传递明文 黑客定理一：网络上传递的数据是不安全的，属网络于黑客公共场所，能被截取 结果：传递明文无异于不穿衣服裸奔 改进方案：先加密，再在网络上传输 二、进阶阶段：传输密文 特点： 服务端和客户端先约定好加密算法，加密密钥 客户端，传输前用约定好的密钥加密 传输密文 服务端，收到消息后用约定好的密钥解密 这么传输消息安全么？ 黑客定理二：客户端的代码是不安全的，属于黑客本地范畴，能被逆向工程，任何客户端与服务端提前约定好的算法与密钥都是不安全的 结果：任何客户端的代码混淆，二进制化都只能提高黑客的破解门槛，本质是不安全的 改进方案：不能固定密钥 三、中级阶段：服务端为每个用户生成密钥 特点： 客户端和服务端提前约定好加密算法，在传递消息前，先协商密钥 客户端，请求密钥 服务端，返回密钥 然后用协商密钥加密消息，传输密文 这么传输安全么？ 结果： 如黑客定理一，网上传输的内容是不安全的，于是乎，黑客能得到加密key=X 如黑客定理二，客户端和服务端提前约定的加密算法是不安全的，于是乎，黑客能得到加密算法 于是乎，黑客截取后续传递的密文，可以用对应的算法和密钥解密 改进方案：协商的密钥不能在网络上传递 四、再进阶阶段：客户端确定密钥，密钥不再传输 特点： 协商的密钥无需在网络传输 使用“具备用户特性的东西”作为加密密钥，例如：用户密码的散列值 一人一密，每个人的密钥不同 然后密钥加密消息，传输密文 服务端从db里获取这个“具备用户特性的东西”，解密 这么传输安全么？ 黑客定理三：用户客户端内存是安全的，属于黑客远端范畴，不能被破解 当然，用户中了木马，用户的机器被控制的情况不在此列，如果机器真被控制，监控用户屏幕就好了，就不用搞得这么麻烦了 结果：使用“具备用户特性的东西”作为加密密钥，一人一密，是安全的。只是，当“具备用户特性的东西”泄漏，就有潜在风险 五、高级阶段：一次一密，密钥协商特点：每次通信前，进行密钥协商，一次一密 密钥协商过程，如下图所述，需要随机生成三次密钥，两次非对称加密密钥（公钥，私钥），一次对称加密密钥，简称安全信道建立的“三次握手”，在客户端发起安全信道建立请求后： 服务端随机生成公私钥对(公钥pk1，私钥pk2)，并将公钥pk1传给客户端(注意：此时黑客能截获pk1) 客户端随机生成公私钥对(公钥pk11，私钥pk22)，并将公钥pk22，通过pk1加密，传给服务端(注意：此时黑客能截获密文，也知道是通过pk1加密的，但由于黑客不知道私钥pk2，是无法解密的) 服务端收到密文，用私钥pk2解密，得到pk11 服务端随机生成对称加密密钥key=X，用pk11加密，传给客户端(注意：同理，黑客由密文无法解密出key) 客户端收到密文，用私钥pk22解密，可到key=X 至此，安全信道建立完毕，后续通讯用key=X加密，以保证信息的安全性 六、总结黑客定理一：网络上传递的数据是不安全的，属于黑客公共场所，能被截取 黑客定理二：客户端的代码是不安全的，属于黑客本地范畴，能被逆向工程，任何客户端与服务端提前约定好的算法与密钥都是不安全的 黑客定理三：用户客户端内存是安全的，属于黑客远端范畴，不能被破解 对于不同加密方法明： 明文消息传递如同裸奔，不安全 客户端和服务端提前约定加密算法和密钥，不安全（好多公司都是这么实现的=_=） 服务端随机生成密钥，发送给客户端，不安全 一人一密，客户端使用“具备用户特性的东西”作为加密密钥，弱安全 一次一密，三次握手建立安全信道，安全 好了，这下明白了，可以放心的和女/男票发送“啪啪啪”“咻咻咻”“嘿嘿嘿”了 只要即时通讯公司有良知，不从服务端偷看，一切都是安全的。额，这个“只要”的假设，貌似不成立]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构师之路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一行一行源码分析清楚 AbstractQueuedSynchronizer (二)]]></title>
    <url>%2F2017%2F07%2F20%2F2017-06-16-abstract-queued-synchronizer-2%2F</url>
    <content type="text"><![CDATA[原文链接:https://javadoop.com/post/AbstractQueuedSynchronizer-2 文章比较长，信息量比较大，建议在 pc 上阅读。文章标题是为了呼应前文，其实可以单独成文的，主要是希望读者看文章能系统看。 本文关注以下几点内容： 深入理解 ReentrantLock 公平锁和非公平锁的区别 深入分析 AbstractQueuedSynchronizer 中的 ConditionObject 深入理解 java 线程中断和 InterruptedException 异常 基本上本文把以上几点都说清楚了，我假设读者看过上一篇文章中对 AbstractQueuedSynchronizer 的介绍 ，当然如果你已经熟悉 AQS 中的独占锁了，那也可以直接看这篇。各小节之间基本上没什么关系，大家可以只关注自己感兴趣的部分。 公平锁和非公平锁ReentrantLock 默认采用非公平锁，除非你在构造方法中传入参数 true 。123456public ReentrantLock() &#123; sync = new NonfairSync();&#125;public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 公平锁的 lock 方法：12345678910111213141516171819202122232425262728293031static final class FairSync extends Sync &#123; final void lock() &#123; acquire(1); &#125; // AbstractQueuedSynchronizer.acquire(int arg) public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); &#125; protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; // 1. 和非公平锁相比，这里多了一个判断：是否有线程在等待 if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false; &#125;&#125; 非公平锁的 lock 方法：12345678910111213141516171819202122232425262728293031323334353637383940static final class NonfairSync extends Sync &#123; final void lock() &#123; // 2. 和公平锁相比，这里会直接先进行一次CAS，成功就返回了 if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); &#125; // AbstractQueuedSynchronizer.acquire(int arg) public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); &#125; protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires); &#125;&#125;/** * Performs non-fair tryLock. tryAcquire is implemented in * subclasses, but both need nonfair try for trylock method. */final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false;&#125; 总结：公平锁和非公平锁只有两处不同： 非公平锁在调用 lock 后，首先就会调用 CAS 进行一次抢锁，如果这个时候恰巧锁没有被占用，那么直接就获取到锁返回了。 非公平锁在 CAS 失败后，和公平锁一样都会进入到 tryAcquire 方法，在 tryAcquire 方法中，如果发现锁这个时候被释放了（state == 0），非公平锁会直接 CAS 抢锁，但是公平锁会判断等待队列是否有线程处于等待状态，如果有则不去抢锁，乖乖排到后面。 公平锁和非公平锁就这两点区别，如果这两次 CAS 都不成功，那么后面非公平锁和公平锁是一样的，都要进入到阻塞队列等待唤醒。 相对来说，非公平锁会有更好的性能，因为它的吞吐量比较大。当然，非公平锁让获取锁的时间变得更加不确定，可能会导致在阻塞队列中的线程长期处于饥饿状态。 ConditionTips: 这里重申一下，要看懂这个，必须要先看懂上一篇关于 AbstractQueuedSynchronizer 的介绍，或者你已经有相关的知识了，否则这节肯定是看不懂的。 我们先来看看 Condition 的使用场景，Condition 经常可以用在生产者-消费者的场景中，请看 Doug Lea 给出的这个例子：1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;class BoundedBuffer &#123; final Lock lock = new ReentrantLock(); // condition 依赖于 lock 来产生 final Condition notFull = lock.newCondition(); final Condition notEmpty = lock.newCondition(); final Object[] items = new Object[100]; int putptr, takeptr, count; // 生产 public void put(Object x) throws InterruptedException &#123; lock.lock(); try &#123; while (count == items.length) notFull.await(); // 队列已满，等待，直到 not full 才能继续生产 items[putptr] = x; if (++putptr == items.length) putptr = 0; ++count; notEmpty.signal(); // 生产成功，队列已经 not empty 了，发个通知出去 &#125; finally &#123; lock.unlock(); &#125; &#125; // 消费 public Object take() throws InterruptedException &#123; lock.lock(); try &#123; while (count == 0) notEmpty.await(); // 队列为空，等待，直到队列 not empty，才能继续消费 Object x = items[takeptr]; if (++takeptr == items.length) takeptr = 0; --count; notFull.signal(); // 被我消费掉一个，队列 not full 了，发个通知出去 return x; &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; （ArrayBlockingQueue 采用这种方式实现了生产者-消费者，所以请只把这个例子当做学习例子，实际生产中可以直接使用 ArrayBlockingQueue） 我们常用obj.wait()，obj.notify() 或 obj.notifyAll()来实现相似的功能，但是，它们是基于对象的监视器锁的。需要深入了解这几个方法的读者，可以参考我的另一篇文章《深入分析 java 8 编程语言规范：Threads and Locks》。而这里说的Condition 是基于 ReentrantLock 实现的，而 ReentrantLock 是依赖于 AbstractQueuedSynchronizer 实现的。 在往下看之前，读者心里要有一个整体的概念。condition 是依赖于 ReentrantLock 的，不管是调用 await 进入等待还是 signal 唤醒，都必须获取到锁才能进行操作。否则会抛出 IllegalMonitorStateException 异常。 每个 ReentrantLock 实例可以通过调用多次 newCondition 产生多个 ConditionObject 的实例：123final ConditionObject newCondition() &#123; return new ConditionObject();&#125; 我们首先来看下我们关注的 Condition 的实现类AbstractQueuedSynchronizer类中的ConditionObject。 12345678public class ConditionObject implements Condition, java.io.Serializable &#123; private static final long serialVersionUID = 1173984872572414699L; // 条件队列的第一个节点 // 不要管这里的关键字 transient，是不参与序列化的意思 private transient Node firstWaiter; // 条件队列的最后一个节点 private transient Node lastWaiter; ...... 在上一篇介绍 AQS 的时候，我们有一个阻塞队列，用于保存等待获取锁的线程的队列。这里我们引入另一个概念，叫条件队列（condition queue），我画了一张简单的图用来说明这个。 这里的阻塞队列如果叫做同步队列（sync queue）其实比较贴切，不过为了和前篇呼应，我就继续使用阻塞队列了。记住这里的两个概念，阻塞队列和条件队列。 这里，我们简单回顾下 Node 的属性： 12345volatile int waitStatus; // 可取值 0、CANCELLED(1)、SIGNAL(-1)、CONDITION(-2)、PROPAGATE(-3)volatile Node prev;volatile Node next;volatile Thread thread;Node nextWaiter; prev 和 next 用于实现阻塞队列的双向链表，nextWaiter 用于实现条件队列的单向链表 我们知道一个 ReentrantLock 实例可以通过多次调用 newCondition() 来产生多个 Condition 实例，这里对应 condition1 和 condition2。注意，ConditionObject 只有两个属性 firstWaiter 和 lastWaiter； 每个 condition 有一个关联的条件队列，如线程 1 调用 condition1.await() 方法即可将当前线程 1 包装成 Node 后加入到条件队列中，然后阻塞在这里，不继续往下执行，条件队列是一个单向链表； 调用 condition1.signal() 会将condition1 对应的条件队列的 firstWaiter 移到阻塞队列的队尾，等待获取锁，获取锁后 await 方法返回，继续往下执行。 我这里说的 1、2、3 是最简单的流程，没有考虑中断、signalAll、还有带有超时参数的 await 方法等，不过把这里弄懂是这节的主要目的。 同时，从图中也可以很直观地看出，哪些操作是线程安全的，哪些操作是线程不安全的。 这个图看懂后，下面的代码分析就简单了。 接下来，我们一步步按照流程来走代码分析，我们先来看看 wait 方法：1234567891011121314151617181920212223242526// 首先，这个方法是可被中断的，不可被中断的是另一个方法 awaitUninterruptibly()// 这个方法会阻塞，直到调用 signal 方法（指 signal() 和 signalAll()，下同），或被中断public final void await() throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); // 添加到 condition 的条件队列中 Node node = addConditionWaiter(); // 释放锁，返回值是释放锁之前的 state 值 int savedState = fullyRelease(node); int interruptMode = 0; // 这里退出循环有两种情况，之后再仔细分析 // 1. isOnSyncQueue(node) 返回 true，即当前 node 已经转移到阻塞队列了 // 2. checkInterruptWhileWaiting(node) != 0 会到 break，然后退出循环，代表的是线程中断 while (!isOnSyncQueue(node)) &#123; LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; // 被唤醒后，将进入阻塞队列，等待获取锁 if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode);&#125; 其实，我大体上也把整个 await 过程说得十之八九了，下面我们分步把上面的几个点用源码说清楚。 1. 将节点加入到条件队列addConditionWaiter() 是将当前节点加入到条件队列，看图我们知道，这种条件队列内的操作是线程安全的。123456789101112131415161718// 将当前线程对应的节点入队，插入队尾private Node addConditionWaiter() &#123; Node t = lastWaiter; // 如果条件队列的最后一个节点取消了，将其清除出去 if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) &#123; // 这个方法会遍历整个条件队列，然后会将已取消的所有节点清除出队列 unlinkCancelledWaiters(); t = lastWaiter; &#125; Node node = new Node(Thread.currentThread(), Node.CONDITION); // 如果队列为空 if (t == null) firstWaiter = node; else t.nextWaiter = node; lastWaiter = node; return node;&#125; 在addWaiter 方法中，有一个unlinkCancelledWaiters()方法，该方法用于清除队列中已经取消等待的节点。 当 await 的时候如果发生了取消操作（这点之后会说），或者是在节点入队的时候，发现最后一个节点是被取消的，会调用一次这个方法。12345678910111213141516171819202122// 等待队列是一个单向链表，遍历链表将已经取消等待的节点清除出去// 纯属链表操作，很好理解，看不懂多看几遍就可以了private void unlinkCancelledWaiters() &#123; Node t = firstWaiter; Node trail = null; while (t != null) &#123; Node next = t.nextWaiter; // 如果节点的状态不是 Node.CONDITION 的话，这个节点就是被取消的 if (t.waitStatus != Node.CONDITION) &#123; t.nextWaiter = null; if (trail == null) firstWaiter = next; else trail.nextWaiter = next; if (next == null) lastWaiter = trail; &#125; else trail = t; t = next; &#125;&#125; 2. 完全释放独占锁回到 wait 方法，节点入队了以后，会调用 int savedState = fullyRelease(node);方法释放锁，注意，这里是完全释放独占锁，因为 ReentrantLock 是可以重入的。123456789101112131415161718192021// 首先，我们要先观察到返回值 savedState 代表 release 之前的 state 值// 对于最简单的操作：先 lock.lock()，然后 condition1.await()。// 那么 state 经过这个方法由 1 变为 0，锁释放，此方法返回 1// 相应的，如果 lock 重入了 n 次，savedState == n// 如果这个方法失败，会将节点设置为"取消"状态，并抛出异常 IllegalMonitorStateExceptionfinal int fullyRelease(Node node) &#123; boolean failed = true; try &#123; int savedState = getState(); // 这里使用了当前的 state 作为 release 的参数，也就是完全释放掉锁，将 state 置为 0 if (release(savedState)) &#123; failed = false; return savedState; &#125; else &#123; throw new IllegalMonitorStateException(); &#125; &#125; finally &#123; if (failed) node.waitStatus = Node.CANCELLED; &#125;&#125; 3. 等待进入阻塞队列释放掉锁以后，接下来是这段，这边会自旋，如果发现自己还没到阻塞队列，那么挂起，等待被转移到阻塞队列。12345678int interruptMode = 0;while (!isOnSyncQueue(node)) &#123; // 线程挂起 LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break;&#125; isOnSyncQueue(Node node)用于判断节点是否已经转移到阻塞队列了： 12345678910111213141516171819202122232425262728293031323334// 在节点入条件队列的时候，初始化时设置了 waitStatus = Node.CONDITION// 前面我提到，signal 的时候需要将节点从条件队列移到阻塞队列，// 这个方法就是判断 node 是否已经移动到阻塞队列了final boolean isOnSyncQueue(Node node) &#123; // 移动过去的时候，node 的 waitStatus 会置为 0，这个之后在说 signal 方法的时候会说到 // 如果 waitStatus 还是 Node.CONDITION，也就是 -2，那肯定就是还在条件队列中 // 如果 node 的前驱 prev 指向还是 null，说明肯定没有在 阻塞队列 if (node.waitStatus == Node.CONDITION || node.prev == null) return false; // 如果 node 已经有后继节点 next 的时候，那肯定是在阻塞队列了 if (node.next != null) return true; // 这个方法从阻塞队列的队尾开始从后往前遍历找，如果找到相等的，说明在阻塞队列，否则就是不在阻塞队列 // 可以通过判断 node.prev() != null 来推断出 node 在阻塞队列吗？答案是：不能。 // 这个可以看上篇 AQS 的入队方法，首先设置的是 node.prev 指向 tail， // 然后是 CAS 操作将自己设置为新的 tail，可是这次的 CAS 是可能失败的。 // 调用这个方法的时候，往往我们需要的就在队尾的部分，所以一般都不需要完全遍历整个队列的 return findNodeFromTail(node);&#125;// 从同步队列的队尾往前遍历，如果找到，返回 trueprivate boolean findNodeFromTail(Node node) &#123; Node t = tail; for (;;) &#123; if (t == node) return true; if (t == null) return false; t = t.prev; &#125;&#125; 回到前面的循环，isOnSyncQueue(node) 返回 false 的话，那么进到LockSupport.park(this);这里线程挂起。 4. signal 唤醒线程，转移到阻塞队列为了大家理解，这里我们先看唤醒操作，因为刚刚到LockSupport.park(this);把线程挂起了，等待唤醒。 唤醒操作通常由另一个线程来操作，就像生产者-消费者模式中，如果线程因为等待消费而挂起，那么当生产者生产了一个东西后，会调用 signal 唤醒正在等待的线程来消费。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// 唤醒等待了最久的线程// 其实就是，将这个线程对应的 node 从条件队列转移到阻塞队列public final void signal() &#123; // 调用 signal 方法的线程必须持有当前的独占锁 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignal(first);&#125;// 从条件队列队头往后遍历，找出第一个需要转移的 node// 因为前面我们说过，有些线程会取消排队，但是还在队列中private void doSignal(Node first) &#123; do &#123; // 将 firstWaiter 指向 first 节点后面的第一个 // 如果将队头移除后，后面没有节点在等待了，那么需要将 lastWaiter 置为 null if ( (firstWaiter = first.nextWaiter) == null) lastWaiter = null; // 因为 first 马上要被移到阻塞队列了，和条件队列的链接关系在这里断掉 first.nextWaiter = null; &#125; while (!transferForSignal(first) &amp;&amp; (first = firstWaiter) != null); // 这里 while 循环，如果 first 转移不成功，那么选择 first 后面的第一个节点进行转移，依此类推&#125;// 将节点从条件队列转移到阻塞队列// true 代表成功转移// false 代表在 signal 之前，节点已经取消了final boolean transferForSignal(Node node) &#123; // CAS 如果失败，说明此 node 的 waitStatus 已不是 Node.CONDITION，说明节点已经取消， // 既然已经取消，也就不需要转移了，方法返回，转移后面一个节点 // 否则，将 waitStatus 置为 0 if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; // enq(node): 自旋进入阻塞队列的队尾 // 注意，这里的返回值 p 是 node 在阻塞队列的前驱节点 Node p = enq(node); int ws = p.waitStatus; // ws &gt; 0 说明 node 在阻塞队列中的前驱节点取消了等待锁，直接唤醒 node 对应的线程。唤醒之后会怎么样，后面再解释 // 如果 ws &lt;= 0, 那么 compareAndSetWaitStatus 将会被调用，上篇介绍的时候说过，节点入队后，需要把前驱节点的状态设为 Node.SIGNAL(-1) if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) // 如果前驱节点取消或者 CAS 失败，会进到这里唤醒线程，之后的操作看下一节 LockSupport.unpark(node.thread); return true;&#125; 正常情况下，ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL) 这句中，ws &lt;= 0，而且 compareAndSetWaitStatus(p, ws, Node.SIGNAL) 会返回 true，所以一般也不会进去 if 语句块中唤醒 node 对应的线程。然后这个方法返回 true，也就意味着 signal 方法结束了，节点进入了阻塞队列。 假设发生了阻塞队列中的前驱节点取消等待，或者 CAS 失败，只要唤醒线程，让其进到下一步即可。 5. 唤醒后检查中断状态上一步 signal 之后，我们的线程由条件队列转移到了阻塞队列，之后就准备获取锁了。只要重新获取到锁了以后，继续往下执行。 等线程从挂起中恢复过来，继续往下看12345678int interruptMode = 0;while (!isOnSyncQueue(node)) &#123; // 线程挂起 LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break;&#125; 先解释下 interruptMode。interruptMode 可以取值为 REINTERRUPT（1），THROW_IE（-1），0 REINTERRUPT： 代表 await 返回的时候，需要重新设置中断状态 THROW_IE： 代表 await 返回的时候，需要抛出 InterruptedException 异常 0 ：说明在 await 期间，没有发生中断 有以下四种情况会让 LockSupport.park(this); 这句返回继续往下执行： 常规路劲。signal -&gt; 转移节点到阻塞队列 -&gt; 获取了锁（unpark） 线程中断。在 park 的时候，另外一个线程对这个线程进行了中断 signal 的时候我们说过，转移以后的前驱节点取消了，或者对前驱节点的CAS操作失败了 假唤醒。这个也是存在的，和 Object.wait() 类似，都有这个问题 线程唤醒后第一步是调用checkInterruptWhileWaiting(node)这个方法，此方法用于判断是否在线程挂起期间发生了中断，如果发生了中断，是 signal 调用之前中断的，还是 signal 之后发生的中断。 // 1. 如果在 signal 之前已经中断，返回 THROW_IE// 2. 如果是 signal 之后中断，返回 REINTERRUPT// 3. 没有发生中断，返回 012345private int checkInterruptWhileWaiting(Node node) &#123; return Thread.interrupted() ? (transferAfterCancelledWait(node) ? THROW_IE : REINTERRUPT) : 0;&#125; Thread.interrupted()：如果当前线程已经处于中断状态，那么该方法返回 true，同时将中断状态重置为 false，所以，才有后续的 重新中断（REINTERRUPT） 的使用。 看看怎么判断是 signal 之前还是之后发生的中断： 1234567891011121314151617181920// 只有线程处于中断状态，才会调用此方法// 如果需要的话，将这个已经取消等待的节点转移到阻塞队列// 返回 true：如果此线程在 signal 之前被取消，final boolean transferAfterCancelledWait(Node node) &#123; // 用 CAS 将节点状态设置为 0 // 如果这步 CAS 成功，说明是 signal 方法之前发生的中断，因为如果 signal 先发生的话，signal 中会将 waitStatus 设置为 0 if (compareAndSetWaitStatus(node, Node.CONDITION, 0)) &#123; // 将节点放入阻塞队列 // 这里我们看到，即使中断了，依然会转移到阻塞队列 enq(node); return true; &#125; // 到这里是因为 CAS 失败，肯定是因为 signal 方法已经将 waitStatus 设置为了 0 // signal 方法会将节点转移到阻塞队列，但是可能还没完成，这边自旋等待其完成 // 当然，这种事情还是比较少的吧：signal 调用之后，没完成转移之前，发生了中断 while (!isOnSyncQueue(node)) Thread.yield(); return false;&#125; 这里再说一遍，即使发生了中断，节点依然会转移到阻塞队列。 到这里，大家应该都知道这个 while 循环怎么退出了吧。要么中断，要么转移成功。 6. 获取独占锁while 循环出来以后，下面是这段代码：12345678910111213141516171819202122232425262728293031323334353637if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT;``` 由于 while 出来后，我们确定节点已经进入了阻塞队列，准备获取锁。这里的 ``acquireQueued(node, savedState)`` ``的第一个参数 node 之前已经经过 enq(node) 进入了队列，参数 savedState 是之前释放锁前的 state，这个方法返回的时候，代表当前线程获取了锁，而且 state == savedState了。注意，前面我们说过，不管有没有发生中断，都会进入到阻塞队列，而 acquireQueued(node, savedState) 的返回值就是代表线程是否被中断。如果返回 true，说明被中断了，而且 interruptMode != THROW_IE，说明在 signal 之前就发生中断了，这里将 interruptMode 设置为 REINTERRUPT，用于待会重新中断。### 7. 处理中断状态到这里，我们终于可以好好说下这个 interruptMode 干嘛用了。* 0：什么都不做。* THROW_IE：await 方法抛出 InterruptedException 异常* REINTERRUPT：重新中断当前线程```javaprivate void reportInterruptAfterWait(int interruptMode) throws InterruptedException &#123; if (interruptMode == THROW_IE) throw new InterruptedException(); else if (interruptMode == REINTERRUPT) selfInterrupt();&#125;``` &gt; 为什么这么处理？这部分的知识在本文的最后一节### 带超时机制的 await经过前面的 7 步，整个 ConditionObject 类基本上都分析完了，接下来简单分析下带超时机制的 await 方法。```javapublic final long awaitNanos(long nanosTimeout) throws InterruptedExceptionpublic final boolean awaitUntil(Date deadline) throws InterruptedExceptionpublic final boolean await(long time, TimeUnit unit) throws InterruptedException 这三个方法都差不多，我们就挑一个出来看看吧：123456789101112131415161718192021222324252627282930313233343536373839public final boolean await(long time, TimeUnit unit) throws InterruptedException &#123; // 等待这么多纳秒 long nanosTimeout = unit.toNanos(time); if (Thread.interrupted()) throw new InterruptedException(); Node node = addConditionWaiter(); int savedState = fullyRelease(node); // 当前时间 + 等待时长 = 过期时间 final long deadline = System.nanoTime() + nanosTimeout; // 用于返回 await 是否超时 boolean timedout = false; int interruptMode = 0; while (!isOnSyncQueue(node)) &#123; // 时间到啦 if (nanosTimeout &lt;= 0L) &#123; // 这里因为要 break 取消等待了。取消等待的话一定要调用 transferAfterCancelledWait(node) 这个方法 // 如果这个方法返回 true，在这个方法内，将节点转移到阻塞队列成功 // 返回 false 的话，说明 signal 已经发生，signal 方法将节点转移了。也就是说没有超时嘛 timedout = transferAfterCancelledWait(node); break; &#125; // spinForTimeoutThreshold 的值是 1000 纳秒，也就是 1 毫秒 // 也就是说，如果不到 1 毫秒了，那就不要选择 parkNanos 了，自旋的性能反而更好 if (nanosTimeout &gt;= spinForTimeoutThreshold) LockSupport.parkNanos(this, nanosTimeout); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; // 得到剩余时间 nanosTimeout = deadline - System.nanoTime(); &#125; if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); return !timedout;&#125; 超时的思路还是很简单的，不带超时参数的 await 是 park，然后等待别人唤醒。而现在就是调用 parkNanos 方法来休眠指定的时间，醒来后判断是否 signal 调用了，调用了就是没有超时，否则就是超时了。超时的话，自己来进行转移到阻塞队列，然后抢锁。 不抛出 InterruptedException 的 await关于 Condition 最后一小节了。123456789101112public final void awaitUninterruptibly() &#123; Node node = addConditionWaiter(); int savedState = fullyRelease(node); boolean interrupted = false; while (!isOnSyncQueue(node)) &#123; LockSupport.park(this); if (Thread.interrupted()) interrupted = true; &#125; if (acquireQueued(node, savedState) || interrupted) selfInterrupt();&#125; 很简单，我就不废话了。 AbstractQueuedSynchronizer 独占锁的取消排队这篇文章说的是 AbstractQueuedSynchronizer，只不过好像 Condition 说太多了，赶紧把思路拉回来。 接下来，我想说说怎么取消对锁的竞争？ 上篇文章提到过，最重要的方法是这个，我们要在这里面找答案：123456789101112131415161718192021final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 首先，到这个方法的时候，节点一定是入队成功的。 我把 parkAndCheckInterrupt() 代码贴过来：1234private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; 这两段代码联系起来看，是不是就清楚了。 如果我们要取消一个线程的排队，我们需要在另外一个线程中对其进行中断。比如某线程调用 lock()老久不返回，我想中断它。一旦对其进行中断，此线程会从 LockSupport.park(this); 中唤醒，然后 Thread.interrupted(); 返回 true。 我们发现一个问题，即使是中断唤醒了这个线程，也就只是设置了 interrupted = true 然后继续下一次循环。而且，由于 Thread.interrupted(); 会清除中断状态，第二次进 parkAndCheckInterrupt 的时候，返回会是 false。 所以，我们要看到，在这个方法中，interrupted 只是用来记录是否发生了中断，然后用于方法返回值，其他没有做任何相关事情。 所以，我们看外层方法怎么处理 acquireQueued 返回 false 的情况。12345678public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125;static void selfInterrupt() &#123; Thread.currentThread().interrupt();&#125; 所以说，lock() 方法处理中断的方法就是，你中断归中断，我抢锁还是照样抢锁，几乎没关系，只是我抢到锁了以后，设置线程的中断状态而已，也不抛出任何异常出来。调用者获取锁后，可以去检查是否发生过中断，也可以不理会。 有没有被骗的感觉，我说了一大堆，可是和取消没有任何关系啊。 我们来看 ReentrantLock 的另一个 lock 方法：123public void lockInterruptibly() throws InterruptedException &#123; sync.acquireInterruptibly(1);&#125; 方法上多了个 throws InterruptedException ，经过前面那么多知识的铺垫，这里我就不再啰里啰嗦了。1234567public final void acquireInterruptibly(int arg) throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); if (!tryAcquire(arg)) doAcquireInterruptibly(arg);&#125; 继续往里：12345678910111213141516171819202122232425private void doAcquireInterruptibly(int arg) throws InterruptedException &#123; final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) // 就是这里了，一旦异常，马上结束这个方法，抛出异常。 // 这里不再只是标记这个方法的返回值代表中断状态 // 而是直接抛出异常，而且外层也不捕获，一直往外抛到 lockInterruptibly throw new InterruptedException(); &#125; &#125; finally &#123; // 如果通过 InterruptedException 异常出去，那么 failed 就是 true 了 if (failed) cancelAcquire(node); &#125;&#125; 既然到这里了，顺便说说 cancelAcquire 这个方法吧：12345678910111213141516171819202122232425262728293031323334353637private void cancelAcquire(Node node) &#123; // Ignore if node doesn't exist if (node == null) return; node.thread = null; // Skip cancelled predecessors Node pred = node.prev; while (pred.waitStatus &gt; 0) node.prev = pred = pred.prev; // predNext is the apparent node to unsplice. CASes below will // fail if not, in which case, we lost race vs another cancel // or signal, so no further action is necessary. Node predNext = pred.next; // Can use unconditional write instead of CAS here. // After this atomic step, other Nodes can skip past us. // Before, we are free of interference from other threads. node.waitStatus = Node.CANCELLED; // If we are the tail, remove ourselves. if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123; compareAndSetNext(pred, predNext, null); &#125; else &#123; // If successor needs signal, try to set pred's next-link // so it will get one. Otherwise wake it up to propagate. int ws; if (pred != head &amp;&amp; ((ws = pred.waitStatus) == Node.SIGNAL || (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp; pred.thread != null) &#123; Node next = node.next; if (next != null &amp;&amp; next.waitStatus &lt;= 0) compareAndSetNext(pred, predNext, next); &#125; else &#123; unparkSuccessor(node); &#125; node.next = node; // help GC &#125;&#125; 到这里，我想我应该把取消排队这件事说清楚了吧。 再说 java 线程中断和 InterruptedException 异常在之前的文章中，我们接触了大量的中断，这边算是个总结吧。如果你完全熟悉中断了，没有必要再看这节，本节为新手而写。 线程中断首先，我们要明白，中断不是类似 linux 里面的命令 kill -9 pid，不是说我们中断某个线程，这个线程就停止运行了。中断代表线程状态，每个线程都关联了一个中断状态，是一个 true 或 false 的 boolean 值，初始值为 false。 关于中断状态，我们需要重点关注以下几个方法：12345678910// Thread 类中的实例方法，持有线程实例引用即可检测线程中断状态public boolean isInterrupted() &#123;&#125;// Thread 中的静态方法，检测调用这个方法的线程是否已经中断// 注意：这个方法返回中断状态的同时，会将此线程的中断状态重置为 false// 所以，如果我们连续调用两次这个方法的话，第二次的返回值肯定就是 false 了public static boolean interrupted() &#123;&#125;// Thread 类中的实例方法，用于设置一个线程的中断状态为 truepublic void interrupt() &#123;&#125; 我们说中断一个线程，其实就是设置了线程的 interrupted status 为 true，至于说被中断的线程怎么处理这个状态，那是那个线程自己的事。如以下代码：1234while (!Thread.interrupted()) &#123; doWork(); System.out.println("我做完一件事了，准备做下一件，如果没有其他线程中断我的话");&#125; 当然，中断除了是线程状态外，还有其他含义，否则也不需要专门搞一个这个概念出来了。 如果线程处于以下三种情况，那么当线程被中断的时候，能自动感知到： 来自 Object 类的 wait()、wait(long)、wait(long, int)，来自 Thread 类的 join()、join(long)、join(long, int)、sleep(long)、sleep(long, int) 这几个方法的相同之处是，方法上都有: throws InterruptedException如果线程阻塞在这些方法上（我们知道，这些方法会让当前线程阻塞），这个时候如果其他线程对这个线程进行了中断，那么这个线程会从这些方法中立即返回，抛出 InterruptedException 异常，同时重置中断状态为 false。 实现了InterruptibleChannel接口的类中的一些 I/O 阻塞操作，如 DatagramChannel 中的 connect 方法和 receive 方法等 如果线程阻塞在这里，中断线程会导致这些方法抛出 ClosedByInterruptException 并重置中断状态。 Selector 中的 select 方法，这个有机会我们在讲 NIO 的时候说 一旦中断，方法立即返回 对于以上 3 种情况是最特殊的，因为他们能自动感知到中断（这里说自动，当然也是基于底层实现），并且在做出相应的操作后都会重置中断状态为 false。 那是不是只有以上 3 种方法能自动感知到中断呢？不是的，如果线程阻塞在 LockSupport.park(Object obj) ``方法，也叫挂起，这个时候的中断也会导致线程唤醒，但是唤醒后不会重置中断状态，所以唤醒后去检测中断状态将是 true。 InterruptedException 概述它是一个特殊的异常，不是说 JVM 对其有特殊的处理，而是它的使用场景比较特殊。通常，我们可以看到，像 Object 中的 wait() 方法，ReentrantLock 中的 lockInterruptibly() 方法，Thread 中的 sleep() 方法等等，这些方法都带有 throws InterruptedException，我们通常称这些方法为阻塞方法（blocking method）。 阻塞方法一个很明显的特征是，它们需要花费比较长的时间（不是绝对的，只是说明时间不可控），还有它们的方法结束返回往往依赖于外部条件，如 wait 方法依赖于其他线程的 notify，lock 方法依赖于其他线程的 unlock等等。 当我们看到方法上带有 throws InterruptedException 时，我们就要知道，这个方法应该是阻塞方法，我们如果希望它能早点返回的话，我们往往可以通过中断来实现。 除了几个特殊类（如 Object，Thread等）外，感知中断并提前返回是通过轮询中断状态来实现的。我们自己需要写可中断的方法的时候，就是通过在合适的时机（通常在循环的开始处）去判断线程的中断状态，然后做相应的操作（通常是方法直接返回或者抛出异常）。当然，我们也要看到，如果我们一次循环花的时间比较长的话，那么就需要比较长的时间才能注意到线程中断了。 处理中断一旦中断发生，我们接收到了这个信息，然后怎么去处理中断呢？本小节将简单分析这个问题。 我们经常会这么写代码：12345try &#123; Thread.sleep(10000);&#125; catch (InterruptedException e) &#123; // ignore&#125; // go on当 sleep 结束继续往下执行的时候，我们往往都不知道这块代码是真的 sleep 了 10 秒，还是只休眠了 1 秒就被中断了。这个代码的问题在于，我们将这个异常信息吞掉了。（对于 sleep 方法，我相信大部分情况下，我们都不在意是否是中断了，这里是举例） AQS 的做法很值得我们借鉴，我们知道 ReentrantLock 有两种 lock 方法：1234567public void lock() &#123; sync.lock();&#125;public void lockInterruptibly() throws InterruptedException &#123; sync.acquireInterruptibly(1);&#125; 前面我们提到过，lock() 方法不响应中断。如果 thread1 调用了 lock() 方法，过了很久还没抢到锁，这个时候 thread2 对其进行了中断，thread1 是不响应这个请求的，它会继续抢锁，当然它不会把“被中断”这个信息扔掉。我们可以看以下代码：1234567public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) // 我们看到，这里也没做任何特殊处理，就是记录下来中断状态。 // 这样，如果外层方法需要去检测的时候，至少我们没有把这个信息丢了 selfInterrupt();// Thread.currentThread().interrupt();&#125; 而对于 lockInterruptibly() 方法，因为其方法上面有 throws InterruptedException ，这个信号告诉我们，如果我们要取消线程抢锁，直接中断这个线程即可，它会立即返回，抛出 InterruptedException 异常。 在并发包中，有非常多的这种处理中断的例子，提供两个方法，分别为响应中断和不响应中断，对于不响应中断的方法，记录中断而不是丢失这个信息。如 Condition 中的两个方法就是这样的：12void await() throws InterruptedException;void awaitUninterruptibly(); 通常，如果方法会抛出 InterruptedException 异常，往往方法体的第一句就是：12345public final void await() throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); ......&#125; 熟练使用中断，对于我们写出优雅的代码是有帮助的，也有助于我们分析别人的源码。 总结 Conditon await()调用过程: Conditon signal()调用过程: ReentrantLock、AQS之间的关系:]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[一行一行源码分析清楚 AbstractQueuedSynchronizer (三)]]></title>
    <url>%2F2017%2F07%2F20%2F2017-07-30-abstract-queued-synchronizer-3%2F</url>
    <content type="text"><![CDATA[原文链接:https://javadoop.com/post/AbstractQueuedSynchronizer-2 这篇文章是 AQS 系列的最后一篇，第一篇，我们通过 ReentrantLock 公平锁分析了 AQS 的核心，第二篇的重点是把 Condition 说明白，同时也说清楚了对于线程中断的使用。 这篇，我们的关注点是 AQS 最后的部分，共享模式的使用。有前两篇文章的铺垫，剩下的源码分析将会简单很多。 本文先用 CountDownLatch 将共享模式说清楚，然后顺着把其他 AQS 相关的类 CyclicBarrier、Semaphore 的源码一起过一下。 老规矩：不放过任何一行代码，没有任何糊弄，没有任何瞎说。 CountDownLatchCountDownLatch 这个类是比较典型的 AQS 的共享模式的使用，这是一个高频使用的类。latch 的中文意思是门栓、栅栏，具体怎么解释我就不废话了，大家随意，看两个例子就知道在哪里用、怎么用了。 使用例子我们看下 Doug Lea 在 java doc 中给出的例子，这个例子非常实用，我们经常会写这个代码。 假设我们有 N ( N &gt; 0 ) 个任务，那么我们会用 N 来初始化一个 CountDownLatch，然后将这个 latch 的引用传递到各个线程中，在每个线程完成了任务后，调用 latch.countDown() 代表完成了一个任务。 调用 latch.await() 的方法的线程会阻塞，直到所有的任务完成。 12345678910111213141516171819202122232425262728293031323334class Driver2 &#123; // ... void main() throws InterruptedException &#123; CountDownLatch doneSignal = new CountDownLatch(N); Executor e = Executors.newFixedThreadPool(8); // 创建 N 个任务，提交给线程池来执行 for (int i = 0; i &lt; N; ++i) // create and start threads e.execute(new WorkerRunnable(doneSignal, i)); // 等待所有的任务完成，这个方法才会返回 doneSignal.await(); // wait for all to finish &#125;&#125;class WorkerRunnable implements Runnable &#123; private final CountDownLatch doneSignal; private final int i; WorkerRunnable(CountDownLatch doneSignal, int i) &#123; this.doneSignal = doneSignal; this.i = i; &#125; public void run() &#123; try &#123; doWork(i); // 这个线程的任务完成了，调用 countDown 方法 doneSignal.countDown(); &#125; catch (InterruptedException ex) &#123; &#125; // return; &#125; void doWork() &#123; ...&#125;&#125; 所以说 CountDownLatch 非常实用，我们常常会将一个比较大的任务进行拆分，然后开启多个线程来执行，等所有线程都执行完了以后，再往下执行其他操作。这里例子中，只有 main 线程调用了 await 方法。 我们再来看另一个例子，这个例子很典型，用了两个 CountDownLatch：12345678910111213141516171819202122232425262728293031323334353637383940class Driver &#123; // ... void main() throws InterruptedException &#123; CountDownLatch startSignal = new CountDownLatch(1); CountDownLatch doneSignal = new CountDownLatch(N); for (int i = 0; i &lt; N; ++i) // create and start threads new Thread(new Worker(startSignal, doneSignal)).start(); // 这边插入一些代码，确保上面的每个线程先启动起来，才执行下面的代码。 doSomethingElse(); // don't let run yet // 因为这里 N == 1，所以，只要调用一次，那么所有的 await 方法都可以通过 startSignal.countDown(); // let all threads proceed doSomethingElse(); // 等待所有任务结束 doneSignal.await(); // wait for all to finish &#125;&#125;class Worker implements Runnable &#123; private final CountDownLatch startSignal; private final CountDownLatch doneSignal; Worker(CountDownLatch startSignal, CountDownLatch doneSignal) &#123; this.startSignal = startSignal; this.doneSignal = doneSignal; &#125; public void run() &#123; try &#123; // 为了让所有线程同时开始任务，我们让所有线程先阻塞在这里 // 等大家都准备好了，再打开这个门栓 startSignal.await(); doWork(); doneSignal.countDown(); &#125; catch (InterruptedException ex) &#123; &#125; // return; &#125; void doWork() &#123; ...&#125;&#125; 这个例子中，doneSignal 同第一个例子的使用，我们说说这里的 startSignal。N 个新开启的线程都调用了startSignal.await() 进行阻塞等待，它们阻塞在栅栏上，只有当条件满足的时候（startSignal.countDown()），它们才能同时通过这个栅栏。 !()[https://javadoop.com/blogimages/AbstractQueuedSynchronizer-3/5.png] 如果始终只有一个线程调用 await 方法等待任务完成，那么 CountDownLatch 就会简单很多，所以之后的源码分析读者一定要在脑海中构建出这么一个场景：有 m 个线程是做任务的，有 n 个线程在某个栅栏上等待这 m 个线程做完任务，直到所有 m 个任务完成后，n 个线程同时通过栅栏。 源码分析Talk is cheap, show me the code. 构造方法，需要传入一个不小于 0 的整数：123456789101112public CountDownLatch(int count) &#123; if (count &lt; 0) throw new IllegalArgumentException("count &lt; 0"); this.sync = new Sync(count);&#125;// 老套路了，内部封装一个 Sync 类继承自 AQSprivate static final class Sync extends AbstractQueuedSynchronizer &#123; Sync(int count) &#123; // 这样就 state == count 了 setState(count); &#125; ...&#125; 代码都是套路，先分析套路：AQS 里面的 state 是一个整数值，这边用一个 int count 参数其实初始化就是设置了这个值，所有调用了 await 方法的等待线程会挂起，然后有其他一些线程会做 state = state - 1 操作，当 state 减到 0 的同时，那个线程会负责唤醒调用了 await 方法的所有线程。都是套路啊，只是 Doug Lea 的套路很深，代码很巧妙，不然我们也没有要分析源码的必要。 对于 CountDownLatch，我们仅仅需要关心两个方法，一个是 countDown() 方法，另一个是 await() 方法。countDown() 方法每次调用都会将 state 减 1，直到 state 的值为 0；而 await 是一个阻塞方法，当 state 减为 0 的时候，await 方法才会返回。await 可以被多个线程调用，读者这个时候脑子里要有个图：所有调用了 await 方法的线程阻塞在 AQS 的阻塞队列中，等待条件满足（state == 0），将线程从队列中一个个唤醒过来。 我们用以下程序来分析源码，t1 和 t2 负责调用 countDown() 方法，t3 和 t4 调用 await 方法阻塞：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class CountDownLatchDemo &#123; public static void main(String[] args) &#123; CountDownLatch latch = new CountDownLatch(2); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(5000); &#125; catch (InterruptedException ignore) &#123; &#125; // 休息 5 秒后(模拟线程工作了 5 秒)，调用 countDown() latch.countDown(); &#125; &#125;, "t1"); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(10000); &#125; catch (InterruptedException ignore) &#123; &#125; // 休息 10 秒后(模拟线程工作了 10 秒)，调用 countDown() latch.countDown(); &#125; &#125;, "t2"); t1.start(); t2.start(); Thread t3 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; // 阻塞，等待 state 减为 0 latch.await(); System.out.println("线程 t3 从 await 中返回了"); &#125; catch (InterruptedException e) &#123; System.out.println("线程 t3 await 被中断"); Thread.currentThread().interrupt(); &#125; &#125; &#125;, "t3"); Thread t4 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; // 阻塞，等待 state 减为 0 latch.await(); System.out.println("线程 t4 从 await 中返回了"); &#125; catch (InterruptedException e) &#123; System.out.println("线程 t4 await 被中断"); Thread.currentThread().interrupt(); &#125; &#125; &#125;, "t4"); t3.start(); t4.start(); &#125;&#125; 上述程序，大概在过了 10 秒左右的时候，会输出：1234线程 t3 从 await 中返回了线程 t4 从 await 中返回了// 这两条输出，顺序不是绝对的// 后面的分析，我们假设 t3 先进入阻塞队列 接下来，我们按照流程一步一步走：先 await 等待，然后被唤醒，await 方法返回。 首先，我们来看 await() 方法，它代表线程阻塞，等待 state 的值减为 0。1234567891011121314151617public void await() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1);&#125;public final void acquireSharedInterruptibly(int arg) throws InterruptedException &#123; // 这也是老套路了，我在第二篇的中断那一节说过了 if (Thread.interrupted()) throw new InterruptedException(); // t3 和 t4 调用 await 的时候，state 都大于 0。 // 也就是说，这个 if 返回 true，然后往里看 if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg);&#125;// 只有当 state == 0 的时候，这个方法才会返回 1protected int tryAcquireShared(int acquires) &#123; return (getState() == 0) ? 1 : -1;&#125; 从方法名我们就可以看出，这个方法是获取共享锁，并且此方法是可中断的（中断的时候抛出 InterruptedException 退出这个方法）。12345678910111213141516171819202122232425262728private void doAcquireSharedInterruptibly(int arg) throws InterruptedException &#123; // 1. 入队 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; // 同上，只要 state 不等于 0，那么这个方法返回 -1 int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; setHeadAndPropagate(node, r); p.next = null; // help GC failed = false; return; &#125; &#125; // 2 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 由于 tryAcquireShared 这个方法会返回 -1，所以 if (r &gt;= 0) 这个分支不会进去。到 shouldParkAfterFailedAcquire 的时候，t3 将 head 的 waitStatus 值设置为 -1，如下： 然后进入到 parkAndCheckInterrupt 的时候，t3 挂起。我们再分析 t4 入队，t4 会将前驱节点 t3 所在节点的 waitStatus 设置为 -1，t4 入队后，应该是这样的： 然后，t4 也挂起。接下来，t3 和 t4 就等待唤醒了。 接下来，我们来看唤醒的流程，我们假设用 10 初始化 CountDownLatch。 当然，我们的例子中，其实没有 10 个线程，只有 2 个线程 t1 和 t2，只是为了让图好看些罢了。 我们再一步步看具体的流程。首先，我们看 countDown() 方法:123456789101112131415161718192021222324public void countDown() &#123; sync.releaseShared(1);&#125;public final boolean releaseShared(int arg) &#123; // 只有当 state 减为 0 的时候，tryReleaseShared 才返回 true // 否则只是简单的 state = state - 1 那么 countDown 方法就结束了 if (tryReleaseShared(arg)) &#123; // 唤醒 await 的线程 doReleaseShared(); return true; &#125; return false;&#125;// 这个方法很简单，用自旋的方法实现 state 减 1protected boolean tryReleaseShared(int releases) &#123; for (;;) &#123; int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; &#125;&#125; countDown 方法就是每次调用都将 state 值减 1，如果 state 减到 0 了，那么就调用下面的方法进行唤醒阻塞队列中的线程：1234567891011121314151617181920212223/ 调用这个方法的时候，state == 0// 这个方法先不要看所有的代码，按照思路往下到我写注释的地方，其他的之后还会仔细分析private void doReleaseShared() &#123; for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; // t3 入队的时候，已经将头节点的 waitStatus 设置为 Node.SIGNAL（-1） 了 if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases // 就是这里，唤醒 head 的后继节点，也就是阻塞队列中的第一个节点 // 在这里，也就是唤醒 t3 unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) // todo continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125;&#125; 一旦 t3 被唤醒后，我们继续回到 await 的这段代码，parkAndCheckInterrupt 返回，我们先不考虑中断的情况：1234567891011121314151617181920212223242526private void doAcquireSharedInterruptibly(int arg) throws InterruptedException &#123; final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; setHeadAndPropagate(node, r); // 2. 这里是下一步 p.next = null; // help GC failed = false; return; &#125; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; // 1. 唤醒后这个方法返回 parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 接下来，t3 会进到 setHeadAndPropagate(node, r) 这个方法，先把 head 给占了，然后唤醒队列中其他的线程：1234567891011121314private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; // Record old head for check below setHead(node); // 下面说的是，唤醒当前 node 之后的节点，即 t3 已经醒了，马上唤醒 t4 // 类似的，如果 t4 后面还有 t5，那么 t4 醒了以后，马上将 t5 给唤醒了 if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; if (s == null || s.isShared()) // 又是这个方法，只是现在的 head 已经不是原来的空节点了，是 t3 的节点了 doReleaseShared(); &#125;&#125; 又回到这个方法了，那么接下来，我们好好分析 doReleaseShared 这个方法，我们根据流程，头节点 head 此时是 t3 节点了：12345678910111213141516171819202122232425262728293031// 调用这个方法的时候，state == 0private void doReleaseShared() &#123; for (;;) &#123; Node h = head; // 1. h == null: 说明阻塞队列为空 // 2. h == tail: 说明头结点可能是刚刚初始化的头节点， // 或者是普通线程节点，但是此节点既然是头节点了，那么代表已经被唤醒了，阻塞队列没有其他节点了 // 所以这两种情况不需要进行唤醒后继节点 if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; // t4 将头节点(此时是 t3)的 waitStatus 设置为 Node.SIGNAL（-1） 了 if (ws == Node.SIGNAL) &#123; // 这里 CAS 失败的场景请看下面的解读 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases // 就是这里，唤醒 head 的后继节点，也就是阻塞队列中的第一个节点 // 在这里，也就是唤醒 t4 unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; // 这个 CAS 失败的场景是：执行到这里的时候，刚好有一个节点入队，入队会将这个 ws 设置为 -1 !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; // 如果到这里的时候，前面唤醒的线程已经占领了 head，那么再循环 // 否则，就是 head 没变，那么退出循环， // 退出循环是不是意味着阻塞队列中的其他节点就不唤醒了？当然不是，唤醒的线程之后还是会调用这个方法的 if (h == head) // loop if head changed break; &#125;&#125; 我们分析下最后一个 if 语句，然后才能解释第一个 CAS 为什么可能会失败： h == head：说明头节点还没有被刚刚用 unparkSuccessor 唤醒的线程（这里可以理解为 t4）占有，此时 break 退出循环。 h != head：头节点被刚刚唤醒的线程（这里可以理解为 t4）占有，那么这里重新进入下一轮循环，唤醒下一个节点（这里是 t4 ）。我们知道，等到 t4 被唤醒后，其实是会主动唤醒 t5、t6、t7…，那为什么这里要进行下一个循环来唤醒 t5 呢？我觉得是出于吞吐量的考虑。 满足上面的 2 的场景，那么我们就能知道为什么上面的 CAS 操作 compareAndSetWaitStatus(h, Node.SIGNAL, 0) 会失败了？ 因为当前进行 for 循环的线程到这里的时候，可能刚刚唤醒的线程 t4 也刚刚好到这里了，那么就有可能 CAS 失败了。 for 循环第一轮的时候会唤醒 t4，t4 醒后会将自己设置为头节点，如果在 t4 设置头节点后，for 循环才跑到 if (h == head)，那么此时会返回 false，for 循环会进入下一轮。t4 唤醒后也会进入到这个方法里面，那么 for 循环第二轮和 t4 就有可能在这个 CAS 相遇，那么就只会有一个成功了。 CyclicBarrier字面意思是“可重复使用的栅栏”，CyclicBarrier 相比 CountDownLatch 来说，要简单很多，其源码没有什么高深的地方，它是 ReentrantLock 和 Condition 的组合使用。看如下示意图，CyclicBarrier 和 CountDownLatch 是不是很像，只是 CyclicBarrier 可以有不止一个栅栏，因为它的栅栏（Barrier）可以重复使用（Cyclic）。 首先，CyclicBarrier 的源码实现和 CountDownLatch 大相径庭，CountDownLatch 基于 AQS 的共享模式的使用，而 CyclicBarrier 基于 Condition 来实现。 因为 CyclicBarrier 的源码相对来说简单许多，读者只要熟悉了前面关于 Condition 的分析，那么这里的源码是毫无压力的，就是几个特殊概念罢了。 废话结束，先上基本属性和构造方法，往下拉一点点，和图一起看：1234567891011121314151617181920212223242526272829303132333435public class CyclicBarrier &#123; // 我们说了，CyclicBarrier 是可以重复使用的，我们把每次从开始使用到穿过栅栏当做"一代" private static class Generation &#123; boolean broken = false; &#125; /** The lock for guarding barrier entry */ private final ReentrantLock lock = new ReentrantLock(); // CyclicBarrier 是基于 Condition 的 // Condition 是“条件”的意思，CyclicBarrier 的等待线程通过 barrier 的“条件”是大家都到了栅栏上 private final Condition trip = lock.newCondition(); // 参与的线程数 private final int parties; // 如果设置了这个，代表越过栅栏之前，要执行相应的操作 private final Runnable barrierCommand; // 当前所处的“代” private Generation generation = new Generation(); // 还没有到栅栏的线程数，这个值初始为 parties，然后递减 // 还没有到栅栏的线程数 = parties - 已经到栅栏的数量 private int count; public CyclicBarrier(int parties, Runnable barrierAction) &#123; if (parties &lt;= 0) throw new IllegalArgumentException(); this.parties = parties; this.count = parties; this.barrierCommand = barrierAction; &#125; public CyclicBarrier(int parties) &#123; this(parties, null); &#125; 我用一图来描绘下 CyclicBarrier 里面的一些概念： 看图我们也知道了，CyclicBarrier 的源码最重要的就是 await() 方法了。 首先，先看怎么开启新的一代：123456789// 开启新的一代，当最后一个线程到达栅栏上的时候，调用这个方法来唤醒其他线程，同时初始化“下一代”private void nextGeneration() &#123; // 首先，需要唤醒所有的在栅栏上等待的线程 trip.signalAll(); // 更新 count 的值 count = parties; // 重新生成“新一代” generation = new Generation();&#125; 看看怎么打破一个栅栏：12345678private void breakBarrier() &#123; // 设置状态 broken 为 true generation.broken = true; // 重置 count 为初始值 parties count = parties; // 唤醒所有已经在等待的线程 trip.signalAll();&#125; 这两个方法之后用得到，现在开始分析最重要的等待通过栅栏方法 await 方法：123456789101112131415// 不带超时机制public int await() throws InterruptedException, BrokenBarrierException &#123; try &#123; return dowait(false, 0L); &#125; catch (TimeoutException toe) &#123; throw new Error(toe); // cannot happen &#125;&#125;// 带超时机制，如果超时抛出 TimeoutException 异常public int await(long timeout, TimeUnit unit) throws InterruptedException, BrokenBarrierException, TimeoutException &#123; return dowait(true, unit.toNanos(timeout));&#125; 继续往里看：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException, TimeoutException &#123; final ReentrantLock lock = this.lock; // 先要获取到锁，然后在 finally 中要记得释放锁 // 如果记得 Condition 部分的话，我们知道 condition 的 await 会释放锁，signal 的时候需要重新获取锁 lock.lock(); try &#123; final Generation g = generation; // 检查栅栏是否被打破，如果被打破，抛出 BrokenBarrierException 异常 if (g.broken) throw new BrokenBarrierException(); // 检查中断状态，如果中断了，抛出 InterruptedException 异常 if (Thread.interrupted()) &#123; breakBarrier(); throw new InterruptedException(); &#125; // index 是这个 await 方法的返回值 // 注意到这里，这个是从 count 递减后得到的值 int index = --count; // 如果等于 0，说明所有的线程都到栅栏上了，准备通过 if (index == 0) &#123; // tripped boolean ranAction = false; try &#123; // 如果在初始化的时候，指定了通过栅栏前需要执行的操作，在这里会得到执行 final Runnable command = barrierCommand; if (command != null) command.run(); // 如果 ranAction 为 true，说明执行 command.run() 的时候，没有发生异常退出的情况 ranAction = true; // 唤醒等待的线程，然后开启新的一代 nextGeneration(); return 0; &#125; finally &#123; if (!ranAction) // 进到这里，说明执行指定操作的时候，发生了异常，那么需要打破栅栏 // 之前我们说了，打破栅栏意味着唤醒所有等待的线程，设置 broken 为 true，重置 count 为 parties breakBarrier(); &#125; &#125; // loop until tripped, broken, interrupted, or timed out // 如果是最后一个线程调用 await，那么上面就返回了 // 下面的操作是给那些不是最后一个到达栅栏的线程执行的 for (;;) &#123; try &#123; // 如果带有超时机制，调用带超时的 Condition 的 await 方法等待，直到最后一个线程调用 await if (!timed) trip.await(); else if (nanos &gt; 0L) nanos = trip.awaitNanos(nanos); &#125; catch (InterruptedException ie) &#123; // 如果到这里，说明等待的线程在 await（是 Condition 的 await）的时候被中断 if (g == generation &amp;&amp; ! g.broken) &#123; // 打破栅栏 breakBarrier(); // 打破栅栏后，重新抛出这个 InterruptedException 异常给外层调用的方法 throw ie; &#125; else &#123; // 到这里，说明 g != generation, 说明新的一代已经产生，即最后一个线程 await 执行完成， // 那么此时没有必要再抛出 InterruptedException 异常，记录下来这个中断信息即可 // 或者是栅栏已经被打破了，那么也不应该抛出 InterruptedException 异常， // 而是之后抛出 BrokenBarrierException 异常 Thread.currentThread().interrupt(); &#125; &#125; // 唤醒后，检查栅栏是否是“破的” if (g.broken) throw new BrokenBarrierException(); // 这个 for 循环除了异常，就是要从这里退出了 // 我们要清楚，最后一个线程在执行完指定任务(如果有的话)，会调用 nextGeneration 来开启一个新的代 // 然后释放掉锁，其他线程从 Condition 的 await 方法中得到锁并返回，然后到这里的时候，其实就会满足 g != generation 的 // 那什么时候不满足呢？barrierCommand 执行过程中抛出了异常，那么会执行打破栅栏操作， // 设置 broken 为true，然后唤醒这些线程。这些线程会从上面的 if (g.broken) 这个分支抛 BrokenBarrierException 异常返回 // 当然，还有最后一种可能，那就是 await 超时，此种情况不会从上面的 if 分支异常返回，也不会从这里返回，会执行后面的代码 if (g != generation) return index; // 如果醒来发现超时了，打破栅栏，抛出异常 if (timed &amp;&amp; nanos &lt;= 0L) &#123; breakBarrier(); throw new TimeoutException(); &#125; &#125; &#125; finally &#123; lock.unlock(); &#125;&#125; 好了，我想我应该讲清楚了吧，我好像几乎没有漏掉任何一行代码吧？ 下面开始收尾工作。 首先，我们看看怎么得到有多少个线程到了栅栏上，处于等待状态：123456789public int getNumberWaiting() &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; return parties - count; &#125; finally &#123; lock.unlock(); &#125;&#125; 判断一个栅栏是否被打破了，这个很简单，直接看 broken 的值即可：123456789public boolean isBroken() &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; return generation.broken; &#125; finally &#123; lock.unlock(); &#125;&#125; 前面我们在说 await 的时候也几乎说清楚了，什么时候栅栏会被打破，总结如下： ** 中断，我们说了，如果某个等待的线程发生了中断，那么会打破栅栏，同时抛出 InterruptedException 异常； 超时，打破栅栏，同时抛出 TimeoutException 异常； 指定执行的操作抛出了异常，这个我们前面也说过。 最后，我们来看看怎么重置一个栅栏：12345678910public void reset() &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; breakBarrier(); // break the current generation nextGeneration(); // start a new generation &#125; finally &#123; lock.unlock(); &#125;&#125; 我们设想一下，如果初始化时，指定了线程 parties = 4，前面有 3 个线程调用了 await 等待，在第 4 个线程调用 await 之前，我们调用 reset 方法，那么会发生什么？ 首先，打破栅栏，那意味着所有等待的线程（3个等待的线程）会唤醒，await 方法会通过抛出 BrokenBarrierException 异常返回。然后开启新的一代，重置了 count 和 generation，相当于一切归零了。 怎么样，CyclicBarrier 源码很简单吧。 Semaphore有了 CountDownLatch 的基础后，分析 Semaphore 会简单很多。Semaphore 是什么呢？它类似一个资源池（读者可以类比线程池），每个线程需要调用 acquire() 方法获取资源，然后才能执行，执行完后，需要 release 资源，让给其他的线程用。 大概大家也可以猜到，Semaphore 其实也是 AQS 中共享锁的使用，因为每个线程共享一个池嘛。 套路解读：创建 Semaphore 实例的时候，需要一个参数permits，这个基本上可以确定是设置给 AQS 的 state 的，然后每个线程调用 acquire 的时候，执行 state = state - 1，release 的时候执行 state = state + 1，当然，acquire 的时候，如果 state = 0，说明没有资源了，需要等待其他线程 release。 构造方法：1234567public Semaphore(int permits) &#123; sync = new NonfairSync(permits);&#125;public Semaphore(int permits, boolean fair) &#123; sync = fair ? new FairSync(permits) : new NonfairSync(permits);&#125; 这里和 ReentrantLock 类似，用了公平策略和非公平策略。 看 acquire 方法：1234567891011121314public void acquire() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1);&#125;public void acquireUninterruptibly() &#123; sync.acquireShared(1);&#125;public void acquire(int permits) throws InterruptedException &#123; if (permits &lt; 0) throw new IllegalArgumentException(); sync.acquireSharedInterruptibly(permits);&#125;public void acquireUninterruptibly(int permits) &#123; if (permits &lt; 0) throw new IllegalArgumentException(); sync.acquireShared(permits);&#125; 这几个方法也是老套路了，大家基本都懂了吧，这边多了两个可以传参的 acquire 方法，不过大家也都懂的吧，如果我们需要一次获取超过一个的资源，会用得着这个的。 我们接下来看不抛出 InterruptedException 异常的acquireUninterruptibly()方法吧：1234567public void acquireUninterruptibly() &#123; sync.acquireShared(1);&#125;public final void acquireShared(int arg) &#123; if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);&#125; 前面说了，Semaphore 分公平策略和非公平策略，我们对比一下两个tryAcquireShared方法：1234567891011121314151617181920212223242526// 公平策略：protected int tryAcquireShared(int acquires) &#123; for (;;) &#123; // 区别就在于是不是会先判断是否有线程在排队，然后才进行 CAS 减操作 if (hasQueuedPredecessors()) return -1; int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; &#125;&#125;// 非公平策略：protected int tryAcquireShared(int acquires) &#123; return nonfairTryAcquireShared(acquires);&#125;final int nonfairTryAcquireShared(int acquires) &#123; for (;;) &#123; int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; &#125;&#125; 也是老套路了，所以从源码分析角度的话，我们其实不太需要关心是不是公平策略还是非公平策略，它们的区别往往就那么一两行。 我们再回到 acquireShared 方法，1234public final void acquireShared(int arg) &#123; if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);&#125; 由于 tryAcquireShared(arg) 返回小于 0 的时候，说明 state 已经小于 0 了（没资源了），此时 acquire 不能立马拿到资源，需要进入到阻塞队列等待，虽然贴了很多代码，不在乎多这点了：123456789101112131415161718192021222324252627private void doAcquireShared(int arg) &#123; final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; &#125; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 这个方法我就不介绍了，线程挂起后等待有资源被 release 出来。接下来，我们就要看 release 的方法了：1234567891011121314151617181920212223// 任务介绍，释放一个资源public void release() &#123; sync.releaseShared(1);&#125;public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125;protected final boolean tryReleaseShared(int releases) &#123; for (;;) &#123; int current = getState(); int next = current + releases; // 溢出，当然，我们一般也不会用这么大的数 if (next &lt; current) // overflow throw new Error("Maximum permit count exceeded"); if (compareAndSetState(current, next)) return true; &#125;&#125; tryReleaseShared 方法总是会返回 true，然后是 doReleaseShared，这个也是我们熟悉的方法了，我就贴下代码，不分析了，这个方法用于唤醒所有的等待线程： Semphore 的源码确实很简单，基本上都是分析过的老代码的组合使用了。 总结写到这里，终于把 AbstractQueuedSynchronizer 基本上说完了，对于 Java 并发，Doug Lea 真的是神一样的存在。日后我们还会接触到很多 Doug Lea 的代码，希望我们大家都可以朝着大神的方向不断打磨自己的技术，少一些高大上的架构，多一些实实在在的优秀代码吧。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[websocket学习整理(二)]]></title>
    <url>%2F2017%2F07%2F09%2F2017-07-10-websocket2%2F</url>
    <content type="text"><![CDATA[原文链接：http://www.ruanyifeng.com/blog/archives.html WebSocket 是一种网络通信协议，很多高级功能都需要它。本文介绍 WebSocket 协议的使用方法。 一、为什么需要 WebSocket？初次接触 WebSocket 的人，都会问同样的问题：我们已经有了 HTTP 协议，为什么还需要另一个协议？它能带来什么好处？ 答案很简单，因为 HTTP 协议有一个缺陷：通信只能由客户端发起。 举例来说，我们想了解今天的天气，只能是客户端向服务器发出请求，服务器返回查询结果。HTTP 协议做不到服务器主动向客户端推送信息。 这种单向请求的特点，注定了如果服务器有连续的状态变化，客户端要获知就非常麻烦。我们只能使用”轮询”：每隔一段时候，就发出一个询问，了解服务器有没有新的信息。最典型的场景就是聊天室。轮询的效率低，非常浪费资源（因为必须不停连接，或者 HTTP 连接始终打开）。因此，工程师们一直在思考，有没有更好的方法。WebSocket 就是这样发明的。 二、简介WebSocket 协议在2008年诞生，2011年成为国际标准。所有浏览器都已经支持了。它的最大特点就是，服务器可以主动向客户端推送信息，客户端也可以主动向服务器发送信息，是真正的双向平等对话，属于服务器推送技术的一种 其他特点包括： （1）建立在 TCP 协议之上，服务器端的实现比较容易。 （2）与 HTTP 协议有着良好的兼容性。默认端口也是80和443，并且握手阶段采用 HTTP 协议，因此握手时不容易屏蔽，能通过各种 HTTP 代理服务器。 （3）数据格式比较轻量，性能开销小，通信高效。 （4）可以发送文本，也可以发送二进制数据。 （5）没有同源限制，客户端可以与任意服务器通信。 （6）协议标识符是ws（如果加密，则为wss），服务器网址就是 URL。1ws://example.com:80/some/path 三、客户端的简单示例WebSocket 的用法相当简单。下面是一个网页脚本的例子（点击这里看运行结果），基本上一眼就能明白。123456789101112131415var ws = new WebSocket(&quot;wss://echo.websocket.org&quot;);ws.onopen = function(evt) &#123; console.log(&quot;Connection open ...&quot;); ws.send(&quot;Hello WebSockets!&quot;);&#125;;ws.onmessage = function(evt) &#123; console.log( &quot;Received Message: &quot; + evt.data); ws.close();&#125;;ws.onclose = function(evt) &#123; console.log(&quot;Connection closed.&quot;);&#125;; 四、客户端的 APIWebSocket 客户端的 API 如下。 4.1 WebSocket 构造函数WebSocket 对象作为一个构造函数，用于新建 WebSocket 实例。1var ws = new WebSocket(&apos;ws://localhost:8080&apos;); 执行上面语句之后，客户端就会与服务器进行连接。实例对象的所有属性和方法清单，参见这里。 4.2 webSocket.readyStatereadyState属性返回实例对象的当前状态，共有四种。1234CONNECTING：值为0，表示正在连接。OPEN：值为1，表示连接成功，可以通信了。CLOSING：值为2，表示连接正在关闭。CLOSED：值为3，表示连接已经关闭，或者打开连接失败。 下面是一个示例。1234567891011121314151617switch (ws.readyState) &#123; case WebSocket.CONNECTING: // do something break; case WebSocket.OPEN: // do something break; case WebSocket.CLOSING: // do something break; case WebSocket.CLOSED: // do something break; default: // this never happens break;&#125; 4.3 webSocket.onopen实例对象的onopen属性，用于指定连接成功后的回调函数。123ws.onopen = function () &#123; ws.send(&apos;Hello Server!&apos;);&#125; 如果要指定多个回调函数，可以使用addEventListener方法。123ws.addEventListener(&apos;open&apos;, function (event) &#123; ws.send(&apos;Hello Server!&apos;);&#125;); 4.4 webSocket.onclose实例对象的onclose属性，用于指定连接关闭后的回调函数。 12345678910111213ws.onclose = function(event) &#123; var code = event.code; var reason = event.reason; var wasClean = event.wasClean; // handle close event&#125;;ws.addEventListener(&quot;close&quot;, function(event) &#123; var code = event.code; var reason = event.reason; var wasClean = event.wasClean; // handle close event&#125;); 4.5 webSocket.onmessage实例对象的onmessage属性，用于指定收到服务器数据后的回调函数。123456789ws.onmessage = function(event) &#123; var data = event.data; // 处理数据&#125;;ws.addEventListener(&quot;message&quot;, function(event) &#123; var data = event.data; // 处理数据&#125;); 注意，服务器数据可能是文本，也可能是二进制数据（blob对象或Arraybuffer对象）。12345678910ws.onmessage = function(event)&#123; if(typeof event.data === String) &#123; console.log(&quot;Received data string&quot;); &#125; if(event.data instanceof ArrayBuffer)&#123; var buffer = event.data; console.log(&quot;Received arraybuffer&quot;); &#125;&#125; 除了动态判断收到的数据类型，也可以使用binaryType属性，显式指定收到的二进制数据类型。1234567891011// 收到的是 blob 数据ws.binaryType = &quot;blob&quot;;ws.onmessage = function(e) &#123; console.log(e.data.size);&#125;;// 收到的是 ArrayBuffer 数据ws.binaryType = &quot;arraybuffer&quot;;ws.onmessage = function(e) &#123; console.log(e.data.byteLength);&#125;; 4.6 webSocket.send()实例对象的send()方法用于向服务器发送数据。发送文本的例子。1ws.send(&apos;your message&apos;); 发送 Blob 对象的例子。1234var file = document .querySelector(&apos;input[type=&quot;file&quot;]&apos;) .files[0];ws.send(file); 发送 ArrayBuffer 对象的例子。1234567// Sending canvas ImageData as ArrayBuffervar img = canvas_context.getImageData(0, 0, 400, 320);var binary = new Uint8Array(img.data.length);for (var i = 0; i &lt; img.data.length; i++) &#123; binary[i] = img.data[i];&#125;ws.send(binary.buffer); 4.7 webSocket.bufferedAmount实例对象的bufferedAmount属性，表示还有多少字节的二进制数据没有发送出去。它可以用来判断发送是否结束。12345678var data = new ArrayBuffer(10000000);socket.send(data);if (socket.bufferedAmount === 0) &#123; // 发送完毕&#125; else &#123; // 发送还没结束&#125; 4.8 webSocket.onerror实例对象的onerror属性，用于指定报错时的回调函数。1234567socket.onerror = function(event) &#123; // handle error event&#125;;socket.addEventListener(&quot;error&quot;, function(event) &#123; // handle error event&#125;); 五、服务端的实现WebSocket 服务器的实现，可以查看维基百科的列表。 常用的 Node 实现有以下三种。 µWebSockets Socket.IO WebSocket-Node 具体的用法请查看它们的文档，这里不详细介绍了。 六、WebSocketd下面，我要推荐一款非常特别的 WebSocket 服务器：Websocketd。它的最大特点，就是后台脚本不限语言，标准输入（stdin）就是 WebSocket 的输入，标准输出（stdout）就是 WebSocket 的输出。1234567echo 1sleep 1echo 2sleep 1echo 3 命令行下运行这个脚本，会输出1、2、3，每个值之间间隔1秒。1234$ bash ./counter.sh123 现在，启动websocketd，指定这个脚本作为服务。1$ websocketd --port=8080 bash ./counter.sh 上面的命令会启动一个 WebSocket 服务器，端口是8080。每当客户端连接这个服务器，就会执行counter.sh脚本，并将它的输出推送给客户端。12345var ws = new WebSocket(&apos;ws://localhost:8080/&apos;);ws.onmessage = function(event) &#123; console.log(event.data);&#125;; 上面是客户端的 JavaScript 代码，运行之后会在控制台依次输出1、2、3。 有了它，就可以很方便地将命令行的输出，发给浏览器。1$ websocketd --port=8080 ls 上面的命令会执行ls命令，从而将当前目录的内容，发给浏览器。使用这种方式实时监控服务器，简直是轻而易举（代码）. 更多的用法可以参考官方示例。 Bash 脚本读取客户端输入的例子 五行代码实现一个最简单的聊天服务器 websocketd 的实质，就是命令行的 WebSocket 代理。只要命令行可以执行的程序，都可以通过它与浏览器进行 WebSocket 通信。下面是一个 Node 实现的回声服务greeter.js。12345678process.stdin.setEncoding(&apos;utf8&apos;);process.stdin.on(&apos;readable&apos;, function() &#123; var chunk = process.stdin.read(); if (chunk !== null) &#123; process.stdout.write(&apos;data: &apos; + chunk); &#125;&#125;); 启动这个脚本的命令如下。1$ websocketd --port=8080 node ./greeter.js 官方仓库还有其他各种语言的例子。]]></content>
      <categories>
        <category>网络通信</category>
      </categories>
      <tags>
        <tag>websocket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[websocket学习整理(一)]]></title>
    <url>%2F2017%2F07%2F09%2F2017-07-09-websocket%2F</url>
    <content type="text"><![CDATA[导语 尽管现在对于客户端的实时通讯已经涌现出很多新的解决方案，但websocket在其中依然占据着重要地位。一直以来，对websocket技术处于一知半解的状态，为此搜集整理了网上的相关内容，对webscoket技术作一个完整梳理记录。本篇文章以websocket的原理和落地为核心，来叙述websocket的使用，以及相关应用场景。 websocket是什么Websocket是html5提出的一个协议规范，参考rfc6455。 websocket约定了一个通信的规范，通过一个握手的机制，客户端（浏览器）和服务器（webserver）之间能建立一个类似tcp的连接，从而方便c－s之间的通信。在websocket出现之前，web交互一般是基于http协议的短连接或者长连接。 WebSocket是为解决客户端与服务端实时通信而产生的技术。websocket协议本质上是一个基于tcp的协议，是先通过HTTP/HTTPS协议发起一条特殊的http请求进行握手后创建一个用于交换数据的TCP连接，此后服务端与客户端通过此TCP连接进行实时通信(此时不再需要原HTTP协议的参与了)。 websocket的优点以前web server实现推送技术或者即时通讯，用的都是轮询（polling），在特点的时间间隔（比如1秒钟）由浏览器自动发出请求，将服务器的消息主动的拉回来，在这种情况下，我们需要不断的向服务器发送请求，然而HTTP request 的header是非常长的，里面包含的数据可能只是一个很小的值，这样会占用很多的带宽和服务器资源。 而最比较新的技术去做轮询的效果是Comet – 用了AJAX。但这种技术虽然可达到全双工通信，但依然需要发出请求(reuqest)。 WebSocket API最伟大之处在于服务器和客户端可以在给定的时间范围内的任意时刻，相互推送信息。 浏览器和服务器只需要要做一个握手的动作，在建立连接之后，服务器可以主动传送数据给客户端，客户端也可以随时向服务器发送数据。 此外，服务器与客户端之间交换的标头信息很小。 WebSocket并不限于以Ajax(或XHR)方式通信，因为Ajax技术需要客户端发起请求，而WebSocket服务器和客户端可以彼此相互推送信息；因此从服务器角度来说，websocket有以下好处： 节省每次请求的header,http的header一般有几十字节 Server Push,服务器可以主动传送数据给客户端 为什么要使用websocket那么了解http与websocket之间的不同以后，我们为什么要使用websocket呢？ 他的应用场景是什么呢？ 我找到了一个比较符合websocket使用场景的描述 “The best fit for WebSocket is in web applications where the client and server need to exchange events at high frequency and with low latency.” 翻译: 在客户端与服务器端交互的web应用中，websocket最适合在高频率低延迟的场景下，进行事件的交换和处理(此段来源于spring websocket的官方文档) 了解以上知识后，我举出几个比较常见的场景: 游戏中的数据传输 股票K线图数据 客服系统根据如上所述，各个系统都来使用websocket不是更好吗？ 其实并不是，websocket建立连接之后，后边交互都由tcp协议进行交互，故开发的复杂度会较高。当然websocket通讯，本身要考虑的事情要比HTTP协议的通讯考虑的更多. 所以如果不是有特殊要求(即 应用不是”高频率低延迟”的要求),需要优先考虑HTTP协议是否可以满足。 比如新闻系统，新闻的数据晚上10分钟-30分钟，是可以接受的，那么就可以采用HTTP的方式进行轮询(polling)操作调用REST接口。 当然有时我们建立了websocket通讯，并且希望通过HTTP提供的REST接口推送给某客户端，此时需要考虑REST接口接受数据传送给websocket中，进行广播式的通讯方式。 至此，我已经讲述了三种交互方式的使用场景: websocket独立使用场景 HTTP独立使用场景 HTTP中转websocket使用场景 发展历史http协议1996年IETF HTTP工作组发布了HTTP协议的1.0版本，到现在普遍使用的版本1.1，HTTP协议经历了17年的发展。 这种分布式、无状态、基于TCP的请求/响应式、在互联网盛行的今天得到广泛应用的协议。互联网从兴起到现在，经历了门户网站盛行的web1.0时代，而后随着ajax技术的出现，发展为web应用盛行的web2.0时代，如今又朝着web3.0的方向迈进。反观http协议，从版本1.0发展到1.1，除了默认长连接之外就是缓存处理、带宽优化和安全性等方面的不痛不痒的改进。它一直保留着无状态、请求/响应模式，似乎从来没意识到这应该有所改变。 通过脚本发送的http请求（Ajax）传统的web应用要想与服务器交互，必须提交一个表单（form），服务器接收并处理传来的表单，然后返回全新的页面，因为前后两个页面的数据大部分都是相同的，这个过程传输了很多冗余的数据、浪费了带宽。于是Ajax技术便应运而生。 Ajax是Asynchronous JavaScript and XML的简称，由Jesse James Garrett 首先提出。这种技术开创性地允许浏览器脚本（JS）发送http请求。Outlook Web Access小组于98年使用，并很快成为IE4.0的一部分，但是这个技术一直很小众，直到2005年初，google在他的goole groups、gmail等交互式应用中广泛使用此种技术，才使得Ajax迅速被大家所接受。 Ajax的出现使客户端与服务器端传输数据少了很多，也快了很多，也满足了以丰富用户体验为特点的web2.0时代 初期发展的需要，但是慢慢地也暴露了他的弊端。比如无法满足即时通信等富交互式应用的实时更新数据的要求。这种浏览器端的小技术毕竟还是基于http协议，http协议要求的请求/响应的模式也是无法改变的，除非http协议本身有所改变。 一种hack技术（Comet）以即时通信为代表的web应用程序对数据的低延时要求，传统的基于轮询的方式已经无法满足，而且也会带来不好的用户体验。于是一种基于http长连接的“服务器推”技术便被hack出来。这种技术被命名为Comet，这个术语由Dojo Toolkit 的项目主管Alex Russell在博文Comet: Low Latency Data for the Browser首次提出，并沿用下来。 其实，服务器推很早就存在了，在经典的client/server模型中有广泛使用，只是浏览器太懒了，并没有对这种技术提供很好的支持。但是Ajax的出现使这种技术在浏览器上实现成为可能， google的gmail和gtalk的整合首先使用了这种技术。随着一些关键问题的解决（比如IE的加载显示问题），很快这种技术得到了认可，目前已经有很多成熟的开源Comet框架。 以下是典型的Ajax和Comet数据传输方式的对比，区别简单明了。典型的Ajax通信方式也是http协议的经典使用方式，要想取得数据，必须首先发送请求。在Low Latency要求比较高的web应用中，只能增加服务器请求的频率。Comet则不同，客户端与服务器端保持一个长连接，只有客户端需要的数据更新时，服务器才主动将数据推送给客户端。 Comet的实现主要有两种方式： 基于Ajax的长轮询（long-polling）方式 基于 Iframe 及 htmlfile 的流（http streaming）方式Iframe是html标记，这个标记的src属性会保持对指定服务器的长连接请求，服务器端则可以不停地返回数据，相对于第一种方式，这种方式跟传统的服务器推则更接近。在第一种方式中，浏览器在收到数据后会直接调用JS回调函数，但是这种方式该如何响应数据呢？可以通过在返回数据中嵌入JS脚本的方式，服务器端将返回的数据作为回调函数的参数，浏览器在收到数据后就会执行这段JS脚本。 Websocket—未来的解决方案如果说Ajax的出现是互联网发展的必然，那么Comet技术的出现则更多透露出一种无奈，仅仅作为一种hack技术，因为没有更好的解决方案。Comet解决的问题应该由谁来解决才是合理的呢？浏览器，还是http标准？主角应该是谁呢？本质上讲，这涉及到数据传输方式，http协议应首当其冲，是时候改变一下这个懒惰的协议的请求/响应模式了。 W3C给出了答案，在新一代html标准html5中提供了一种浏览器和服务器间进行全双工通讯的网络技术Websocket。从Websocket草案得知，Websocket是一个全新的、独立的协议，基于TCP协议，与http协议兼容、却不会融入http协议，仅仅作为html5的一部分。于是乎脚本又被赋予了另一种能力：发起websocket请求。这种方式我们应该很熟悉，因为Ajax就是这么做的，所不同的是，Ajax发起的是http请求而已。 websocket逻辑与http协议不同的请求/响应模式不同，Websocket在建立连接之前有一个Handshake（Opening Handshake）过程，在关闭连接前也有一个Handshake（Closing Handshake）过程，建立连接之后，双方即可双向通信。在websocket协议发展过程中前前后后就出现了多个版本的握手协议，这里分情况说明一下： 基于flash的握手协议 使用场景是IE的多数版本，因为IE的多数版本不都不支持WebSocket协议，以及FF、CHROME等浏览器的低版本，还没有原生的支持WebSocket。此处，server唯一要做的，就是准备一个WebSocket-Location域给client，没有加密，可靠性很差。 客户端请求： 12345GET /ls HTTP/1.1Upgrade: WebSocketConnection: UpgradeHost: www.qixing318.comOrigin: http://www.qixing318.com 服务器返回： 12345HTTP/1.1 101 Web Socket Protocol HandshakeUpgrade: WebSocketConnection: UpgradeWebSocket-Origin: http://www.qixing318.comWebSocket-Location: ws://www.qixing318.com/ls 基于md5加密方式的握手协议 客户端请求： 12345678 GET /demo HTTP/1.1Host: example.comConnection: UpgradeSec-WebSocket-Key2:Upgrade: WebSocketSec-WebSocket-Key1:Origin: http://www.qixing318.com[8-byte security key] 服务端返回：123456HTTP/1.1 101 WebSocket Protocol HandshakeUpgrade: WebSocketConnection: UpgradeWebSocket-Origin: http://www.qixing318.comWebSocket-Location: ws://example.com/demo[16-byte hash response] 其中 Sec-WebSocket-Key1，Sec-WebSocket-Key2 和 [8-byte security key] 这几个头信息是web server用来生成应答信息的来源，依据 draft-hixie-thewebsocketprotocol-76 草案的定义。web server基于以下的算法来产生正确的应答信息：1234561. 逐个字符读取 Sec-WebSocket-Key1 头信息中的值，将数值型字符连接到一起放到一个临时字符串里，同时统计所有空格的数量；2. 将在第（1）步里生成的数字字符串转换成一个整型数字，然后除以第（1）步里统计出来的空格数量，将得到的浮点数转换成整数型；3. 将第（2）步里生成的整型值转换为符合网络传输的网络字节数组；4. 对 Sec-WebSocket-Key2 头信息同样进行第（1）到第（3）步的操作，得到另外一个网络字节数组；5. 将 [8-byte security key] 和在第（3）、（4）步里生成的网络字节数组合并成一个16字节的数组；6. 对第（5）步生成的字节数组使用MD5算法生成一个哈希值，这个哈希值就作为安全密钥返回给客户端，以表明服务器端获取了客户端的请求，同意创建websocket连接 基于sha加密方式的握手协议也是目前见的最多的一种方式，这里的版本号目前是需要13以上的版本。客户端请求：1234567GET /ls HTTP/1.1Upgrade: websocketConnection: UpgradeHost: www.qixing318.comSec-WebSocket-Origin: http://www.qixing318.comSec-WebSocket-Key: 2SCVXUeP9cTjV+0mWB8J6A==Sec-WebSocket-Version: 13 服务器返回：1234HTTP/1.1 101 Switching ProtocolsUpgrade: websocketConnection: UpgradeSec-WebSocket-Accept: mLDKNeBNWz6T9SxU+o0Fy/HgeSw= 其中 server就是把客户端上报的key拼上一段GUID（ “258EAFA5-E914-47DA-95CA-C5AB0DC85B11″），拿这个字符串做SHA-1 hash计算，然后再把得到的结果通过base64加密，最后再返回给客户端。 Opening Handshake：客户端发起连接Handshake请求12345678GET /chat HTTP/1.1Host: server.example.comUpgrade: websocketConnection: UpgradeSec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ==Origin: http://example.comSec-WebSocket-Protocol: chat, superchatSec-WebSocket-Version: 13 服务器端响应：12345HTTP/1.1 101 Switching ProtocolsUpgrade: websocketConnection: UpgradeSec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo=Sec-WebSocket-Protocol: chat Upgrade：WebSocket表示这是一个特殊的 HTTP 请求，请求的目的就是要将客户端和服务器端的通讯协议从 HTTP 协议升级到 WebSocket 协议。 Sec-WebSocket-Key是一段浏览器base64加密的密钥，server端收到后需要提取Sec-WebSocket-Key 信息，然后加密。 Sec-WebSocket-Accept服务器端在接收到的Sec-WebSocket-Key密钥后追加一段神奇字符串“258EAFA5-E914-47DA-95CA-C5AB0DC85B11”，并将结果进行sha-1哈希，然后再进行base64加密返回给客户端（就是Sec-WebSocket-Key）。 比如： 1234567function encry($req)&#123; $key = $this-&gt;getKey($req); $mask = "258EAFA5-E914-47DA-95CA-C5AB0DC85B11"; # 将 SHA-1 加密后的字符串再进行一次 base64 加密 return base64_encode(sha1($key . '258EAFA5-E914-47DA-95CA-C5AB0DC85B11', true));&#125; Sec-WebSocket-Protocol表示客户端请求提供的可供选择的子协议，及服务器端选中的支持的子协议，“Origin”服务器端用于区分未授权的websocket浏览器 Sec-WebSocket-Version: 13客户端在握手时的请求中携带，这样的版本标识，表示这个是一个升级版本，现在的浏览器都是使用的这个版本。 HTTP/1.1 101 Switching Protocols101为服务器返回的状态码，所有非101的状态码都表示handshake并未完成。 Data FramingWebsocket协议通过序列化的数据帧传输数据。数据封包协议中定义了opcode、payload length、Payload data等字段。其中要求： 客户端向服务器传输的数据帧必须进行掩码处理：服务器若接收到未经过掩码处理的数据帧，则必须主动关闭连接。 服务器向客户端传输的数据帧一定不能进行掩码处理。客户端若接收到经过掩码处理的数据帧，则必须主动关闭连接。针对上情况，发现错误的一方可向对方发送close帧（状态码是1002，表示协议错误），以关闭连接。具体数据帧格式如下图所示： Closing Handshake相对于Opening Handshake，Closing Handshake则简单得多，主动关闭的一方向另一方发送一个关闭类型的数据包，对方收到此数据包之后，再回复一个相同类型的数据包，关闭完成。 关闭类型数据包遵守封包协议，Opcode为0x8，Payload data可以用于携带关闭原因或消息 websocket的事件响应以上的Opening Handshake、Data Framing、Closing Handshake三个步骤其实分别对应了websocket的三个事件： onopen 当接口打开时响应 onmessage 当收到信息时响应 onclose 当接口关闭时响应任何程序语言的websocket api都至少要提供上面三个事件的api接口， 有的可能还提供的有onerror事件的处理机制。 websocket 在任何时候都会处于下面4种状态中的其中一种： CONNECTING (0)：表示还没建立连接； OPEN (1)： 已经建立连接，可以进行通讯； CLOSING (2)：通过关闭握手，正在关闭连接； CLOSED (3)：连接已经关闭或无法打开； 如何使用websocket客户端在支持WebSocket的浏览器中，在创建socket之后。可以通过onopen，onmessage，onclose即onerror四个事件实现对socket进行响应一个简单是示例：123456789var ws = new WebSocket(“ws://localhost:8080”);ws.onopen = function()&#123; console.log(“open”); ws.send(“hello”);&#125;;ws.onmessage = function(evt) &#123; console.log(evt.data); &#125;;ws.onclose = function(evt) &#123; console.log(“WebSocketClosed!”); &#125;;ws.onerror = function(evt) &#123; console.log(“WebSocketError!”); &#125;; 首先申请一个WebSocket对象，参数是需要连接的服务器端的地址，同http协议使用http://开头一样，WebSocket协议的URL使用ws://开头，另外安全的WebSocket协议使用wss://开头。 client先发起握手请求：12345678910111213GET /echobot HTTP/1.1Host: 192.168.14.215:9000Connection: UpgradePragma: no-cacheCache-Control: no-cacheUpgrade: websocketOrigin: http://192.168.14.215Sec-WebSocket-Version: 13User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36Accept-Encoding: gzip, deflate, sdchAccept-Language: zh-CN,zh;q=0.8Sec-WebSocket-Key: mh3xLXeRuIWNPwq7ATG9jA==Sec-WebSocket-Extensions: permessage-deflate; client_max_window_bits 服务端响应：1234HTTP/1.1 101 Switching ProtocolsUpgrade: websocketConnection: UpgradeSec-WebSocket-Accept: SIEylb7zRYJAEgiqJXaOW3V+ZWQ= 交互数据：12ws.send(“hello”); # 用于将消息发送到服务端ws.recv($buffer); # 用于接收服务端的消息 相关技术概念websocketwebsocket为一次HTTP握手后，后续通讯为tcp协议的通讯方式。 当然，和HTTP一样，websocket也有一些约定的通讯方式，http通讯方式为http开头的方式,e.g. http://xxx.com/path ,websocket通讯方式则为ws开头的方式,e.g. ws://xxx.com/path SSL: HTTP: https://xxx.com/path WEBSOCKET: wss://xxx.com/path SockJS正如我们所知,websocket协议虽然已经被制定，当时还有很多版本的浏览器或浏览器厂商还没有支持的很好。 所以,SockJS,可以理解为是websocket的一个备选方案。 那它如何规定备选方案的呢？ 它大概支持这样几个方案: Websockets Streaming Polling当然，开启并使用SockJS后，它会优先选用websocket协议作为传输协议，如果浏览器不支持websocket协议，则会在其他方案中，选择一个较好的协议进行通讯。 看一下目前浏览器的支持情况: 所以，如果使用SockJS进行通讯，它将在使用上保持一致，底层由它自己去选择相应的协议。可以认为SockJS是websocket通讯层上的上层协议。底层对于开发者来说是透明的。 STOMPSTOMP 中文为: 面向消息的简单文本协议 websocket定义了两种传输信息类型: 文本信息 和 二进制信息 ( text and binary ). 类型虽然被确定，但是他们的传输体是没有规定的。 当然你可以自己来写传输体，来规定传输内容。(当然，这样的复杂度是很高的) 所以,需要用一种简单的文本传输类型来规定传输内容，它可以作为通讯中的文本传输协议,即交互中的高级协议来定义交互信息。 STOMP本身可以支持流类型的网络传输协议: websocket协议和tcp协议 它的格式为:123456789101112131415161718COMMANDheader1:value1header2:value2Body^@SUBSCRIBEid:sub-1destination:/topic/price.stock.*^@SENDdestination:/queue/tradecontent-type:application/jsoncontent-length:44&#123;&quot;action&quot;:&quot;BUY&quot;,&quot;ticker&quot;:&quot;MMM&quot;,&quot;shares&quot;,44&#125;^@ 当然STOMP已经应用于很多消息代理中，作为一个传输协议的规定，如:RabbitMQ, ActiveMQ 我们皆可以用STOMP和这类MQ进行消息交互. 除了STOMP相关的代理外，实际上还提供了一个stomp.js,用于浏览器客户端使用STOMP消息协议传输的js库。 让我们很方便的使用stomp.js进行与STOMP协议相关的代理进行交互. 正如我们所知，如果websocket内容传输信息使用STOMP来进行交互，websocket也很好的于消息代理器进行交互(如:RabbitMQ, ActiveMQ) 这样就很好的提供了消息代理的集成方案。 总结，使用STOMP的优点如下: 不需要自建一套自定义的消息格式现有stomp.js客户端(浏览器中使用)可以直接使用能路由信息到指定消息地点可以直接使用成熟的STOMP代理进行广播 如:RabbitMQ, ActiveMQ 技术落地后端技术方案选型websocket服务端选型:spring websocket 支持SockJS,开启SockJS后，可应对不同浏览器的通讯支持支持STOMP传输协议，可无缝对接STOMP协议下的消息代理器(如:RabbitMQ, ActiveMQ) 前端技术方案选型前端选型: stomp.js,sockjs.js 总结上述所用技术，是这样的逻辑: 开启socktJS: 如果有浏览器不支持websocket协议，可以在其他两种协议中进行选择，但是对于应用层来讲，使用起来是一样的。这是为了支持浏览器不支持websocket协议的一种备选方案 使用STOMP: 使用STOMP进行交互，前端可以使用stomp.js类库进行交互，消息一STOMP协议格式进行传输，这样就规定了消息传输格式。 消息进入后端以后，可以将消息与实现STOMP格式的代理器进行整合。这是为了消息统一管理，进行机器扩容时，可进行负载均衡部署 使用spring websocket: 使用spring websocket,是因为他提供了STOMP的传输自协议的同时，还提供了StockJS的支持。当然，除此之外，spring websocket还提供了权限整合的功能，还有自带天生与spring家族等相关框架进行无缝整合。 参考文档再谈websocket-论架构设计http://lrwinx.github.io/2017/07/09/%E5%86%8D%E8%B0%88websocket-%E8%AE%BA%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/ Websocket协议的学习、调研和实现http://www.cnblogs.com/lizhenghn/p/5155933.html]]></content>
      <categories>
        <category>网络通信</category>
      </categories>
      <tags>
        <tag>websocket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工作线程数究竟要设置为多少]]></title>
    <url>%2F2017%2F07%2F08%2F2017-07-08-work-thread%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 一、缘起Web-Server通常有个配置，最大工作线程数，后端服务一般也有个配置，工作线程池的线程数量，这个线程数的配置不同的业务架构师有不同的经验值，有些业务设置为CPU核数的2倍，有些业务设置为CPU核数的8倍，有些业务设置为CPU核数的32倍。 “工作线程数”的设置依据是什么，到底设置为多少能够最大化CPU性能，是本文要讨论的问题。 二、共性认知在进行进一步深入讨论之前，先以提问的方式就一些共性认知达成一致。 问：工作线程数是不是设置的越大越好？答：肯定不是的 服务器CPU核数有限，能够同时并发的线程数有限，单核CPU设置10000个工作线程没有意义 线程切换是有开销的，如果线程切换过于频繁，反而会使性能降低 问：调用sleep()函数的时候，线程是否一直占用CPU？答：不占用，等待时会把CPU让出来，给其他需要CPU资源的线程使用。 不止sleep()函数，在进行一些阻塞调用时，例如网络编程中的： 阻塞accept()，等待客户端连接 阻塞recv()，等待下游回包都不占用CPU资源。 问：单核CPU，设置多线程有意义么，是否能提高并发性能？答：即使是单核，使用多线程也是有意义的，大多数情况也能提高并发 多线程编码可以让代码更加清晰，例如：IO线程收发包，Worker线程进行任务处理，Timeout线程进行超时检测 如果有一个任务一直占用CPU资源在进行计算，此时增加线程并不能增加并发，例如以下代码会一直占用CPU，并使得CPU占用率达到100%：while(1){ i++; } 通常来说，Worker线程一般不会一直占用CPU进行计算，此时即使CPU是单核，增加Worker线程也能够提高并发，因为这个线程在休息的时候，其他的线程可以继续工作 三、常见服务线程模型了解常见的服务线程模型，有助于理解服务并发的原理，一般来说互联网常见的服务线程模型有两种： IO线程与工作现场通过任务队列解耦 纯异步 IO线程与工作线程通过队列解耦类模型 如上图，大部分Web-Server与服务框架都是使用这样的一种“IO线程与Worker线程通过队列解耦”类线程模型： 有少数几个IO线程监听上游发过来的请求，并进行收发包（生产者） 有一个或者多个任务队列，作为IO线程与Worker线程异步解耦的数据传输通道（临界资源） 有多个工作线程执行正真的任务（消费者） 这个线程模型应用很广，符合大部分场景，这个线程模型的特点是，工作线程内部是同步阻塞执行任务的（回想一下tomcat线程中是怎么执行Java程序的，dubbo工作线程中是怎么执行任务的），因此可以通过增加Worker线程数来增加并发能力，今天要讨论的重点是“该模型Worker线程数设置为多少能达到最大的并发”。 纯异步线程模型没有阻塞，这种线程模型只需要设置很少的线程数就能够做到很高的吞吐量，该模型的缺点是： 如果使用单线程模式，难以利用多CPU多核的优势 程序员更习惯写同步代码，callback的方式对代码的可读性有冲击，对程序员的要求也更高 框架更复杂，往往需要server端收发组件，server端队列，client端收发组件，client端队列，上下文管理组件，有限状态机组件，超时管理组件的支持 文章《RPC-client异步收发核心细节？》中有更详细的介绍，however，这个模型不是今天讨论的重点， 四、工作线程的工作模式了解工作线程的工作模式，对量化分析线程数的设置非常有帮助： 上图是一个典型的工作线程的处理过程，从开始处理start到结束处理end，该任务的处理共有7个步骤： 从工作队列里拿出任务，进行一些本地初始化计算，例如http协议分析、参数解析、参数校验等 访问cache拿一些数据 拿到cache里的数据后，再进行一些本地计算，这些计算和业务逻辑相关 通过RPC调用下游service再拿一些数据，或者让下游service去处理一些相关的任务 RPC调用结束后，再进行一些本地计算，怎么计算和业务逻辑相关 访问DB进行一些数据操作 操作完数据库之后做一些收尾工作，同样这些收尾工作也是本地计算，和业务逻辑相关 分析整个处理的时间轴，会发现： 其中1，3，5，7步骤中（上图中粉色时间轴），线程进行本地业务逻辑计算时需要占用CPU 而2，4，6步骤中（上图中橙色时间轴），访问cache、service、DB过程中线程处于一个等待结果的状态，不需要占用CPU，进一步的分解，这个“等待结果”的时间共分为三部分：2.1）请求在网络上传输到下游的cache、service、DB2.2）下游cache、service、DB进行任务处理2.3）cache、service、DB将报文在网络上传回工作线程 五、量化分析并合理设置工作线程数最后一起来回答工作线程数设置为多少合理的问题。 通过上面的分析，Worker线程在执行的过程中，有一部计算时间需要占用CPU，另一部分等待时间不需要占用CPU，通过量化分析，例如打日志进行统计，可以统计出整个Worker线程执行过程中这两部分时间的比例，例如： 执行计算，占用CPU的时间（粉色时间轴）是100ms 等待时间，不占用CPU的时间（橙色时间轴）也是100ms 得到的结果是，这个线程计算和等待的时间是1：1，即有50%的时间在计算（占用CPU），50%的时间在等待（不占用CPU）： 假设此时是单核，则设置为2个工作线程就可以把CPU充分利用起来，让CPU跑到100% 假设此时是N核，则设置为2N个工作现场就可以把CPU充分利用起来，让CPU跑到N*100% 结论：N核服务器，通过执行业务的单线程分析出本地计算时间为x，等待时间为y，则工作线程数（线程池线程数）设置为 N*(x+y)/x，能让CPU的利用率最大化。 经验：一般来说，非CPU密集型的业务（加解密、压缩解压缩、搜索排序等业务是CPU密集型的业务），瓶颈都在后端数据库访问或者RPC调用，本地CPU计算的时间很少，所以设置几十或者几百个工作线程是能够提升吞吐量的。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>架构师之路</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一行一行源码分析清楚AbstractQueuedSynchronizer(一)]]></title>
    <url>%2F2017%2F06%2F16%2F2017-07-20-abstract-queued-synchronizer-1%2F</url>
    <content type="text"><![CDATA[原文链接:https://javadoop.com/post/AbstractQueuedSynchronizer 在分析 Java 并发包 java.util.concurrent 源码的时候，少不了需要了解 AbstractQueuedSynchronizer（以下简写AQS）这个抽象类，因为它是 Java 并发包的基础工具类，是实现ReentrantLock、CountDownLatch、Semaphore、FutureTask等类的基础。 Google 一下 AbstractQueuedSynchronizer，我们可以找到很多关于 AQS 的介绍，但是很多都没有介绍清楚，因为大部分文章没有把其中的一些关键的细节说清楚。 本文将从ReentrantLock的公平锁源码出发，分析下 AbstractQueuedSynchronizer 这个类是怎么工作的，希望能给大家提供一些简单的帮助。 申明以下几点： 本文有点长，但是很简单很简单很简单，主要面向读者对象为并发编程的初学者，或者想要阅读java并发包源码的开发者。 建议在电脑上阅读，如果你想好好地理解所有的细节，而且你从来没看过相关的分析，你可能至少需要 20 分钟仔细看所有的描述，本文后面的 1/3 以上很简单，前面的 1/4 更简单，中间的部分要好好看。 如果你不知道为什么要看这个，我想告诉你，即使你看懂了所有的细节，你可能也不能把你的业务代码写得更好 源码环境 JDK1.7，看到不懂或有疑惑的部分，最好能自己打开源码看看。Doug Lea 大神的代码写得真心不错。 有很多英文注释我没有删除，这样读者可以参考着英文说的来，万一被我忽悠了呢 本文不分析共享模式，这样可以给读者减少很多负担，只要把独占模式看懂，共享模式读者应该就可以顺着代码看懂了。而且也不分析 condition 部分，所以应该说很容易就可以看懂了。 本文大量使用我们平时用得最多的 ReentrantLock 的概念，本质上来说是不正确的，读者应该清楚，AQS 不仅仅用来实现锁，只是希望读者可以用锁来联想 AQS 的使用场景，降低读者的阅读压力 ReentrantLock 的公平锁和非公平锁只有一点点区别，没有任何阅读压力 你需要提前知道什么是 CAS(CompareAndSet) 废话结束，开始。 AQS 结构先来看看 AQS 有哪些属性，搞清楚这些基本就知道 AQS 是什么套路了，毕竟可以猜嘛！ 1234567891011// 头结点，你直接把它当做 当前持有锁的线程 可能是最好理解的private transient volatile Node head;// 阻塞的尾节点，每个新的节点进来，都插入到最后，也就形成了一个隐式的链表private transient volatile Node tail;// 这个是最重要的，不过也是最简单的，代表当前锁的状态，0代表没有被占用，大于0代表有线程持有当前锁// 之所以说大于0，而不是等于1，是因为锁可以重入嘛，每次重入都加上1private volatile int state;// 代表当前持有独占锁的线程，举个最重要的使用例子，因为锁可以重入// reentrantLock.lock()可以嵌套调用多次，所以每次用这个来判断当前线程是否已经拥有了锁// if (currentThread == getExclusiveOwnerThread()) &#123;state++&#125;private transient Thread exclusiveOwnerThread; //继承自AbstractOwnableSynchronizer 怎么样，看样子应该是很简单的吧，毕竟也就四个属性啊。 AbstractQueuedSynchronizer 的等待队列示意如下所示，注意了，之后分析过程中所说的 queue，也就是阻塞队列不包含 head，不包含 head，不包含 head。 首先，我们先看下 ReentrantLock 的使用方式。123456789101112131415161718192021// 我用个web开发中的service概念吧public class OrderService &#123; // 使用static，这样每个线程拿到的是同一把锁，当然，spring mvc中service默认就是单例，别纠结这个 private static ReentrantLock reentrantLock = new ReentrantLock(true); public void createOrder() &#123; // 比如我们同一时间，只允许一个线程创建订单 reentrantLock.lock(); // 通常，lock 之后紧跟着 try 语句 try &#123; // 这块代码同一时间只能有一个线程进来(获取到锁的线程)， // 其他的线程在lock()方法上阻塞，等待获取到锁，再进来 // 执行代码... // 执行代码... // 执行代码... &#125; finally &#123; // 释放锁 reentrantLock.unlock(); &#125; &#125;&#125; ReentrantLock 在内部用了内部类Sync来管理锁，所以真正的获取锁和释放锁是由 Sync 的实现类来控制的。123abstract static class Sync extends AbstractQueuedSynchronizer &#123;&#125; Sync 有两个实现，分别为NonfairSync（非公平锁）和FairSync（公平锁），我们看 FairSync 部分。123public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 线程抢锁很多人肯定开始嫌弃上面废话太多了，下面跟着代码走，我就不废话了。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255static final class FairSync extends Sync &#123; private static final long serialVersionUID = -3000897897090466540L; // 争锁 final void lock() &#123; acquire(1); &#125; // 来自父类AQS，我直接贴过来这边，下面分析的时候同样会这样做，不会给读者带来阅读压力 // 我们看到，这个方法，如果tryAcquire(arg) 返回true, 也就结束了。 // 否则，acquireQueued方法会将线程压到队列中 public final void acquire(int arg) &#123; // 此时 arg == 1 // 首先调用tryAcquire(1)一下，名字上就知道，这个只是试一试 // 因为有可能直接就成功了呢，也就不需要进队列排队了， // 对于公平锁的语义就是：本来就没人持有锁，根本没必要进队列等待(又是挂起，又是等待被唤醒的) if (!tryAcquire(arg) &amp;&amp; // tryAcquire(arg)没有成功，这个时候需要把当前线程挂起，放到阻塞队列中。 acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) &#123; selfInterrupt(); &#125; &#125; /** * Fair version of tryAcquire. Don't grant access unless * recursive call or no waiters or is first. */ // 尝试直接获取锁，返回值是boolean，代表是否获取到锁 // 返回true：1.没有线程在等待锁；2.重入锁，线程本来就持有锁，也就可以理所当然可以直接获取 protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); // state == 0 此时此刻没有线程持有锁 if (c == 0) &#123; // 虽然此时此刻锁是可以用的，但是这是公平锁，既然是公平，就得讲究先来后到， // 看看有没有别人在队列中等了半天了 if (!hasQueuedPredecessors() &amp;&amp; // 如果没有线程在等待，那就用CAS尝试一下，成功了就获取到锁了， // 不成功的话，只能说明一个问题，就在刚刚几乎同一时刻有个线程抢先了 =_= // 因为刚刚还没人的，我判断过了😂😂😂 compareAndSetState(0, acquires)) &#123; // 到这里就是获取到锁了，标记一下，告诉大家，现在是我占用了锁 setExclusiveOwnerThread(current); return true; &#125; &#125; // 会进入这个else if分支，说明是重入了，需要操作：state=state+1 else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; // 如果到这里，说明前面的if和else if都没有返回true，说明没有获取到锁 // 回到上面一个外层调用方法继续看: // if (!tryAcquire(arg) // &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) // selfInterrupt(); return false; &#125; // 假设tryAcquire(arg) 返回false，那么代码将执行： // acquireQueued(addWaiter(Node.EXCLUSIVE), arg)， // 这个方法，首先需要执行：addWaiter(Node.EXCLUSIVE) /** * Creates and enqueues node for current thread and given mode. * * @param mode Node.EXCLUSIVE for exclusive, Node.SHARED for shared * @return the new node */ // 此方法的作用是把线程包装成node，同时进入到队列中 // 参数mode此时是Node.EXCLUSIVE，代表独占模式 private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure // 以下几行代码想把当前node加到链表的最后面去，也就是进到阻塞队列的最后 Node pred = tail; // tail!=null =&gt; 队列不为空(tail==head的时候，其实队列是空的，不过不管这个吧) if (pred != null) &#123; // 设置自己的前驱 为当前的队尾节点 node.prev = pred; // 用CAS把自己设置为队尾, 如果成功后，tail == node了 if (compareAndSetTail(pred, node)) &#123; // 进到这里说明设置成功，当前node==tail, 将自己与之前的队尾相连， // 上面已经有 node.prev = pred // 加上下面这句，也就实现了和之前的尾节点双向连接了 pred.next = node; // 线程入队了，可以返回了 return node; &#125; &#125; // 仔细看看上面的代码，如果会到这里， // 说明 pred==null(队列是空的) 或者 CAS失败(有线程在竞争入队) // 读者一定要跟上思路，如果没有跟上，建议先不要往下读了，往回仔细看，否则会浪费时间的 enq(node); return node; &#125; /** * Inserts node into queue, initializing if necessary. See picture above. * @param node the node to insert * @return node's predecessor */ // 采用自旋的方式入队 // 之前说过，到这个方法只有两种可能：等待队列为空，或者有线程竞争入队， // 自旋在这边的语义是：CAS设置tail过程中，竞争一次竞争不到，我就多次竞争，总会排到的 private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; // 之前说过，队列为空也会进来这里 if (t == null) &#123; // Must initialize // 初始化head节点 // 细心的读者会知道原来head和tail初始化的时候都是null，反正我不细心 // 还是一步CAS，你懂的，现在可能是很多线程同时进来呢 if (compareAndSetHead(new Node())) // 给后面用：这个时候head节点的waitStatus==0, 看new Node()构造方法就知道了 // 这个时候有了head，但是tail还是null，设置一下， // 把tail指向head，放心，马上就有线程要来了，到时候tail就要被抢了 // 注意：这里只是设置了tail=head，这里可没return哦，没有return，没有return // 所以，设置完了以后，继续for循环，下次就到下面的else分支了 tail = head; &#125; else &#123; // 下面几行，和上一个方法 addWaiter 是一样的， // 只是这个套在无限循环里，反正就是将当前线程排到队尾，有线程竞争的话排不上重复排 node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125; &#125; // 现在，又回到这段代码了 // if (!tryAcquire(arg) // &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) // selfInterrupt(); // 下面这个方法，参数node，经过addWaiter(Node.EXCLUSIVE)，此时已经进入阻塞队列 // 注意一下：如果acquireQueued(addWaiter(Node.EXCLUSIVE), arg))返回true的话， // 意味着上面这段代码将进入selfInterrupt()，所以正常情况下，下面应该返回false // 这个方法非常重要，应该说真正的线程挂起，然后被唤醒后去获取锁，都在这个方法里了 final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); // p == head 说明当前节点虽然进到了阻塞队列，但是是阻塞队列的第一个，因为它的前驱是head // 注意，阻塞队列不包含head节点，head一般指的是占有锁的线程，head后面的才称为阻塞队列 // 所以当前节点可以去试抢一下锁 // 这里我们说一下，为什么可以去试试： // 首先，它是队头，这个是第一个条件，其次，当前的head有可能是刚刚初始化的node， // enq(node) 方法里面有提到，head是延时初始化的，而且new Node()的时候没有设置任何线程 // 也就是说，当前的head不属于任何一个线程，所以作为队头，可以去试一试， // tryAcquire已经分析过了, 忘记了请往前看一下，就是简单用CAS试操作一下state if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; // 到这里，说明上面的if分支没有成功，要么当前node本来就不是队头， // 要么就是tryAcquire(arg)没有抢赢别人，继续往下看 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; &#125; /** * Checks and updates status for a node that failed to acquire. * Returns true if thread should block. This is the main signal * control in all acquire loops. Requires that pred == node.prev * * @param pred node's predecessor holding status * @param node the node * @return &#123;@code true&#125; if thread should block */ // 刚刚说过，会到这里就是没有抢到锁呗，这个方法说的是："当前线程没有抢到锁，是否需要挂起当前线程？" // 第一个参数是前驱节点，第二个参数才是代表当前线程的节点 private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; // 前驱节点的 waitStatus == -1 ，说明前驱节点状态正常，当前线程需要挂起，直接可以返回true if (ws == Node.SIGNAL) /* * This node has already set status asking a release * to signal it, so it can safely park. */ return true; // 前驱节点 waitStatus大于0 ，之前说过，大于0 说明前驱节点取消了排队。这里需要知道这点： // 进入阻塞队列排队的线程会被挂起，而唤醒的操作是由前驱节点完成的。 // 所以下面这块代码说的是将当前节点的prev指向waitStatus&lt;=0的节点， // 简单说，就是为了找个好爹，因为你还得依赖它来唤醒呢，如果前驱节点取消了排队， // 找前驱节点的前驱节点做爹，往前循环总能找到一个好爹的 if (ws &gt; 0) &#123; /* * Predecessor was cancelled. Skip over predecessors and * indicate retry. */ do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don't park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ // 仔细想想，如果进入到这个分支意味着什么 // 前驱节点的waitStatus不等于-1和1，那也就是只可能是0，-2，-3 // 在我们前面的源码中，都没有看到有设置waitStatus的，所以每个新的node入队时，waitStatu都是0 // 用CAS将前驱节点的waitStatus设置为Node.SIGNAL(也就是-1) compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false; &#125; // private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) // 这个方法结束根据返回值我们简单分析下： // 如果返回true, 说明前驱节点的waitStatus==-1，是正常情况，那么当前线程需要被挂起，等待以后被唤醒 // 我们也说过，以后是被前驱节点唤醒，就等着前驱节点拿到锁，然后释放锁的时候叫你好了 // 如果返回false, 说明当前不需要被挂起，为什么呢？往后看 // 跳回到前面是这个方法 // if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; // parkAndCheckInterrupt()) // interrupted = true; // 1. 如果shouldParkAfterFailedAcquire(p, node)返回true， // 那么需要执行parkAndCheckInterrupt(): // 这个方法很简单，因为前面返回true，所以需要挂起线程，这个方法就是负责挂起线程的 // 这里用了LockSupport.park(this)来挂起线程，然后就停在这里了，等待被唤醒======= private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted(); &#125; // 2. 接下来说说如果shouldParkAfterFailedAcquire(p, node)返回false的情况 // 仔细看shouldParkAfterFailedAcquire(p, node)，我们可以发现，其实第一次进来的时候，一般都不会返回true的，原因很简单，前驱节点的waitStatus=-1是依赖于后继节点设置的。也就是说，我都还没给前驱设置-1呢，怎么可能是true呢，但是要看到，这个方法是套在循环里的，所以第二次进来的时候状态就是-1了。 // 解释下为什么shouldParkAfterFailedAcquire(p, node)返回false的时候不直接挂起线程： // =&gt; 是为了应对在经过这个方法后，node已经是head的直接后继节点了。剩下的读者自己想想吧。&#125; 解锁操作最后，就是还需要介绍下唤醒的动作了。我们知道，正常情况下，如果线程没获取到锁，线程会被 LockSupport.park(this); 挂起停止，等待被唤醒。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// 唤醒的代码还是比较简单的，你如果上面加锁的都看懂了，下面都不需要看就知道怎么回事了public void unlock() &#123; sync.release(1);&#125;public final boolean release(int arg) &#123; // 往后看吧 if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125;// 回到ReentrantLock看tryRelease方法protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); // 是否完全释放锁 boolean free = false; // 其实就是重入的问题，如果c==0，也就是说没有嵌套锁了，可以释放了，否则还不能释放掉 if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free;&#125;/** * Wakes up node's successor, if one exists. * * @param node the node */// 唤醒后继节点// 从上面调用处知道，参数node是head头结点private void unparkSuccessor(Node node) &#123; /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; // 如果head节点当前waitStatus&lt;0, 将其修改为0 if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ // 下面的代码就是唤醒后继节点，但是有可能后继节点取消了等待（waitStatus==1） // 从队尾往前找，找到waitStatus&lt;=0的所有节点中排在最前面的 Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; // 从后往前找，仔细看代码，不必担心中间有节点取消(waitStatus==1)的情况 for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) // 唤醒线程 LockSupport.unpark(s.thread);&#125; 唤醒线程以后，被唤醒的线程将从以下代码中继续往前走：12345private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); // 刚刚线程被挂起在这里了 return Thread.interrupted();&#125;// 又回到这个方法了：acquireQueued(final Node node, int arg)，这个时候，node的前驱是head了 好了，后面就不分析源码了，剩下的还有问题自己去仔细看看代码吧。 总结总结一下吧。 在并发环境下，加锁和解锁需要以下三个部件的协调： 锁状态。我们要知道锁是不是被别的线程占有了，这个就是 state 的作用，它为 0 的时候代表没有线程占有锁，可以去争抢这个锁，用 CAS 将 state 设为 1，如果 CAS 成功，说明抢到了锁，这样其他线程就抢不到了，如果锁重入的话，state进行+1 就可以，解锁就是减 1，直到 state 又变为 0，代表释放锁，所以 lock() 和 unlock() 必须要配对啊。然后唤醒等待队列中的第一个线程，让其来占有锁。 线程的阻塞和解除阻塞。AQS 中采用了LockSupport.park(thread) 来挂起线程，用 unpark 来唤醒线程。 阻塞队列。因为争抢锁的线程可能很多，但是只能有一个线程拿到锁，其他的线程都必须等待，这个时候就需要一个 queue 来管理这些线程，AQS 用的是一个FIFO 的队列，就是一个链表，每个 node 都持有后继节点的引用。AQS 采用了 CLH 锁的变体来实现，感兴趣的读者可以参考这篇文章关于CLH的介绍，写得简单明了。 示例图解析下面属于回顾环节，用简单的示例来说一遍，如果上面的有些东西没看懂，这里还有一次帮助你理解的机会。 首先，第一个线程调用reentrantLock.lock()，翻到最前面可以发现，tryAcquire(1) 直接就返回 true 了，结束。只是设置了 state=1，连 head 都没有初始化，更谈不上什么阻塞队列了。要是线程 1 调用 unlock() 了，才有线程 2 来，那世界就太太太平了，完全没有交集嘛，那我还要 AQS 干嘛。 如果线程 1 没有调用 unlock() 之前，线程 2 调用了 lock(), 想想会发生什么？ 线程 2 会初始化 head【new Node()】，同时线程 2 也会插入到阻塞队列并挂起 (注意看这里是一个 for 循环，而且设置 head 和 tail 的部分是不 return 的，只有入队成功才会跳出循环)123456789101112131415private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; 首先，是线程 2 初始化 head 节点，此时 head==tail, waitStatus==0 然后线程 2 入队： 同时我们也要看此时节点的 waitStatus，我们知道 head 节点是线程2 初始化的，此时的 waitStatus 没有设置， java 默认会设置为 0，但是到 shouldParkAfterFailedAcquire 这个方法的时候，线程 2 会把前驱节点，也就是 head 的waitStatus设置为-1。 那线程 2 节点此时的 waitStatus 是多少呢，由于没有设置，所以是 0； 如果线程3此时再进来，直接插到线程2的后面就可以了，此时线程 3 的 waitStatus 是 0，到 shouldParkAfterFailedAcquire 方法的时候把前驱节点线程 2 的 waitStatus 设置为 -1。 这里可以简单说下 waitStatus 中 SIGNAL(-1) 状态的意思，Doug Lea 注释的是：代表后继节点需要被阻塞。也就是说这个 waitStatus 其实代表的不是自己的状态，而是后继节点的状态，我们知道，每个 node 在入队的时候，都会把前驱节点的状态改为 SIGNAL，然后阻塞，等待被前驱唤醒。这里涉及的是两个问题：有线程取消了排队、唤醒操作。其实本质是一样的，读者也可以顺着 “waitStatus代表后继节点的状态” 这种思路去看一遍源码。 补充几个流程图： 公平锁加锁 非公平锁加锁 公平与非公平锁解锁(没有区别)]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[互联网智能广告系统简易流程与架构]]></title>
    <url>%2F2017%2F05%2F26%2F2017-05-26-advertisement%2F</url>
    <content type="text"><![CDATA[转自： 架构师之路 很多朋友估计没有做过这一块，争取最简洁的语言描述清楚。 一、业务简述 从业务上看，整个智能广告系统，主要分为： 1）业务端：广告主的广告后台 2）展现端：用户实际访问的页面 业务端，广告主主要有两类行为： 1）广告设置行为：例如设置投放计划，设置地域，类别，关键字，竞价等 2）效果查看行为：例如广告展示次数是多少，广告点击次数是多少等 展现端，用户主要也有两类行为： 1）站点浏览行为：用户浏览实际的信息，此时广告系统决定出广告主的什么广告 2）广告点击行为：此时广告系统会对广告主进行扣费 二、业务流程下面通过一个的例子，让业务流程更直观。 步骤一：广告主在业务端投递广告 广告主登录业务端后台，进行设置： 今日投放地域是“北京-上地” 投放类别是“租房” 定向人群为“女”，“30岁以下” 需要推广的广告内容是他发布的一条“房屋出租”的帖子 竞价设置的是0.2元 单日预算是20元 这些数据，当然通过业务端存储到了数据层，即数据库和缓存里。 步骤二：用户来到了网站，进入了“北京-上地-租房”类别，广告初筛实施 用户产生了平台浏览行为，网站除了展示自然内容，还要展示广告内容。被展现的广告不能太离谱，太离谱用户也不会点击。 合适的广告，必须符合“语义相关性”，即基础检索属性（广告属性）必须符合（广告能否满足用户的需求，满足了点击率才高），这个工作是通过BS-basic search检索服务完成的。 BS从数据层检索到“北京-上地-租房”的广告帖子。 步骤三：用户属性与广告主属性匹配，广告精筛实施 步骤二中，基础属性初筛了以后，要进行更深层次的策略筛选（用户能否满足广告的需求），此例中，广告主的精准需求为： 用户性别为“女” 用户年龄为“30岁以下” 用户访问IP是“北京” 系统将初筛出来的M条广告和用户属性进行匹配筛选，又过滤掉了一部分，最后剩余N条待定广告，这些广告既满足用户的需求（初筛），这些用户也满足广告主的需求（精筛），后者是在AS-advanced search策略服务完成的。 步骤四：综合排序，并返回Top X的广告 经过步骤2和步骤3的初筛和精筛之后，待选的N条广告既能满足用户当前的需求，用户亦能满足广告主的筛选需求，但实际情况是，广告位只有3个，怎么办呢？就需要我们对N条广告进行综合打分排序（满足平台的需求，广告平台要多赚钱嘛）。 打分排序的依据是什么呢？有人说按照竞价排序bid，出价高的打分高（这是大家对百度最大的误解，百度是cpc收费） 有人说按照CTR点击率排序，CTR高的点的人多（百度的kpi指标可不是pv） 出价高，但没人点击，广告平台没有收益；点击率高，但出价低，广告平台还是没有收益。最终应该按照广告的出价与CTR的乘积作为综合打分排序的依据，bid * CTR。 既然bidCTR是所有广告综合打分的依据，且出价bid又是广告主事先设定好的，那么实际上，*广告排序问题的核心又转向了广告CTR的预测，CTR预测是推荐系统、广告系统、搜索系统里非常重要的一部分，是一个工程，算法，业务三方结合的问题，本文就不展开讨论了。 无论如何，N条广告，根据bid*预估CTR进行综合打分排序后，返回了打分最高的3个广告（广告位只有3个）。 有些系统没有第二步骤用户属性过滤，而是将用户属性因素考虑到综合排序中。 步骤五：展现端展示了广告，用户点击了广告 展示了广告后，展现端js会上报广告展示日志，有部分用户点击了广告，服务端会记录点击日志，这些日志可以作为广告算法实施的数据源，同时，他们经过统计分析之后，会被展示给广告主，让他们能够看到自己广告的展示信息，点击信息。 这些日志（一般会实施AB测），也是算法效果好坏评估的重要依据，根据效果逐步优化改进算法。 步骤六：对广告主进行扣费 用户既然点击了广告，平台就要对投放广告的广告主进行扣费了，扣费前当然要经过反作弊系统的过滤（主要是恶意点击），扣费后信息会实时反映到数据层，费用扣光后，广告就要从数据层下线。 三、系统综述聊完业务流程，再来看系统架构，任何脱离业务的架构设计都是耍流氓。 从系统分层架构上看，智能广告系统分为三层： 站点层：用户和广告主直接面向的网站站点 服务层：为了实现智能广告的业务逻辑，提供的通用服务，此处又主要分为四大类服务： 策略服务BS：实施广告策略，综合排序 检索服务AS：语义相关性检索 计费服务：用户点击广告时进行扣费 反作弊服务：不是每次点击都扣费，要经过反作弊，去除恶意点击（相对独立，未在架构图中画出） 数据层：用户数据，广告数据，竞价数据，日志数据等等等等 四、总结智能广告系统的业务流程与系统架构： 广告主投放与设置广告 用户访问平台，展现合适广告 通过 广告属性 ，进行“语义相关性”初筛，通过BS完成 通过 用户属性 ，出价信息，点击率预测信息，进行综合打分排序筛选，通过AS完成 记录展现日志，点击日志，进行扣费 广告是展现，是一个： 广告满足用户需求（初筛） 用户满足广告需求（精筛） 平台利益最大化（bid*CTR综合排序） 的过程 广告的排序不是由出价(bid)决定的，而是由出价(bid)*点击率(ctr)决定的。 点击率(ctr)是一个未来将要发生的行为，智能广告系统的核心与难点是点击率预测。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构师之路</tag>
        <tag>广告系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNS在架构设计中的巧用]]></title>
    <url>%2F2017%2F05%2F17%2F2017-05-17-dns%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 一、缘起一个http请求从客户端到服务端，整个执行流程是怎么样的呢？ 一个典型流程如上：(1)客户端通过域名daojia.com请求dns-server(2)dns-server返回域名对应的外网ip(1.2.3.4)(3)客户端访问外网ip(1.2.3.4)向反向代理nginx(4)反向代理nginx配置了多个后端web-server服务内网ip(192.168.0.1/192.168.0.2)(5)请求最终落到某一个web-server进行处理 其中，第一个步骤域名daojia.com到外网ip(1.2.3.4)的转换，发生在整个服务端外部，服务端不可控。 架构设计时，能够巧用dns做一些什么事情呢，是本文要讨论的问题。 二、反向代理水平扩展 典型的互联网架构中，可以通过增加web-server来扩充web层的性能，但反向代理nginx仍是整个系统对外的唯一入口，如果系统吞吐超过nginx的性能极限，难以扩容，此时就需要dns-server来配合水平扩展。 具体做法是：在dns-server对于同一个域名可以配置多个nginx的外网ip，每次dns解析请求，轮询返回不同的ip，这样就能实现nginx的水平扩展，这个方法叫“dns轮询”。 三、web-server负载均衡 既然“dns轮询”可以将同一个域名的流量均匀分配到不同的nginx，那么也可以利用它来做web-server的负载均衡：(1)架构中去掉nginx层(2)将多个web-server的内网ip直接改为外网ip(3)在dns-server将域名对应的外网ip进行轮询解析 和nginx相比，dns来实施负载均衡有什么优缺点呢？优点： 利用第三方dns实施，服务端架构不用动 少了一层网络请求 不足： dns只具备解析功能，不能保证对应外网ip的可用性（即使能够做80口的探测，实时性肯定也是比nginx差很多的），而nginx做反向代理时，与web-server之间有保活探测机制，当web-server挂掉时，能够自动迁移流量 当web-server需要扩容时，通过dns扩容生效时间长，而nginx是服务端完全自己可控的部分，web-server扩容更实时更方便 因为上面两个原因，架构上很少取消反向代理层，而直接使用dns来实施负载均衡。 四、用户就近访问 如文章“缘起”中所述，http请求的第一个步骤域名到外网ip的转换，发生在整个服务端外部，服务端不可控，那么如果要实施“根据客户端ip来分配最近的服务器机房访问”，就只能在dns-server上做了：(1)电信用户想要访问某一个服务器资源(2)浏览器向dns-server发起服务器域名解析请求(3)dns-server识别出访问者是电信用户(4)dns-server将电信机房的nginx外网ip返回给访问者(5)访问者就近访问 根据用户ip来返回最近的服务器ip，称为“智能dns”，cdn以及多机房多活中最常用。 五、总结架构设计中，dns有它独特的功能和作用： dns轮询，水平扩展反向代理层 去掉反向代理层，利用dns实施负载均衡 智能dns，根据用户ip来就近访问服务器]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构师之路</tag>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跨公网调用的大坑与架构优化方案]]></title>
    <url>%2F2017%2F05%2F13%2F2017-05-13-cross-network-call%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路第三方接口挂掉，我们的服务会受影响么？ 一、缘起与大坑很多时候，业务需要跨公网调用一个第三方服务提供的接口，为了避免每个调用方都依赖于第三方服务，往往会抽象一个服务： 解除调用方与第三方接口的耦合 当第三方的接口变动时，只有服务需要修改，而不是所有调用方均修改 此时接口调用流程是什么样的呢？ 如上图1-4所述：1）业务调用方调用内部service（2）内部service跨公网调用第三方接口（3）第三方接口返回结果给内部service（4）内部service返回结果给业务调用方 这个过程存在什么潜在的大坑呢？ 内部服务可能对上游业务提供了很多服务接口，当有一个接口跨公网第三方调用超时时，可能导致所有接口都不可用，即使大部分接口不依赖于跨公网第三方调用。 为什么会出现这种情况呢？ 内部服务对业务方提供的N个接口，会共用服务容器内的工作线程（假设有100个工作线程）。 假设这N个接口的某个接口跨公网依赖于第三方的接口，发生了网络抖动，或者接口超时（不妨设超时时间为5秒）。 潜台词是，这个工作线程会被占用5秒钟，然后超时返回业务调用方。 假设这个请求的吞吐量为20qps，言下之意，很短的时间内，所有的100个工作线程都会被卡在这个第三方超时等待上，而其他N-1个原本没有问题的接口，也得不到工作线程处理。 跨公网调用的稳定性优化，是本文要讨论的问题。 二、异步代理法业务场景：通过OpenID实时获取微信用户基本信息解决方案：增加一个代理，向服务屏蔽究竟是“本地实时”还是“异步远程”去获取返回结果 本地实时流程如上图1-5：（1）业务调用方调用内部service（2）内部service调用异步代理service（3）异步代理service通过OpenID在本地拿取数据（4）异步代理service将数据返回内部service（5）内部service返回结果给业务调用方 异步远程流程如上图6-8粗箭头的部分：（6）异步代理service定期跨公网调用微信服务（7）微信服务返回数据（8）刷新本地数据 优点：公网抖动，第三方接口超时，不影响内部接口调用 不足：本地返回的不是最新数据（很多业务可以接受数据延时） 有时候，内部service和异步代理service可以合成一个service 三、第三方接口备份与切换法业务场景：调用第三方短信网关，或者电子合同等解决方案：同时使用（或者备份）多个第三方服务 流程如上图1-4：（1）业务调用方调用内部service（2）内部service调用第一个三方接口（3）超时后，调用第二个备份服务，未来都直接调用备份服务，直到超时的服务恢复（4）内部service返回结果给业务调用方 优点：公网抖动，第三方接口超时，不影响内部接口调用（初期少数几个请求会超时） 不足：不是所有公网调用都能够像短息网关，电子合同服务一样有备份接口的，像微信、支付宝等就只此一家 四、异步调用法业务场景：本地结果，同步第三方服务，例如用户在58到家平台下单，58到家平台需要通知平台商家为用户提供服务解决方案：本地调用成功就返回成功，异步调用第三方接口同步数据（和异步代理有微小差别） 本地流程如上图1-3：（1）业务调用方调用内部service（2）内部service写本地数据（3）内部service返回结果给业务调用方成功 异步流程如上图4-5粗箭头的部分：（4）异步service定期将本地数据取出（或者通知也行，实时性好）（5）异步调用第三方接口同步数据 优点：公网抖动，第三方接口超时，不影响内部接口调用 不足：不是所有业务场景都可以异步同步数据 五、总结跨公网调用第三方，可能存在的问题： 公网抖动，第三方服务不稳定，影响自身服务 一个接口超时，占住工作线程，影响其他接口 降低影响的优化方案： 增大工作线程数 降低超时时间 服务垂直拆分 业务需求决定技术方案，结合业务的解决方案： 业务能接受旧数据：读取本地数据，异步代理定期更新数据 有多个第三方服务提供商：多个第三方互备 向第三方同步数据：本地写成功就算成功，异步向第三方同步数据 希望第三方的服务挂掉，不再影响大家的服务。 这个锅，我们不背。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构师之路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[系统通知，居然有人使用拉取？]]></title>
    <url>%2F2017%2F05%2F10%2F2017-05-10-pull-message%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 任何脱离业务场景的架构设计都是耍流氓。 广义系统通知，有1对1的通知，以及一对多的通知，有相对实时的业务通知，以及能够容忍一定延时的系统通知。结合具体的场景来看下，这样的一些系统通知，究竟是推还是拉？ 一、系统对1的通知典型业务，计数类通知： 有10个美女添加了你为好友 有8个好友私信了你 很多业务经常有这类计数通知，通知结果只针对你，这类通知是推送，还是拉取的呢？常见的有这样一些实践： 如果业务需求对计数需求需要实时展现，例如微博的加好友计数，假如希望实现不刷新网页，计数就实时变化： 登录微博时，会有一个计数的拉取，对网页端的计数进行初始化 int getCountByType(int countType) 在浏览微博的过程中，一旦有人加你为好友，服务端对网页端进行实时推送，告之增加了1个（或者N个）好友int addCountByType(int countType, int diff) 这里的思路是，一开始得到初始值，后续推送增量值，由网页端计算最终计数并呈现最终结果。需要注意，针对不同业务，计数变化的差值可增可减。 上述方案的坏处是，一旦有消息丢失，网页端的计数会一直不一致，直至再次登录重新初始化计数。这个计算计数可以优化为在服务器直接计算并通知网页端最终的结果，网页端只负责呈现即可，这样网页端的逻辑会变轻。 如果业务对此类通知的展现不需要这么实时，完全可以通过拉取：只有在链接跳转，或者刷新网页时，才重新拉取最新的通知，例如上述计数int getCountByType(int countType)这样系统的实现会最简单。需要注意，通知拉取要异步，不要影响主页面的快速返回。 系统对1的推送，例如针对1个用户的业务计数推送，计数的变化频率其实非常低，使用cache来存储这些计数能够极大提升系统性能。 更多计数系统架构实践可详见《计数系统架构实践一次搞定》。 二、系统对多的通知系统对多的通知消息，会比系统对1的通知消息复杂一些，以两个场景为例： QQ登录弹窗新闻 QQ右下角弹窗广告 IM登录弹窗新闻这个通知的需求是： 同一天，用户登录弹出的新闻是相同的（很多业务符合这样的场景），不同天新闻则不一样（但所有用户都一样） 每天第一次登录弹出新闻，当天的后续登录不出新闻 不妨设有一个表存放弹窗新闻：t_msg(msg_id, date, msg_content) 有一个表来存放用户信息：t_user(user_id, user_info, …) 有一个表来存放用户收到的新闻弹窗：t_user_msg(user_id, msg_id, date) 这里的实现明显不能采用推送的方式： 将t_user_msg里对于所有user_id推送插入一个msg_id，表示未读 在user每天第一次登录的时候，将当天的msg_id拉取出来，并删除，表示已读 在user每天非第一次登录的时候，就拉取不到msg_id于是不会再次弹窗 这个笨拙的方式，会导致t_user_msg里有大量的脏数据，毕竟大部分用户并不会登录。 如果改为拉取的方式会好很多： 在user每天第一次登陆时，将当天的msg_id拉取出来，并插入t_user_msg，表示已读 在user每天非第一次登陆时，则会插入t_user_msg失败，则说明已读，不再进行二次弹窗展现 这个方式虽然有所优化，但t_user_msg的数据量依然很大。 还有一种巧妙的方式，去除t_user_msg表，改为在t_user表加一列，表示用户最近拉取的弹窗时间：t_user(user_id, user_info, last_msg_date, …)这样业务流程会升级为： 在user每天第一次登录时，将当天的msg_id拉取出来，并将last_msg_date修改为今天 在user每天非第一次登录时，发现last_msg_date为今天，则说明今天已读 这种方式不再存储消息与用户的笛卡尔关系，数据量会大大减少，是不是有点意思？ IM右下角弹窗广告这个通知的需求是： 每天会对一批在线用户推送相同的弹窗TIPS广告，例如球鞋广告，手机广告等 画外音：如果1个推送一块钱，5KW用户推送收入就有5KW收入哟，一天推个几次，实现1个亿的小目标居然如此简单。 最直观的感受，这是一个*for循环批量推送的过程。如果是推送，必须要考虑的问题是，推送限速控制，避免短时间内对系统造成冲击，引发雪崩。 能不能用拉取呢？完全可以，这是一个对实时性要求不太高的场景，用户早1分钟晚1分钟收到这个广告影响不大，其实可以借助IM原本已有的keepalive请求，在请求返回时，告之“有消息拉取”，然后采用拉取的方式拉取广告消息。 这个方案的好处是，由于5KW在线用户的keepalive请求是均匀的，所以可以很均匀的将广告拉取的请求同样？ 均匀的分散？ 到一段时间内，避免5KW集中推送对系统造成冲击。 三、总结广义系统通知，究竟是推送还是拉取呢？不同业务，不同需求，实现方式不同。系统对1的通知： 实时性要求高，可以推送 实时性要求低，可以拉取 系统对N的通知： 登录弹窗新闻，拉取更佳，可以用一个last_msg_date来避免大量数据的存储 批量弹窗广告，常见的方法是推送，需要注意限速，也可以拉取，以实现请求的均匀分散 系统通知究竟是推还是拉，是一个相对比较简单的场景。对于feed流，单聊群聊，状态同步会更为复杂，这些场景，下期分解。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>服务化</tag>
        <tag>架构师之路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[feed留，单聊群聊，系统通知，状态同步，到底是推还是拉？]]></title>
    <url>%2F2017%2F05%2F05%2F2017-05-05-pull-or-push%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 今天抛一个话题，根据业务现象，一起讨论其后端实现是推还是拉？ 一、feed流可以理解为一个发布订阅业务，典型业务是微博（朋友圈）。你关注了姚晨的微博，姚晨发布了消息，你的主页能看到她最新发布的消息，这个场景是推送，还是拉取呢？ 画外音：微博是弱关系，关注无需对方同意，粉丝可以无上限；朋友圈是强关系，好友需要对方同意，好友个数有上线。 如果推送，姚晨发布消息的时候，要把消息ID投递到所有粉丝的主页消息队列里，推送量巨大。 如果拉取，一来主页消息无法实时更新，二来每次刷新动作非常复杂： 拉取你关注人的list 拉取这些人的消息list 对于这些人的这些消息进行rank处理，例如按照时间排序 还无法对主页进行缓存，因为只要有关注人发布消息，主页内容就会变化 还得考虑“不看谁的消息”，以及“消息不给谁看”… 是不是觉得有点烦？如果你是架构师，你会怎么做？ 二、聊天消息聊天消息又分为单聊和群聊，典型的业务是微信。和朋友小窗沟通是单聊，群内扯淡是群聊。 单聊，很容易想到是服务器推送，但浏览器里的聊天工具JS只能使用http式的request - response协议，又能不能保证消息的实时性呢？ 群聊，一个群500个人，有人在线，有人离线，到底是推送，还是拉取呢？如果是推送，1条消息将转变为500条消息，系统压力会异常之大。如果是拉取，消息的实时性又该如何保障呢？ 还有一个坑爹的需求，“钉钉”的群聊天消息“已读回执”，这个需求简单描述是：对于每一条你发出的每一群消息，你能够看到，多少人已读，多少人未读。这个群消息已读回执，猜猜看，又是怎么实现的呢？ 三、系统通知系统消息听上去比较泛，典型的业务是QQ的登录广告弹窗，以及登录后的右下角广告提示。 QQ每天首次登录后的新闻弹窗拉取？第二次登录却又没有。 QQ运行过程中的QQ弹窗广告推送？一次推送几千万条，会不会系统抖动？ 或许，真实的实现方式或许与我们想的并不一样。 四、状态同步玩桌面QQ时，收到过“你的好友XXOO登录了”的弹窗提示么？这是一个好友登录/登出状态的客户端同步。同理，群有500人，每个群友的在线/不在线状态又是怎么实现同步的呢？ 推送？那一个用户登录退出都要推送N个好友？M个群友？拉取？如何保证好友状态，群友状态的实时性？ 画外音：好友/群友状态一致性是非常复杂的，移动的时代，索性引入“一律在线”的概念，微信的好友就不存在所谓“头像亮”和“头像灰”的概念了，客户端状态同步这一块复杂性有所降低。 看到产品功能，思考后面的技术实现，其实是很有意思的一件事。究竟是推，还是拉？大伙怎么看。还有其他业务场景的疑惑，也欢迎评论提问，有价值的问题，5月份逐条解答。 画外音：自从有了群消息已读回执，我再也不能装作不在线，领导的消息没看到了。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>服务化</tag>
        <tag>架构师之路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot 使用心得(一)]]></title>
    <url>%2F2017%2F04%2F13%2F2017-04-13-springboot-0%2F</url>
    <content type="text"><![CDATA[https://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#boot-documentation 部门的web framework一直使用 SprignMvc + Spring, 最近想在新项目中尝试转型使用Spring Boot，于是把学习和使用的心得整理出来。 Spring Boot简介从 Spring Boot 项目名称中的 Boot 可以看出来，Spring Boot 的作用在于创建和启动新的基于 Spring 框架的项目。它的目的是帮助开发人员很容易的创建出独立运行和产品级别的基于 Spring 框架的应用。Spring Boot 会选择最适合的 Spring 子项目和第三方开源库进行整合。大部分 Spring Boot 应用只需要非常少的配置就可以快速运行起来。 Spring Boot 包含的特性如下： 创建可以独立运行的 Spring 应用。 直接嵌入 Tomcat 或 Jetty 服务器，不需要部署 WAR 文件。 提供推荐的基础 POM 文件来简化 Apache Maven 配置。 尽可能的根据项目依赖来自动配置 Spring 框架。 提供可以直接在生产环境中使用的功能，如性能指标、应用信息和应用健康检查。 没有代码生成，也没有 XML 配置文件。 环境要求Spring Boot 2.1.0.BUILD-SNAPSHOT 需要安装Java8 和 Spring Framework 5.0.5以上版本 安装Spring Boot您可以像使用任何标准Java库一样使用Spring Boot，为此，请在类路径中包含相应的spring-boot - *.jar文件。 Spring Boot不需要任何特殊的工具集成，因此您可以使用任何IDE或文本编辑器。此外，Spring Boot应用程序没有什么特别之处，因此您可以像运行其他任何Java程序一样运行和调试Spring Boot应用程序。 Maven安装方式Spring Boot与Apache Maven 3.2兼容 Spring Boot依赖org.springframework.boot groupId,通常，您的Maven POM文件继承自spring-boot-starter-parent项目并将依赖关系声明为一个或多个“Starter”。Spring Boot 也提供Maven插件以创建可执行的jar。 下面展示一个典型的pom例子:1234567891011121314151617181920212223242526272829303132333435&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.example&lt;/groupId&gt; &lt;artifactId&gt;myproject&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;!-- Inherit defaults from Spring Boot --&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;!-- Add typical dependencies for a web application --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- Package as an executable jar --&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 开发第一个SpringBoot程序创建src/main/java/Example.java:123456789101112131415161718import org.springframework.boot.*;import org.springframework.boot.autoconfigure.*;import org.springframework.web.bind.annotation.*;@RestController@EnableAutoConfigurationpublic class Example &#123; @RequestMapping("/") String home() &#123; return "Hello World!"; &#125; public static void main(String[] args) throws Exception &#123; SpringApplication.run(Example.class, args); &#125;&#125; 虽然这段代码不长，但包含好几个重要概念，下面将一一介绍。 @RestController和@RequestMapping 注解两者都是Spring MVC的注解，并不是Spring Boot特有的。@RestController 是stereotype 注解，相当于同时使用了@Controller和@ResponseBody。 @RequestMapping 主要提供请求的路由信息。例子中@RequestMapping作用就是告诉Spring任何路径为/的HTTP请求都映射到home方法，@RestController注解则告诉Spring将结果字符串直接返回给调用者。 @EnableAutoConfiguration注解第二级的注解是@EnableAutoConfiguration,这个注解的作用是告诉Spring Boot根据你添加的jar依赖关系“猜测”你想要如何配置Spring。由于spring-boot-starter-web添加了Tomcat和Spring MVC，因此自动配置假定您正在开发Web应用程序并相应地设置Spring。 Starters 和 自动配置 自动配置一般与“Starter”配合使用，但这两个概念并不直接相关。您可以自由选择”Starters”之外的jar依赖项。 Spring Boot仍然会尽可能自动配置您的应用程序 “main” 方法我们应用程序的最后一部分是主要方法。这只是一个遵循Java约定的应用程序入口点的标准方法。我们的main方法通过调用run来委托Spring Boot的SpringApplication类。SpringApplication启动我们的应用程序，从Spring开始，然后启动自动配置的Tomcat Web服务器。我们需要将Example.class作为参数传递给run方法，以告知SpringApplication它是Spring的主要组件。 args数组也被传递以暴露任何命令行参数。 运行例子 此时，例子就可以运行了。由于您使用了spring-boot-starter-parentPOM, 你可以使用run命令来启动应用程序。项目目录运行mvn spring-boot可以启动应用程序。您应该看到类似于以下内容的输出:12345678910111213$ mvn spring-boot:run . ____ _ __ _ _ /\\ / ___&apos;_ __ _ _(_)_ __ __ _ \ \ \ \( ( )\___ | &apos;_ | &apos;_| | &apos;_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) &apos; |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.0.1.RELEASE)....... . . ........ . . . (log output here)....... . . ......... Started Example in 2.222 seconds (JVM running for 6.514)]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[技术焦虑不是症]]></title>
    <url>%2F2017%2F03%2F29%2F2017-03-29-tech-anxiety%2F</url>
    <content type="text"><![CDATA[技术焦虑算是最近几年频繁出现的新词，而且往往会在后面加个“症”字。但我并不认为这是一种（心理）病，只能说是IT从业人员的一种特有心态，或者说是精神状态。按我理解，就是从业者(特别是程序猿)，由于新技术的更新速度远超出自身的学习速度，从而产生的一种不安，急躁的情绪。 首先，必须阐明一个观点，就是任何行业的从业者都是需要与时并进，持续学习的。当然，这里面需要学习的内容量和难易程度会有差异。纯体力的劳动要求应该算是最低的，但并不是完全不需要（哪怕是派传单，我们往往也会在过程中慢慢学会如何选择派发的对象以加快收工速度，而且现在纯派传单的活已经不多了，往往需要结合销售，那技巧就更多了）。有点扯远了，重点想表达的是，我们的IT行业，很不幸地处于这座金字塔的塔尖。 回顾一下近几年的技术热点： h5、大数据、云计算、VR、人工智能、区块链。对的，这里仅列出了普罗大众都或多或少听过的技术，而更可怕的是这些技术的爆发都只是在这短短4、5年间，基本上是每年一个风口。要知道的是， 这里面的任一门技术要研究透彻都需要穷一生精力，这就不难解释为什么会有部分程序猿会产生出强烈的焦虑感。刚学一门技术，还处于入门起步阶段，然后这个市场又无情地告诉你，现在最火最赚钱已经不是这个了，赶紧入一下个坑…..周而复始，陷入无间地狱…… 由此看来，要缓解技术焦虑的方法也很简单，无非就是调整好自己的心态，勿要随波逐流，盲目跟风。但此事知易行难，尤其当各种公众号都在鼓吹《XX技术人才月入五万仍难找》，《XX人才荒，挣钱太容易》的时候。所以，一个有趣的现象是，年轻的技术人出现技术焦虑的情况反而比年纪大的更多。年轻人优势在于勇于尝试，也有资本去做各种尝试，但技术的炼成不能竞一日之功，这又回到技术应该往深度还是广度发展的经典问题了。 必须承认，虽然我已不年轻，但我一直都有技术焦虑的困扰，只是随着见识越广思考越多，程度已经越来越低，焦虑更多已经逐渐转变为对新技术的(求知)欲求不满了。其实只要认准一个方向，只要不是太偏门的方向，一直深挖钻研，普遍都能获得不错的回本，这样的大牛例子太多。当你有一份稳定且能持续成长的技术在手，再去接触其他新技术的时候，就能以求知的心态去探索，而非迫于谋生的需要。 对了，为什么会心血来潮写这个，缘于今天公司有个童鞋（刚毕业），聊天的时候跟我抱怨说新技术更新得太快，要去追赶学习好累。我问他那你后悔进入这个行业吗，他说不后悔。或许这也反映了很多程序猿或向往成为程序猿的想法吧。如果程序猿的工作是每天回到公司泡杯茶，翻翻报纸，打开电脑看看新闻股市。我们还会选择这个职业吗？ 累，但我很快乐。 (完)]]></content>
      <categories>
        <category>有感</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[10w定时任务，如何高效触发超时]]></title>
    <url>%2F2017%2F03%2F09%2F2017-03-09-timer%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 一、缘起很多时候，业务有定时任务或者定时超时的需求，当任务量很大时，可能需要维护大量的timer，或者进行低效的扫描。 例如：58到家APP实时消息通道系统，对每个用户会维护一个APP到服务器的TCP连接，用来实时收发消息，对这个TCP连接，有这样一个需求：“如果连续30s没有请求包（例如登录，消息，keepalive包），服务端就要将这个用户的状态置为离线”。 其中，单机TCP同时在线量约在10w级别，keepalive请求包大概30s一次，吞吐量约在3000qps。 一般来说怎么实现这类需求呢？ “轮询扫描法”1）用一个Map&lt;uid, last_packet_time&gt;来记录每一个uid最近一次请求时间last_packet_time2）当某个用户uid有请求包来到，实时更新这个Map3）启动一个timer，当Map中不为空时，轮询扫描这个Map，看每个uid的last_packet_time是否超过30s，如果超过则进行超时处理 “多timer触发法”1）用一个Map&lt;uid, last_packet_time&gt;来记录每一个uid最近一次请求时间last_packet_time2）当某个用户uid有请求包来到，实时更新这个Map，并同时对这个uid请求包启动一个timer，30s之后触发3）每个uid请求包对应的timer触发后，看Map中，查看这个uid的last_packet_time是否超过30s，如果超过则进行超时处理 方案一：只启动一个timer，但需要轮询，效率较低方案二：不需要轮询，但每个请求包要启动一个timer，比较耗资源 特别在同时在线量很大时，很容易CPU100%，如何高效维护和触发大量的定时/超时任务，是本文要讨论的问题。 二、环形队列法废话不多说，三个重要的数据结构：1）30s超时，就创建一个index从0到30的环形队列（本质是个数组）2）环上每一个slot是一个Set，任务集合3）同时还有一个Map，记录uid落在环上的哪个slot里 同时：1）启动一个timer，每隔1s，在上述环形队列中移动一格，0-&gt;1-&gt;2-&gt;3…-&gt;29-&gt;30-&gt;0…2）有一个Current Index指针来标识刚检测过的slot 当有某用户uid有请求包到达时：1）从Map结构中，查找出这个uid存储在哪一个slot里2）从这个slot的Set结构中，删除这个uid3）将uid重新加入到新的slot中，具体是哪一个slot呢 =&gt; Current Index指针所指向的上一个slot，因为这个slot，会被timer在30s之后扫描到4）更新Map，这个uid对应slot的index值 哪些元素会被超时掉呢？Current Index每秒种移动一个slot，这个slot对应的Set中所有uid都应该被集体超时！如果最近30s有请求包来到，一定被放到Current Index的前一个slot了，Current Index所在的slot对应Set中所有元素，都是最近30s没有请求包来到的。 所以，当没有超时时，Current Index扫到的每一个slot的Set中应该都没有元素。 优势：（1）只需要1个timer（2）timer每1s只需要一次触发，消耗CPU很低（3）批量超时，Current Index扫到的slot，Set中所有元素都应该被超时掉 如果扫描timer处理超时的处理超过一秒怎么办？可以把超时任务交由其他线程处理 三、总结这个环形队列法是一个通用的方法，Set和Map中可以是任何task，本文的uid是一个最简单的举例。 HashedWheelTimer也是类似的原理，有兴趣的同学可以百度一下这个数据结构，Netty中的一个工具类。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构师之路</tag>
        <tag>定时任务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10w定时任务，如何高效触发超时]]></title>
    <url>%2F2017%2F03%2F09%2F2017-05-18-session%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 一、缘起什么是session？服务器为每个用户创建一个会话，存储用户的相关信息，以便多次请求能够定位到同一个上下文。 Web开发中，web-server可以自动为同一个浏览器的访问用户自动创建session，提供数据存储功能。最常见的，会把用户的登录信息、用户信息存储在session中，以保持登录状态。 什么是session一致性问题？只要用户不重启浏览器，每次http短连接请求，理论上服务端都能定位到session，保持会话。当只有一台web-server提供服务时，每次http短连接请求，都能够正确路由到存储session的对应web-server（废话，因为只有一台）。 此时的web-server是无法保证高可用的，采用“冗余+故障转移”的多台web-server来保证高可用时，每次http短连接请求就不一定能路由到正确的session了。 如上图，假设用户包含登录信息的session都记录在第一台web-server上，反向代理如果将请求路由到另一台web-server上，可能就找不到相关信息，而导致用户需要重新登录。 在web-server高可用时，如何保证session路由的一致性，是今天将要讨论的问题。 二、session同步法 思路：多个web-server之间相互同步session，这样每个web-server之间都包含全部的session 优点：web-server支持的功能，应用程序不需要修改代码 不足： session的同步需要数据传输，占内网带宽，有时延 所有web-server都包含所有session数据，数据量受内存限制，无法水平扩展 有更多web-server时要歇菜 三、客户端存储法 思路：服务端存储所有用户的session，内存占用较大，可以将session存储到浏览器cookie中，每个端只要存储一个用户的数据了 优点：服务端不需要存储 缺点： 每次http请求都携带session，占外网带宽 数据存储在端上，并在网络传输，存在泄漏、篡改、窃取等安全隐患 session存储的数据大小受cookie限制 “端存储”的方案虽然不常用，但确实是一种思路。 四、反向代理hash一致性思路：web-server为了保证高可用，有多台冗余，反向代理层能不能做一些事情，让同一个用户的请求保证落在一台web-server上呢？ 方案一：四层代理hash反向代理层使用 用户ip来做hash ，以 保证同一个ip的请求落在同一个web-server上 方案二：七层代理hash 反向代理使用http协议中的某些业务属性来做hash ，例如sid，city_id，user_id等，能够更加灵活的实施hash策略，以保证同一个浏览器用户的请求落在同一个web-server上 优点： 只需要改nginx配置，不需要修改应用代码 负载均衡，只要hash属性是均匀的，多台web-server的负载是均衡的 可以支持web-server水平扩展（session同步法是不行的，受内存限制） 不足： 如果web-server重启，一部分session会丢失，产生业务影响，例如部分用户重新登录 如果web-server水平扩展，rehash后session重新分布，也会有一部分用户路由不到正确的session session一般是有有效期的 ，所有不足中的两点， 可以认为等同于部分session失效** ，一般问题不大。 对于四层hash还是七层hash，个人推荐前者：让专业的软件做专业的事情，反向代理就负责转发，尽量不要引入应用层业务属性，除非不得不这么做（例如，有时候多机房多活需要按照业务属性路由到不同机房的web-server）。 五、后端统一存储 思路：将session存储在web-server后端的存储层，数据库或者缓存 优点： 没有安全隐患 可以水平扩展，数据库/缓存水平切分即可 web-server重启或者扩容都不会有session丢失 不足：增加了一次网络调用，并且需要修改应用代码 对于db存储还是cache，个人推荐后者：session读取的频率会很高，数据库压力会比较大。如果有session高可用需求，cache可以做高可用，但大部分情况下session可以丢失，一般也不需要考虑高可用。 六、总结保证session一致性的架构设计常见方法： session同步法：多台web-server相互同步数据 客户端存储法：一个用户只存储自己的数据 反向代理hash一致性：四层hash和七层hash都可以做，保证一个用户的请求落在一台web-server上 后端统一存储：web-server重启和扩容，session也不会丢失 对于方案3和方案4，个人建议推荐后者： web层、service层无状态是大规模分布式系统设计原则之一，session属于状态，不宜放在web层 让专业的软件做专业的事情，web-server存session？还是让cache去做这样的事情吧]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构师之路</tag>
        <tag>定时任务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话+图说：Java字节码指令——只为让你懂]]></title>
    <url>%2F2017%2F03%2F07%2F2017-03-07-bytecode%2F</url>
    <content type="text"><![CDATA[原文链接: https://segmentfault.com/a/1190000008606277 前言随着Android开发技术不断被推到新的高度，对于Android程序员来讲越来越需要具备一些对深入的基础性的技术的理解，比如说Java汇编指令。不然，可能很难深入理解一些时下的新框架、新技术，盲目一味追新也会越来越感乏力。 本文既不求照本宣科，亦不求炫技或著文立说，仅力图以最简明、最形象生动的方式，结合例子与实战，让小白也能搞懂这门看似复杂的技术概念。 单刀直入闲言碎语不要讲，先表一表，什么是Java汇编指令？简而言之，Java汇编指令就是Java虚拟机能够听得懂、可执行的指令，也可以说是Java代码的最小执行单元。有点Java基础的人一定都知道，javac命令会将Java源文件编译成字节码文件，也就是.class文件，里面就包含了大量的汇编指令。因此可以将javac命令理解为一个翻译命令，将源文件翻译成虚拟机可以执行的指令。那么最直观的探究方法莫过于直接对比一下翻译前后的内容。具体如何对比呢？就不得不用到Java为我们一直默默提供的一项利器，javap命令，它可以解析字节码，将字节码内部逻辑以可读的方式呈现出来。为了紧贴实战，我们直接在新建的Android工程里，写这样一个Activity类，里面包含几个由简单到复杂的方法和一个名为name的字段： 如图，以上方法，复杂度由低到高依次为：getName&lt;setName&lt;gotoBrowser&lt;staticMethod&lt;onCreate，后面我也会按照这个顺序解读其字节码指令的执行逻辑。下面我们编译工程，然后在下图所示的目录找到该类的字节码文件： cd到这个路径下，运行万能的javap命令：javap -v -p MainActivity，就可以观看到翻译版的Java字节码的胴体了！这里的 -v 意思是啰嗦模式，会输出全面的字节码信息，而 -p 是指涵盖所有成员。原字节码信息输出内容较多，基于本文的目标，取其一方法的内容，整理如下图： 方法1，getName()： 这个getName的方法应该是再简单不过的Java代码，翻译成字节码后也变成了三行，我们先来简单推理一下：第一句，aload_0不知所云，索性略过；第二行，getfield应该可以读懂，后面这个#2似乎是他的参数（实际上是对常量池的引用），//后面注释的内容是javap给我们加上的，意思应该是#2的指向是”Field name:Ljava/lang/String;”这个内容。所以getfield这一行就是取出name这个字段喽，so easy。areturn肯定就是return的意思，a的含义也先略过不表。总之就是取出name字段然后return喽。 那么现在的问题就是aload_0是什么意思了，看似多余，但仔细思考一下，似乎之前给getfield指令传入了“Field name:Ljava/lang/String;”这样一个并不完整的参数，其后半部分的“Ljava/lang/String;”仅仅表示这个name字段的类型是String，也就是说，整个参数里没有说是取的谁的name字段啊！究竟是get谁的feild呢？ 由此可以想到：aload操作一定是在为getfield指令准备了一个主体。 实际上，再结合下面的局部变量表，aload_0中的0正是局部变量表里的Slot 0的含义。意思是将局部变量表里的Slot 0的东西压入操作数栈，这个Slot 0里的东西name正是this，也就是MainActivity的实例，即getfield的主体。 大戏上演好了，对于小白同学有些陌生的概念来了，啥是操作数栈？啥是局部变量表？其实这两个东西理解好了，关于虚拟机指令就懂了一大半了。那么，不妨删繁就简，由易入难，先讲一个这样的故事，故事起名叫： Java方法之创世纪 话说Jvm大帝是神之旨意的履行者（Jvm大帝就是虚拟机，神就是开发者，神之旨意是开发者写好并编译后的字节码…），当Jvm大帝带领Java世界运行进入了一个新的方法后，会为这个方法在栈内存大陆上创造两个重要的领域：局部变量表和操作数栈。 要有栈。要有表。神说。 依照神之旨意，jvm大帝创造的局部变量表里一般会包含this指针（针对实例方法，静态方法当然无此）、方法的所有传入参数和方法中所开辟的本地变量。 那么操作数栈是干嘛用的呢？ 我们再引入另外一个比喻，如果把运行Java方法理解为拍戏，那么局部变量表里的各个局部变量就是这部戏的核心主角，或者说领衔主演，而操作数栈正是这部戏的舞台。所谓操作数栈搭台，局部变量唱戏，是也。那么aload_0就是告诉Jvm导演（大帝已沦落为导演），请0号演员this同志登台（压栈），演后边的本子。当然了，这个比喻并不完全恰当，因为操作数栈并不是“舞台”的结构，而是栈的结构。但是这个比喻可以很好地说明局部变量表和操作数栈之间的关系，以及aload_0的作用。 下面我们用一张图来演示一下getName这个小剧本桥段所导演的故事： 好吧这部剧虽然短的可怜，但已经基本把指令、操作数栈和局部变量表三者的关系演绎了出来。值得注意的是，getfield这条指令对操作数栈进行了复合操作，其流程可以示意如下图： 后面我们将要接触到的许多指令都如此，指令内部执行了弹出—&gt;处理—&gt;压回的流程。下面我们就来分析一个相对复杂一点的方法，setName(String)，如下图： 这里我们看到，变化主要有，指令多了一行，多进行了一次aload，getfield变成了putfield，areturn变成了return，仅此而已。另外领衔主演也就是局部变量表里多了一位，也就是方法的传入参数name字符串对象了。其情节如下： 这里，putfield只弹出栈内的操作数，而没有向操作数栈压回任何数据，而且执行putfield之前，栈内元素的位置也必须符合“值在上，主体在下”要求。 而最后的return仅表示方法结束，而不会像areturn一样返回栈顶元素。这也印证了setName(String)方法没有返回参数。 融会贯通相信有了以上的讲解，大家对指令、操作数栈、局部变量表三者的运作关系有了一定认识，为了后边能够分析更复杂的方法，这里必须概括性地讲解一下更多的Jvm汇编指令。虽然Jvm汇编指令非常多，但其实常用的不外乎几个类别，先从这几个常用类别入手理解，便可渐入佳境。 关于Java汇编指令的分类，可以从两个维度进行：一是指令的功能，二是指令操作的数据类型。我们先从功能说起，指令主要可以分为如下几类： 存储和加载类指令：主要包括load系列指令、store系列指令和ldc、push系列指令，主要用于在局部变量表、操作数栈和常量池三者之间进行数据调度；（关于常量池前面没有特别讲解，这个也很简单，顾名思义，就是这个池子里放着各种常量，好比片场的道具库） 对象操作指令（创建与读写访问）：比如我们刚刚的putfield和getfield就属于读写访问的指令，此外还有putstatic/getstatic，还有new系列指令，以及instanceof等指令。 操作数栈管理指令：如pop和dup，他们只对操作数栈进行操作。 类型转换指令和运算指令：如add/div/l2i等系列指令，实际上这类指令一般也只对操作数栈进行操作。 控制跳转指令：这类里包含常用的if系列指令以及goto类指令。 方法调用和返回指令：主要包括invoke系列指令和return系列指令。这类指令也意味这一个方法空间的开辟和结束，即invoke会唤醒一个新的java方法小宇宙（新的栈和局部变量表），而return则意味着这个宇宙的结束回收。 如下图，展示了各类指令的作用： a对应对象，表示指令操作对象性数据，比如aload和astore、areturn等等。 i对应整形。也就有iload，istore等i系列指令。 f对应浮点型。l对应long，b对应byte，d对应double，c对应char。 另外地，ia对应int array，aa对应object array，da对应double array。不在一一赘述。 了解了以上内容，我们再去看最后几个方法，应该就会容易理解很多了。 下面我们就直捣黄龙gotoBrowser这个方法： 这个过程简单解读如下： new一个Intent对象（在堆内存中开辟空间），并将其引用入栈； dup复制栈顶的刚刚放入的引用，再次压栈，这时栈里有两个重复的内容，深度为2； 从常量池取出“android.intent.action.View”这个字符串（对象引用）压栈，此时栈深度为3； 弹出栈顶的两个对象，调用弹出的第二个对象的方法，栈深度为1； 将此时栈顶引用弹出并存储到局部变量中（slot 2），此时栈就清空了，深度0； 后面的就相对简单了，读者可以尝试自己解读。再看这个包含if跳转的方法staticMethod： 如上图，图中已经说明的比较全面了，不再赘述。值得一提的是，Java的这种基于栈结构的汇编指令，在设计上有一种非常简洁的美感，指令与指令之间并没有较重的依赖，每条指令仅仅与操作数栈等领域内的数据发生关系，充满着某种平衡与秩序感。因此也必须注意，几乎每条指令的运行都有其前提，比如在invokevirtual或invokespecial指令执行前，必须保证操作数栈内提前按顺序压入好所需的操作数，否则就会发生问题。 后话另外，本文为了尽可能地简明生动、直入核心，简化了很多概念和细节，读者须知实际情况的更为复杂。但相信在理解了本文以后，就可以抓住Java汇编指令的核心理念，也就算扣开虚拟机学习的大门并可以开始读书精进了。下面盗图一张（后有出处），可作拓展：]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>字节码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线上服务内存OOM问题定位三板斧]]></title>
    <url>%2F2017%2F02%2F15%2F2017-08-21-oom%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 相信大家都有感触，线上服务内存OOM的问题，是最难定位的问题，于是我们进行了一次线上服务内存OOM问题排查实战演练，将内存OOM问题定位三板斧分享出来。 相信大家都有感触，线上服务内存OOM的问题，是最难定位的问题，不过归根结底，最常见的原因： 本身资源不够 申请的太多 资源耗尽 58到家架构部，运维部，58速运技术部联合进行了一次线上服务内存OOM问题排查实战演练，将内存OOM问题定位三板斧分享出来，希望对大家也有帮助。 题目某服务器上部署了Java服务一枚，出现了OutOfMemoryError，请问有可能是什么原因，问题应该如何定位?不妨设服务进程PID为10765(没错，就是CPU占用高的那个倒霉的进程《线上服务CPU100%问题快速定位实战》)。 解决思路Java服务OOM，最常见的原因为： 有可能是内存分配确实过小，而正常业务使用了大量内存 某一个对象被频繁申请，却没有释放，内存不断泄漏，导致内存耗尽 某一个资源被频繁申请，系统资源耗尽，例如：不断创建线程，不断发起网络连接 更具体的，可以使用以下的一些工具逐一排查。 方法：1jmap -heap 10765 如上图，可以查看新生代，老生代堆内存的分配大小以及使用情况，看是否本身分配过小。二、找到最耗内存的对象方法：1jmap -histo:live 10765 | more 如上图，输入命令后，会以表格的形式显示存活对象的信息，并按照所占内存大小排序： 实例数 所占内存大小 类名 是不是很直观?对于实例数较多，占用内存大小较多的实例/类，相关的代码就要针对性review了。上图中占内存最多的对象是RingBufferLogEvent，共占用内存18M，属于正常使用范围。如果发现某类对象占用内存很大(例如几个G)，很可能是类对象创建太多，且一直未释放。例如： 申请完资源后，未调用close()或dispose()释放资源 消费者消费速度慢(或停止消费了)，而生产者不断往队列中投递任务，导致队列中任务累积过多 三、确认是否是资源耗尽工具： pstree netstat查看进程创建的线程数，以及网络连接数，如果资源耗尽，也可能出现OOM。这里介绍另一种方法，通过12/proc/$&#123;PID&#125;/fd/proc/$&#123;PID&#125;/task 可以分别查看句柄详情和线程数。例如，某一台线上服务器的sshd进程PID是9339，查看12ll /proc/9339/fdll /proc/9339/task 如上图，sshd共占用了四个句柄 0 -&gt; 标准输入 1 -&gt; 标准输出 2 -&gt; 标准错误输出 3 -&gt; socket(容易想到是监听端口) sshd只有一个主线程PID为9339，并没有多线程。所以，只要12ll /proc/$&#123;PID&#125;/fd | wc -lll /proc/$&#123;PID&#125;/task | wc -l （效果等同pstree -p | wc -l） 就能知道进程打开的句柄数和线程数。]]></content>
      <categories>
        <category>Java语言</category>
      </categories>
      <tags>
        <tag>OOM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[58到家数据库30条军规解读]]></title>
    <url>%2F2017%2F02%2F15%2F2017-02-15-mysql%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 军规适用场景：并发量大、数据量大的互联网业务军规：介绍内容解读：讲解原因，解读比军规更重要 一、基础规范（1）必须使用InnoDB存储引擎解读：支持事务、行级锁、并发性能更好、CPU及内存缓存页优化使得资源利用率更高 （2）必须使用UTF8字符集解读：万国码，无需转码，无乱码风险，节省空间 （3）数据表、数据字段必须加入中文注释解读：N年后谁tm知道这个r1,r2,r3字段是干嘛的 （4）禁止使用存储过程、视图、触发器、Event解读：高并发大数据的互联网业务，架构设计思路是“解放数据库CPU，将计算转移到服务层”，并发量大的情况下，这些功能很可能将数据库拖死，业务逻辑放到服务层具备更好的扩展性，能够轻易实现“增机器就加性能”。数据库擅长存储与索引，CPU计算还是上移吧 （5）禁止存储大文件或者大照片解读：为何要让数据库做它不擅长的事情？大文件和照片存储在文件系统，数据库里存URI多好 二、命名规范（6）只允许使用内网域名，而不是ip连接数据库 （7）线上环境、开发环境、测试环境数据库内网域名遵循命名规范业务名称：xxx线上环境：dj.xxx.db开发环境：dj.xxx.rdb测试环境：dj.xxx.tdb从库在名称后加-s标识，备库在名称后加-ss标识线上从库：dj.xxx-s.db线上备库：dj.xxx-sss.db （8）库名、表名、字段名：小写，下划线风格，不超过32个字符，必须见名知意，禁止拼音英文混用 （9）表名t_xxx，非唯一索引名idx_xxx，唯一索引名uniq_xxx 三、表设计规范（10）单实例表数目必须小于500 （11）单表列数目必须小于30 （12）表必须有主键，例如自增主键解读：a）主键递增，数据行写入可以提高插入性能，可以避免page分裂，减少表碎片提升空间和内存的使用b）主键要选择较短的数据类型， Innodb引擎普通索引都会保存主键的值，较短的数据类型可以有效的减少索引的磁盘空间，提高索引的缓存效率c） 无主键的表删除，在row模式的主从架构，会导致备库夯住 （13）禁止使用外键，如果有外键完整性约束，需要应用程序控制解读：外键会导致表与表之间耦合，update与delete操作都会涉及相关联的表，十分影响sql 的性能，甚至会造成死锁。高并发情况下容易造成数据库性能，大数据高并发业务场景数据库使用以性能优先 四、字段设计规范（14）必须把字段定义为NOT NULL并且提供默认值解读：a）null的列使索引/索引统计/值比较都更加复杂，对MySQL来说更难优化b）null 这种类型MySQL内部需要进行特殊处理，增加数据库处理记录的复杂性；同等条件下，表中有较多空字段的时候，数据库的处理性能会降低很多c）null值需要更多的存储空，无论是表还是索引中每行中的null的列都需要额外的空间来标识d）对null 的处理时候，只能采用is null或is not null，而不能采用=、in、&lt;、&lt;&gt;、!=、not in这些操作符号。如：where name!=’shenjian’，如果存在name为null值的记录，查询结果就不会包含name为null值的记录 （15）禁止使用TEXT、BLOB类型解读：会浪费更多的磁盘和内存空间，非必要的大量的大字段查询会淘汰掉热数据，导致内存命中率急剧降低，影响数据库性能 （16）禁止使用小数存储货币解读：使用整数吧，小数容易导致钱对不上 （17）必须使用varchar(20)存储手机号解读：a）涉及到区号或者国家代号，可能出现+-()b）手机号会去做数学运算么？c）varchar可以支持模糊查询，例如：like“138%” （18）禁止使用ENUM，可使用TINYINT代替解读：a）增加新的ENUM值要做DDL操作b）ENUM的内部实际存储就是整数，你以为自己定义的是字符串？ 五、索引设计规范（19）单表索引建议控制在5个以内 （20）单索引字段数不允许超过5个解读：字段超过5个时，实际已经起不到有效过滤数据的作用了 （21）禁止在更新十分频繁、区分度不高的属性上建立索引解读：a）更新会变更B+树，更新频繁的字段建立索引会大大降低数据库性能b）“性别”这种区分度不大的属性，建立索引是没有什么意义的，不能有效过滤数据，性能与全表扫描类似 （22）建立组合索引，必须把区分度高的字段放在前面解读：能够更加有效的过滤数据 六、SQL使用规范（23）禁止使用SELECT *，只获取必要的字段，需要显示说明列属性解读：a）读取不需要的列会增加CPU、IO、NET消耗b）不能有效的利用覆盖索引c）使用SELECT *容易在增加或者删除字段后出现程序BUG （24）禁止使用INSERT INTO t_xxx VALUES(xxx)，必须显示指定插入的列属性解读：容易在增加或者删除字段后出现程序BUG （25）禁止使用属性隐式转换解读：SELECT uid FROM t_user WHERE phone=13812345678会导致全表扫描，而不能命中phone索引，猜猜为什么？（这个线上问题不止出现过一次） （26）禁止在WHERE条件的属性上使用函数或者表达式解读：SELECT uid FROM t_user WHERE from_unixtime(day)&gt;=’2017-02-15’会导致全表扫描正确的写法是：SELECT uid FROM t_user WHERE day&gt;= unix_timestamp(‘2017-02-15 00:00:00’) （27）禁止负向查询，以及%开头的模糊查询解读：a）负向查询条件：NOT、!=、&lt;&gt;、!&lt;、!&gt;、NOT IN、NOT LIKE等，会导致全表扫描b）%开头的模糊查询，会导致全表扫描 （28）禁止大表使用JOIN查询，禁止大表使用子查询解读：会产生临时表，消耗较多内存与CPU，极大影响数据库性能 （29）禁止使用OR条件，必须改为IN查询解读：旧版本Mysql的OR查询是不能命中索引的，即使能命中索引，为何要让数据库耗费更多的CPU帮助实施查询优化呢？ （30）应用程序必须捕获SQL异常，并有相应处理]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜索架构(二) 流量从小到大，检索架构演进]]></title>
    <url>%2F2017%2F02%2F14%2F2017-02-23-search-engine-2%2F</url>
    <content type="text"><![CDATA[转自： 架构师之路 一、缘起《深入浅出搜索架构（上篇）》详细介绍了： （1）全网搜索引擎架构与流程 （2）站内搜索引擎架构与流程 （3）搜索原理与核心数据结构 本文重点介绍： （4）流量数据量由小到大，常见搜索方案与架构变迁 （5）数据量、并发量、扩展性方案 只要业务有检索需求，本文一定对你有帮助。 二、检索需求的满足与架构演进任何互联网需求，或多或少有检索需求，还是以58同城的帖子业务场景为例，帖子的标题，帖子的内容有很强的用户检索需求，在业务、流量、并发量逐步递增的各个阶段，应该如何实现检索需求呢？ 原始阶段-LIKE数据在数据库中可能是这么存储的：t_tiezi(tid, title, content) 满足标题、内容的检索需求可以通过LIKE实现：select tid from t_tiezi where content like ‘%天通苑%’ 能够快速满足业务需求，存在的问题也显而易见： （1）效率低，每次需要全表扫描，计算量大，并发高时cpu容易100%（2）不支持分词 初级阶段-全文索引 如何快速提高效率，支持分词，并对原有系统架构影响尽可能小呢，第一时间想到的是建立全文索引：alter table t_tiezi add fulltext(title,content)使用match和against实现索引字段上的查询需求。 全文索引能够快速实现业务上分词的需求，并且快速提升性能（分词后倒排，至少不要全表扫描了），但也存在一些问题：（1）只适用于MyISAM（2）由于全文索引利用的是数据库特性，搜索需求和普通CURD需求耦合在数据库中：检索需求并发大时，可能影响CURD的请求；CURD并发大时，检索会非常的慢；（3）数据量达到百万级别，性能还是会显著降低，查询返回时间很长，业务难以接受（4）比较难水平扩展 中级阶段-开源外置索引 为了解决全文索的局限性，当数据量增加到大几百万，千万级别时，就要考虑外置索引了。外置索引的核心思路是：索引数据与原始数据分离，前者满足搜索需求，后者满足CURD需求，通过一定的机制（双写，通知，定期重建）来保证数据的一致性。 原始数据可以继续使用Mysql来存储，外置索引如何实施？Solr，Lucene，ES都是常见的开源方案。楼主强烈推荐ES（ElasticSearch），原因是Lucene虽好，但始终有一些不足：（1）Lucene只是一个库，潜台词是，需要自己做服务，自己实现高可用/可扩展/负载均衡等复杂特性（2）Lucene只支持Java，如果要支持其他语言，还是得自己做服务（3）Lucene不友好，这是很致命的，非常复杂，使用者往往需要深入了解搜索的知识来理解它的工作原理，为了屏蔽其复杂性，一个办法是自己做服务 为了改善Lucene的各项不足，解决方案都是“封装一个接口友好的服务，屏蔽底层复杂性”，于是有了ES：（1）ES是一个以Lucene为内核来实现搜索功能，提供REStful接口的服务（2）ES能够支持很大数据量的信息存储，支持很高并发的搜索请求（3）ES支持集群，向使用者屏蔽高可用/可扩展/负载均衡等复杂特性 目前58到家使用ES作为核心，实现了自己的搜索服务平台，能够通过在平台上简单的配置，实现业务方的搜索需求。 搜索服务数据量最大的“接口耗时数据收集”需求，数据量大概在7亿左右；并发量最大的“经纬度，地理位置搜索”需求，线上平均并发量大概在600左右，压测数据并发量在6000左右。 结论：ES完全能满足10亿数据量，5k吞吐量的常见搜索业务需求，强烈推荐。 高级阶段-自研搜索引擎 当数据量进一步增加，达到10亿、100亿数据量；并发量也进一步增加，达到每秒10万吞吐；业务个性也逐步增加的时候，就需要自研搜索引擎了，定制化实现搜索内核了。 三、数据量、并发量、扩展性方案到了定制化自研搜索引擎的阶段，超大数据量、超高并发量为设计重点，为了达到“无限容量、无限并发”的需求，架构设计需要重点考虑“扩展性”，力争做到：增加机器就能扩容（数据量+并发量）。 58同城的自研搜索引擎E-search初步架构图如下： （1）上层proxy（粉色）是接入集群，为对外门户，接受搜索请求，其无状态性能够保证增加机器就能扩充proxy集群性能（2）中层merger（浅蓝色）是逻辑集群，主要用于实现搜索合并，以及打分排序，业务相关的rank就在这一层实现，其无状态性也能够保证增加机器就能扩充merger集群性能（3）底层searcher（暗红色大框）是检索集群，服务和索引数据部署在同一台机器上，服务启动时可以加载索引数据到内存，请求访问时从内存中load数据，访问速度很快（3.1）为了满足数据容量的扩展性，索引数据进行了水平切分，增加切分份数，就能够无限扩展性能，如上图searcher分为了4组（3.2）为了满足一份数据的性能扩展性，同一份数据进行了冗余，理论上做到增加机器就无限扩展性能，如上图每组searcher又冗余了2份 如此设计，真正做到做到增加机器就能承载更多的数据量，响应更高的并发量。 三、总结为了满足搜索业务的需求，随着数据量和并发量的增长，搜索架构一般会经历这么几个阶段： （1）原始阶段-LIKE（2）初级阶段-全文索引（3）中级阶段-开源外置索引（4）高级阶段-自研搜索引擎]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构师之路</tag>
        <tag>搜索引擎</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜索架构(一) 深入浅出搜索架构引擎、方案与细节]]></title>
    <url>%2F2017%2F02%2F14%2F2017-02-14-search-engine-1%2F</url>
    <content type="text"><![CDATA[转自： 架构师之路 一、缘起《100亿数据1万属性数据架构设计》文章发布后，不少朋友对58同城自研搜索引擎E-search比较感兴趣，故专门撰文体系化的聊聊搜索引擎，从宏观到细节，希望把逻辑关系讲清楚，内容比较多，分上下两期。 主要内容如下，本篇（上）会重点介绍前三章： （1）全网搜索引擎架构与流程 （2）站内搜索引擎架构与流程 （3）搜索原理、流程与核心数据结构 （4）流量数据量由小到大，搜索方案与架构变迁 （5）数据量、并发量、策略扩展性及架构方案 （6）实时搜索引擎核心技术 可能99%的同学不实施搜索引擎，但本文一定对你有帮助。 二、全网搜索引擎架构与流程全网搜索的宏观架构长啥样？全网搜索的宏观流程是怎么样的？ 全网搜索引擎的宏观架构如上图，核心子系统主要分为三部分（粉色部分）： spider爬虫系统 search&amp;index建立索引与查询索引系统，这个系统又主要分为两部分： 一部分用于生成索引数据build_index 一部分用于查询索引数据search_index rank打分排序系统 核心数据主要分为两部分（紫色部分）： web网页库 index索引数据 全网搜索引擎的业务特点决定了，这是一个“写入”和“检索”完全分离的系统： 【写入】 系统组成：由spider与search&amp;index两个系统完成 输入：站长们生成的互联网网页 输出：正排倒排索引数据 流程：如架构图中的1，2，3，4 spider把互联网网页抓过来 spider把互联网网页存储到网页库中（这个对存储的要求很高，要存储几乎整个“万维网”的镜像） build_index从网页库中读取数据，完成分词 build_index生成倒排索引 【检索】系统组成：由search&amp;index与rank两个系统完成 输入：用户的搜索词 输出：排好序的第一页检索结果 流程：如架构图中的a，b，c，d （a）search_index获得用户的搜索词，完成分词 （b）search_index查询倒排索引，获得“字符匹配”网页，这是初筛的结果 （c）rank对初筛的结果进行打分排序 （d）rank对排序后的第一页结果返回 三、站内搜索引擎架构与流程做全网搜索的公司毕竟是少数，绝大部分公司要实现的其实只是一个站内搜索，站内搜索引擎的宏观架构和全网搜索引擎的宏观架构有什么异同？ 以58同城100亿帖子的搜索为例，站内搜索系统架构长啥样？站内搜索流程是怎么样的？ 站内搜索引擎的宏观架构如上图，与全网搜索引擎的宏观架构相比，差异只有写入的地方： （1）全网搜索需要spider要被动去抓取数据 （2）站内搜索是内部系统生成的数据，例如“发布系统”会将生成的帖子主动推给build_data系统 看似“很小”的差异，架构实现上难度却差很多：全网搜索如何“实时”发现“全量”的网页是非常困难的，而站内搜索容易实时得到全部数据。 对于spider、search&amp;index、rank三个系统： spider和search&amp;index是相对工程的系统 rank是和业务、策略紧密、算法相关的系统，搜索体验的差异主要在此，而业务、策略的优化是需要时间积累的，这里的启示是： a）Google的体验比Baidu好，根本在于前者rank牛逼 b）国内互联网公司（例如360）短时间要搞一个体验超越Baidu的搜索引擎，是很难的，真心需要时间的积累 四、搜索原理与核心数据结构什么是正排索引？ 什么是倒排索引？ 搜索的过程是什么样的？ 会用到哪些算法与数据结构？ 前面的内容太宏观，为了照顾大部分没有做过搜索引擎的同学，数据结构与算法部分从正排索引、倒排索引一点点开始。 提问：什么是正排索引（forward index）？ 回答：由key查询实体的过程，是正排索引。 用户表：t_user(uid, name, passwd, age, sex)，由uid查询整行的过程，就是正排索引查询。 网页库：t_web_page(url, page_content)，由url查询整个网页的过程，也是正排索引查询。 网页内容分词后，page_content会对应一个分词后的集合list。 简易的，正排索引可以理解为Map&lt;url, list&lt; item &gt;&gt;，能够由网页快速（时间复杂度O(1)）找到内容的一个数据结构。 提问：什么是倒排索引（inverted index）？ 回答：由item查询key的过程，是倒排索引。 对于网页搜索，倒排索引可以理解为Map&lt;item, list&lt; url &gt;&gt;，能够由查询词快速（时间复杂度O(1)）找到包含这个查询词的网页的数据结构。 举个例子，假设有3个网页： url1 -&gt; “我爱北京” url2 -&gt; “我爱到家” url3 -&gt; “到家美好” 这是一个正排索引Map。 分词之后： url1 -&gt; {我，爱，北京} url2 -&gt; {我，爱，到家} url3 -&gt; {到家，美好} 这是一个分词后的正排索引Map&lt;url, list&lt; item &gt;&gt;。 分词后倒排索引： 我 -&gt; {url1, url2} 爱 -&gt; {url1, url2} 北京 -&gt; {url1} 到家 -&gt; {url2, url3} 美好 -&gt; {url3} 由检索词item快速找到包含这个查询词的网页Map&lt;item, list&lt; url &gt;&gt;就是倒排索引。 正排索引和倒排索引是spider和build_index系统提前建立好的数据结构，为什么要使用这两种数据结构，是因为它能够快速的实现“用户网页检索”需求（业务需求决定架构实现）。 提问：搜索的过程是什么样的？ 假设搜索词是“我爱”，用户会得到什么网页呢？ 分词，“我爱”会分词为{我，爱}，时间复杂度为O(1) 每个分词后的item，从倒排索引查询包含这个item的网页list，时间复杂度也是O(1)： 我 -&gt; {url1, url2} 爱 -&gt; {url1, url2} 求list的交集，就是符合所有查询词的结果网页，对于这个例子，{url1, url2}就是最终的查询结果 看似到这里就结束了，其实不然，分词和倒排查询时间复杂度都是O(1)，整个搜索的时间复杂度取决于“求list的交集”，问题转化为了求两个集合交集。 字符型的url不利于存储与计算，一般来说每个url会有一个数值型的url_id来标识，后文为了方便描述，list统一用list替代。 list1和list2，求交集怎么求？ 方案一：for for，土办法，时间复杂度O(nn) 每个搜索词命中的网页是很多的，O(nn)的复杂度是明显不能接受的。倒排索引是在创建之初可以进行排序预处理，问题转化成两个*有序的list求交集，就方便多了。 方案二：有序list求交集，拉链法 有序集合1{1,3,5,7,8,9} 有序集合2{2,3,4,5,6,7} 两个指针指向首元素，比较元素的大小： （1）如果相同，放入结果集，随意移动一个指针 （2）否则，移动值较小的一个指针，直到队尾 这种方法的好处是： （1）集合中的元素最多被比较一次，时间复杂度为O(n) （2）多个有序集合可以同时进行，这适用于多个分词的item求url_id交集这个方法就像一条拉链的两边齿轮，一一比对就像拉链，故称为拉链法 方案三：分桶并行优化 数据量大时，url_id分桶水平切分+并行运算是一种常见的优化方法，如果能将list1和list2分成若干个桶区间，每个区间利用多线程并行求交集，各个线程结果集的并集，作为最终的结果集，能够大大的减少执行时间。 举例： 有序集合1{1,3,5,7,8,9, 10,30,50,70,80,90} 有序集合2{2,3,4,5,6,7, 20,30,40,50,60,70} 求交集，先进行分桶拆分： 桶1的范围为[1, 9] 桶2的范围为[10, 100] 桶3的范围为[101, max_int] 于是： 集合1就拆分成 集合a{1,3,5,7,8,9} 集合b{10,30,50,70,80,90} 集合c{} 集合2就拆分成 集合d{2,3,4,5,6,7} 集合e{20,30,40,50,60,70} 集合e{} 每个桶内的数据量大大降低了，并且每个桶内没有重复元素，可以利用多线程并行计算： 桶1内的集合a和集合d的交集是x{3,5,7} 桶2内的集合b和集合e的交集是y{30, 50, 70} 桶3内的集合c和集合d的交集是z{} 最终，集合1和集合2的交集，是x与y与z的并集，即集合{3,5,7,30,50,70} 方案四：bitmap再次优化 数据进行了水平分桶拆分之后，每个桶内的数据一定处于一个范围之内，如果集合符合这个特点，就可以使用bitmap来表示集合： 如上图，假设set1{1,3,5,7,8,9}和set2{2,3,4,5,6,7}的所有元素都在桶值[1, 16]的范围之内，可以用16个bit来描述这两个集合，原集合中的元素x，在这个16bitmap中的第x个bit为1，此时两个bitmap求交集，只需要将两个bitmap进行“与”操作，结果集bitmap的3，5，7位是1，表明原集合的交集为{3,5,7} 水平分桶，bitmap优化之后，能极大提高求交集的效率，但时间复杂度仍旧是O(n)bitmap需要大量连续空间，占用内存较大 方案五：跳表skiplist有序链表集合求交集，跳表是最常用的数据结构，它可以将有序集合求交集的复杂度由O(n)降至O(log(n)) 集合1{1,2,3,4,20,21,22,23,50,60,70} 集合2{50,70} 要求交集，如果用拉链法，会发现1,2,3,4,20,21,22,23都要被无效遍历一次，每个元素都要被比对，时间复杂度为O(n)，能不能每次比对“跳过一些元素”呢？ 跳表就出现了： 集合1{1,2,3,4,20,21,22,23,50,60,70}建立跳表时，一级只有{1,20,50}三个元素，二级与普通链表相同集合2{50,70}由于元素较少，只建立了一级普通链表 如此这般，在实施“拉链”求交集的过程中，set1的指针能够由1跳到20再跳到50，中间能够跳过很多元素，无需进行一一比对，跳表求交集的时间复杂度近似O(log(n))，这是搜索引擎中常见的算法。 五、总结文字很多，有宏观，有细节，对于大部分不是专门研究搜索引擎的同学，记住以下几点即可： 全网搜索引擎系统由spider， search&amp;index， rank三个子系统构成 站内搜索引擎与全网搜索引擎的差异在于，少了一个spider子系统 spider和search&amp;index系统是两个工程系统，rank系统的优化却需要长时间的调优和积累 正排索引（forward index）是由网页url_id快速找到分词后网页内容list的过程 倒排索引（inverted index）是由分词item快速寻找包含这个分词的网页list的过程 用户检索的过程，是先分词，再找到每个item对应的list，最后进行集合求交集的过程 有序集合求交集的方法有 a）二重for循环法，时间复杂度O(n*n) b）拉链法，时间复杂度O(n) c）水平分桶，多线程并行 d）bitmap，大大提高运算并行度，时间复杂度O(n) e）跳表，时间复杂度为O(log(n)) 六、下章预告a）流量数据量由小到大，搜索方案与架构变迁-&gt; 这个应该很有用，很多处于不同发展阶段的互联网公司都在做搜索系统，58同城经历过流量从0到10亿，数据量从0到100亿，搜索架构也不断演化着 b）数据量、并发量、策略扩展性及架构方案 c）实时搜索引擎核心技术 -&gt; 站长发布1个新网页，Google如何做到15分钟后检索出来]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构师之路</tag>
        <tag>搜索引擎</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP长连接、短连接究竟是什么？]]></title>
    <url>%2F2017%2F02%2F04%2F2017-02-04-http-keep-alive%2F</url>
    <content type="text"><![CDATA[1. HTTP协议与TCP/IP协议的关系HTTP的长连接和短连接本质上是TCP长连接和短连接。HTTP属于应用层协议，在传输层使用TCP协议，在网络层使用IP协议。 IP协议主要解决网络路由和寻址问题，TCP协议主要解决如何在IP层之上可靠地传递数据包，使得网络上接收端收到发送端所发出的所有包，并且顺序与发送顺序一致。TCP协议是可靠的、面向连接的。 2. 如何理解HTTP协议是无状态的HTTP协议是无状态的，指的是协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态。也就是说，打开一个服务器上的网页和上一次打开这个服务器上的网页之间没有任何联系。HTTP是一个无状态的面向连接的协议，无状态不代表HTTP不能保持TCP连接，更不能代表HTTP使用的是UDP协议（无连接）。 3. 什么是长连接、短连接？在HTTP/1.0中默认使用短连接。也就是说，客户端和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中断连接。当客户端浏览器访问的某个HTML或其他类型的Web页中包含有其他的Web资源（如JavaScript文件、图像文件、CSS文件等），每遇到这样一个Web资源，浏览器就会重新建立一个HTTP会话。 而从HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头加入这行代码： Connection:keep-alive 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。 HTTP协议的长连接和短连接，实质上是TCP协议的长连接和短连接。 3.1. TCP连接当网络通信时采用TCP协议时，在真正的读写操作之前，客户端与服务器端之间必须建立一个连接，当读写操作完成后，双方不再需要这个连接时可以释放这个连接。连接的建立依靠“三次握手”，而释放则需要“四次握手”，所以每个连接的建立都是需要资源消耗和时间消耗的。 经典的三次握手建立连接示意图： 第一次握手：Client将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给Server，Client进入SYN_SENT状态，等待Server确认。 第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入SYN_RCVD状态。 第三次握手：Client收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给Server，Server检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client与Server之间可以开始传输数据了。 经典的四次握手关闭连接示意图： 第一次挥手：Client发送一个FIN，用来关闭Client到Server的数据传送，Client进入FIN_WAIT_1状态。 第二次挥手：Server收到FIN后，发送一个ACK给Client，确认序号为收到序号+1（与SYN相同，一个FIN占用一个序号），Server进入CLOSE_WAIT状态。 第三次挥手：Server发送一个FIN，用来关闭Server到Client的数据传送，Server进入LAST_ACK状态。 第四次挥手：Client收到FIN后，Client进入TIME_WAIT状态，接着发送一个ACK给Server，确认序号为收到序号+1，Server进入CLOSED状态，完成四次挥手。 上面是一方主动关闭，另一方被动关闭的情况，实际中还会出现同时发起主动关闭的情况，具体流程如下图： 为什么建立连接是三次握手，而关闭连接却是四次挥手呢？这是因为服务端在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。而关闭连接时，当收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，己方也未必全部数据都发送给对方了，所以己方可以立即close，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，己方ACK和FIN一般都会分开发送。 3.2. TCP短连接模拟一下TCP短连接的情况：client向server发起连接请求，server接到请求，然后双方建立连接。client向server发送消息，server回应client，然后一次请求就完成了。这时候双方任意都可以发起close操作，不过一般都是client先发起close操作。上述可知，短连接一般只会在 client/server间传递一次请求操作。 短连接的优点是：管理起来比较简单，存在的连接都是有用的连接，不需要额外的控制手段。 3.3. TCP长连接我们再模拟一下长连接的情况：client向server发起连接，server接受client连接，双方建立连接，client与server完成一次请求后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。 TCP的保活功能主要为服务器应用提供。如果客户端已经消失而连接未断开，则会使得服务器上保留一个半开放的连接，而服务器又在等待来自客户端的数据，此时服务器将永远等待客户端的数据。保活功能就是试图在服务端器端检测到这种半开放的连接。 如果一个给定的连接在两小时内没有任何动作，服务器就向客户发送一个探测报文段，根据客户端主机响应探测4个客户端状态： 客户主机依然正常运行，且服务器可达。此时客户的TCP响应正常，服务器将保活定时器复位。 客户主机已经崩溃，并且关闭或者正在重新启动。上述情况下客户端都不能响应TCP。服务端将无法收到客户端对探测的响应。服务器总共发送10个这样的探测，每个间隔75秒。若服务器没有收到任何一个响应，它就认为客户端已经关闭并终止连接。 客户端崩溃并已经重新启动。服务器将收到一个对其保活探测的响应，这个响应是一个复位，使得服务器终止这个连接。 客户机正常运行，但是服务器不可达。这种情况与第二种状态类似。 4. 长连接和短连接的优点和缺点由上可以看出，长连接可以省去较多的TCP建立和关闭的操作，减少浪费，节约时间。对于频繁请求资源的客户来说，较适用长连接。不过这里存在一个问题，存活功能的探测周期太长，还有就是它只是探测TCP连接的存活，属于比较斯文的做法，遇到恶意的连接时，保活功能就不够使了。在长连接的应用场景下，client端一般不会主动关闭它们之间的连接，Client与server之间的连接如果一直不关闭的话，会存在一个问题，随着客户端连接越来越多，server早晚有扛不住的时候，这时候server端需要采取一些策略，如关闭一些长时间没有读写事件发生的连接，这样可 以避免一些恶意连接导致server端服务受损；如果条件再允许就可以以客户端机器为颗粒度，限制每个客户端的最大长连接数，这样可以完全避免某个蛋疼的客户端连累后端服务。 短连接对于服务器来说管理较为简单，存在的连接都是有用的连接，不需要额外的控制手段。但如果客户请求频繁，将在TCP的建立和关闭操作上浪费较多时间和带宽。 长连接和短连接的产生在于client和server采取的关闭策略，具体的应用场景采用具体的策略，没有十全十美的选择，只有合适的选择。 长连接短连接操作过程短连接的操作步骤是：建立连接——数据传输——关闭连接…建立连接——数据传输——关闭连接长连接的操作步骤是：建立连接——数据传输…（保持连接）…数据传输——关闭连接 5. 什么时候用长连接，短连接？长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。 而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连好。 总之，长连接和短连接的选择要视情况而定。]]></content>
      <categories>
        <category>HTTP</category>
      </categories>
      <tags>
        <tag>HTTP长连接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[究竟啥才是互联网架构“高并发”]]></title>
    <url>%2F2017%2F01%2F11%2F2017-01-11-high-concurrency%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 一、什么是高并发高并发（High Concurrency）是互联网分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计保证系统能够同时并行处理很多请求。 高并发相关常用的一些指标有响应时间（Response Time），吞吐量（Throughput），每秒查询率QPS（Query Per Second），并发用户数等。 响应时间：系统对请求做出响应的时间。例如系统处理一个HTTP请求需要200ms，这个200ms就是系统的响应时间。吞吐量：单位时间内处理的请求数量。QPS：每秒响应请求数。在互联网领域，这个指标和吞吐量区分的没有这么明显。并发用户数：同时承载正常使用系统功能的用户数量。例如一个即时通讯系统，同时在线量一定程度上代表了系统的并发用户数。 二、如何提升系统的并发能力互联网分布式架构设计，提高系统并发能力的方式，方法论上主要有两种：垂直扩展（Scale Up）与水平扩展（Scale Out）。 垂直扩展：提升单机处理能力。垂直扩展的方式又有两种：（1）增强单机硬件性能，例如：增加CPU核数如32核，升级更好的网卡如万兆，升级更好的硬盘如SSD，扩充硬盘容量如2T，扩充系统内存如128G；（2）提升单机架构性能，例如：使用Cache来减少IO次数，使用异步来增加单服务吞吐量，使用无锁数据结构来减少响应时间； 在互联网业务发展非常迅猛的早期，如果预算不是问题，强烈建议使用“增强单机硬件性能”的方式提升系统并发能力，因为这个阶段，公司的战略往往是发展业务抢时间，而“增强单机硬件性能”往往是最快的方法。 不管是提升单机硬件性能，还是提升单机架构性能，都有一个致命的不足：单机性能总是有极限的。所以互联网分布式架构设计高并发终极解决方案还是水平扩展。 水平扩展：只要增加服务器数量，就能线性扩充系统性能。水平扩展对系统架构设计是有要求的，如何在架构各层进行可水平扩展的设计，以及互联网公司架构各层常见的水平扩展实践，是本文重点讨论的内容。 三、常见的互联网分层架构 常见互联网分布式架构如上，分为：（1）客户端层：典型调用方是浏览器browser或者手机应用APP（2）反向代理层：系统入口，反向代理（3）站点应用层：实现核心应用逻辑，返回html或者json（4）服务层：如果实现了服务化，就有这一层（5）数据-缓存层：缓存加速访问存储（6）数据-数据库层：数据库固化数据存储 整个系统各层次的水平扩展，又分别是如何实施的呢？ 四、分层水平扩展架构实践反向代理层的水平扩展 反向代理层的水平扩展，是通过“DNS轮询”实现的：dns-server对于一个域名配置了多个解析ip，每次DNS解析请求来访问dns-server，会轮询返回这些ip。当nginx成为瓶颈的时候，只要增加服务器数量，新增nginx服务的部署，增加一个外网ip，就能扩展反向代理层的性能，做到理论上的无限高并发。 站点层的水平扩展 站点层的水平扩展，是通过“nginx”实现的。通过修改nginx.conf，可以设置多个web后端。当web后端成为瓶颈的时候，只要增加服务器数量，新增web服务的部署，在nginx配置中配置上新的web后端，就能扩展站点层的性能，做到理论上的无限高并发。 服务层的水平扩展 服务层的水平扩展，是通过“服务连接池”实现的。站点层通过RPC-client调用下游的服务层RPC-server时，RPC-client中的连接池会建立与下游服务多个连接，当服务成为瓶颈的时候，只要增加服务器数量，新增服务部署，在RPC-client处建立新的下游服务连接，就能扩展服务层性能，做到理论上的无限高并发。如果需要优雅的进行服务层自动扩容，这里可能需要配置中心里服务自动发现功能的支持。 数据层的水平扩展在数据量很大的情况下，数据层（缓存，数据库）涉及数据的水平扩展，将原本存储在一台服务器上的数据（缓存，数据库）水平拆分到不同服务器上去，以达到扩充系统性能的目的。 互联网数据层常见的水平拆分方式有这么几种，以数据库为例： 按照范围水平拆分 每一个数据服务，存储一定范围的数据，上图为例：user0库，存储uid范围1-1kwuser1库，存储uid范围1kw-2kw好处是：（1）规则简单，service只需判断一下uid范围就能路由到对应的存储服务；（2）数据均衡性较好；（3）比较容易扩展，可以随时加一个uid[2kw,3kw]的数据服务; 不足是：（1）请求的负载不一定均衡，一般来说，新注册的用户会比老用户更活跃，大range的服务请求压力会更大； 按照哈希水平拆分每一个数据库，存储某个key值hash后的部分数据，上图为例：user0库，存储偶数uid数据user1库，存储奇数uid数据好处是：（1）规则简单，service只需对uid进行hash能路由到对应的存储服务；（2）数据均衡性较好；（3）请求均匀性较好； 不足是：（1）不容易扩展，扩展一个数据服务，hash方法改变时候，可能需要进行数据迁移； 这里需要注意的是，通过水平拆分来扩充系统性能，与主从同步读写分离来扩充数据库性能的方式有本质的不同。 通过水平拆分扩展数据库性能：（1）每个服务器上存储的数据量是总量的1/n，所以单机的性能也会有提升；（2）n个服务器上的数据没有交集，那个服务器上数据的并集是数据的全集；（3）数据水平拆分到了n个服务器上，理论上读性能扩充了n倍，写性能也扩充了n倍（其实远不止n倍，因为单机的数据量变为了原来的1/n）； 通过主从同步读写分离扩展数据库性能：（1）每个服务器上存储的数据量是和总量相同；（2）n个服务器上的数据都一样，都是全集；（3）理论上读性能扩充了n倍，写仍然是单点，写性能不变； 缓存层的水平拆分和数据库层的水平拆分类似，也是以范围拆分和哈希拆分的方式居多，就不再展开。 五、总结高并发（High Concurrency）是互联网分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计保证系统能够同时并行处理很多请求。提高系统并发能力的方式，方法论上主要有两种：垂直扩展（Scale Up）与水平扩展（Scale Out）。前者垂直扩展可以通过提升单机硬件性能，或者提升单机架构性能，来提高并发性，但单机性能总是有极限的，互联网分布式架构设计高并发终极解决方案还是后者：水平扩展。 互联网分层架构中，各层次水平扩展的实践又有所不同：（1）反向代理层可以通过“DNS轮询”的方式来进行水平扩展；（2）站点层可以通过nginx来进行水平扩展；（3）服务层可以通过服务连接池来进行水平扩展；（4）数据库可以按照数据范围，或者数据哈希的方式来进行水平扩展；各层实施水平扩展后，能够通过增加服务器数量的方式来提升系统的性能，做到理论上的性能无限。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构师之路</tag>
        <tag>高并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP接入层的负载均衡、高可用、扩展性架构]]></title>
    <url>%2F2017%2F01%2F11%2F2017-05-07-architecture%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 一、web-server的负载均衡 互联网架构中，web-server接入一般使用nginx来做反向代理，实施负载均衡。整个架构分三层： 上游调用层，一般是browser或者APP 中间反向代理层，nginx 下游真实接入集群，web-server，常见web-server的有tomcat，apache 整个访问过程为： browser向daojia.com发起请求 DNS服务器将daojia.com解析为外网IP(1.2.3.4) browser通过外网IP(1.2.3.4)访问nginx nginx实施负载均衡策略，常见策略有轮询，随机，IP-hash等 nginx将请求转发给内网IP(192.168.0.1)的web-server 由于http短连接，以及web应用无状态的特性，理论上任何一个http请求落在任意一台web-server都应该得到正常处理（如果必须落在一台，说明架构不合理，不能水平扩展）。 问题来了，tcp是有状态的连接，客户端和服务端一旦建立连接，一个client发起的请求必须落在同一台tcp-server上，此时如何做负载均衡，如何保证水平扩展呢？ 二、单机法tcp-server 单个tcp-server显然是可以保证请求一致性： client向tcp.daojia.com发起tcp请求 DNS服务器将tcp.daojia.com解析为外网IP(1.2.3.4) client通过外网IP(1.2.3.4)向tcp-server发起请求 方案的缺点？无法保证高可用。 三、集群法tcp-server 通过搭建tcp-server集群来保证高可用，客户端来实现负载均衡： client内配置有tcp1/tcp2/tcp3.daojia.com三个tcp-server的外网IP 客户端通过“随机”的方式选择tcp-server，假设选择到的是tcp1.daojia.com 通过DNS解析tcp1.daojia.com 通过外网IP连接真实的tcp-server 如何保证高可用呢？如果client发现某个tcp-server连接不上，则选择另一个。 潜在的缺点？每次连接前，需要多实施一次DNS访问： 难以预防DNS劫持 多一次DNS访问意味着更长的连接时间，这个不足在手机端更为明显 如何解决DNS的问题？直接将IP配置在客户端，可以解决上述两个问题，很多公司也就是这么做的（俗称“IP直通车” ）。 “IP直通车”有什么新问题？将IP写死在客户端，在客户端实施负载均衡，扩展性很差： 如果原有IP发生变化，客户端得不到实时通知 如果新增IP，即tcp-sever扩容，客户端也得不到实时通知 如果负载均衡策略变化，需要升级客户端 四、服务端实施负载均衡只有将复杂的策略下沉到服务端，才能根本上解决扩展性的问题。 增加一个http接口，将客户端的“IP配置”与“均衡策略”放到服务端是一个不错的方案： client每次访问tcp-server前，先调用一个新增的get-tcp-ip接口，对于client而言，这个http接口只返回一个tcp-server的IP 这个http接口，实现的是原client的IP均衡策略 拿到tcp-server的IP后，和原来一样向tcp-server发起TCP长连接 这样的话，扩展性问题就解决了： 如果原有IP发生变化，只需要修改get-tcp-ip接口的配置 如果新增IP，也是修改get-tcp-ip接口的配置 如果负载均衡策略变化，需要升级客户端 然而，新的问题又产生了，如果所有IP放在客户端，当有一个IP挂掉的时候，client可以再换一个IP连接，保证可用性，而get-tcp-ip接口只是维护静态的tcp-server集群IP，对于这些IP对应的tcp-server是否可用，是完全不知情的，怎么办呢？ 五、tcp-server状态上报 get-tcp-ip接口怎么知道tcp-server集群中各台服务器是否可用呢，tcp-server主动上报是一个潜在方案，如果某一个tcp-server挂了，则会终止上报，对于停止上报状态的tcp-server，get-tcp-ip接口，将不返回给client相应的tcp-server的外网IP。 该设计的存在的问题？诚然，状态上报解决了tcp-server高可用的问题，但这个设计犯了一个“反向依赖”的耦合小错误：使得tcp-server要依赖于一个与本身业务无关的web-server。 六、tcp-server状态拉取 更优的方案是：web-server通过“拉”的方式获取各个tcp-server的状态，而不是tcp-server通过“推”的方式上报自己的状态。 这样的话，每个tcp-server都独立与解耦，只需专注于资深的tcp业务功能即可。 高可用、负载均衡、扩展性等任务由get-tcp-ip的web-server专注来执行。 多说一句，将负载均衡实现在服务端，还有一个好处，可以实现异构tcp-server的负载均衡，以及过载保护： 静态实施：web-server下的多个tcp-server的IP可以配置负载权重，根据tcp-server的机器配置分配负载（nginx也有类似的功能） 动态实施：web-server可以根据“拉”回来的tcp-server的状态，动态分配负载，并在tcp-server性能极具下降时实施过载保护 七、总结web-server如何实施负载均衡？利用nginx反向代理来轮询、随机、ip-hash。 tcp-server怎么快速保证请求一致性？单机。 如何保证高可用？客户配置多个tcp-server的域名。 如何防止DNS劫持，以及加速？IP直通车，客户端配置多个tcp-server的IP。 如何保证扩展性？服务端提供get-tcp-ip接口，向client屏屏蔽负载均衡策略，并实施便捷扩容。 如何保证高可用？tcp-server“推”状态给get-tcp-ip接口，or get-tcp-ip接口“拉”tcp-server状态。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构师之路</tag>
        <tag>高并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP接入层的负载均衡、高可用、扩展性架构]]></title>
    <url>%2F2017%2F01%2F11%2F2017-04-20-configuration%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 一、缘起随着互联网业务的越来越复杂，用户量与流量越来越大，“服务化分层”是架构演进的必由之路。 如上图：站点应用会调用服务，上游服务调用底层服务，依赖关系会变得非常复杂。 对于同一个服务，它有多个上游调用。为了保证高可用，一个底层服务往往是若干个节点形成一个集群提供服务。 如上图：用户中心服务user-service有三个节点，ip1/ip2/ip3对上游提供服务，任何一个节点当机，都不影响服务的可用性。 那么问题来了，当服务集群增减节点的时候，是否存在“反向依赖”，是否“耦合”，是否上游调用方需要修改配置重启，是否能做到上游无感知，即“配置的架构变迁”，是今天需要讨论的问题。 二、配置私藏“配置私藏”是配置文件架构的最初级阶段，上游调用下游，每个上游都有一个专属的私有配置文件，记录被调用下游的每个节点配置信息。 如上图： 用户中心user-service有ip1/ip2/ip3三个节点 service1调用了用户中心，它有一个专属配置文件s1.conf，里面配置了us的集群是ip1/ip2/ip3 service2也调用了用户中心，同理有个配置文件s2.conf，记录了us集群是ip1/ip2/ip3 web2也调用了用户中心，同理w2.conf，配置了us集群是ip1/ip2/ip3 是不是很熟悉？没错，绝大部分公司，初期都是这么玩的。 配置私藏架构的缺点是什么呢？ 来看一个容量变化的需求：1）运维检测出ip1节点的硬盘性能下降，通知研发未来要将ip1节点下线2）由于5月8日要做大促运营活动，未来流量会激增，研发准备增加两个节点ip4和ip5 此时要怎么做呢？ 需要用户中心的负责人通知所有上游调用者，修改“私藏”的配置，并重启上游，连接到新的集群上去。在ip1上没有流量之后，通知运维将ip1节点下线，以完成整个缩容扩容过程。 大伙是这么做的么？当业务复杂度较高，研发人数较多，服务依赖关系较复杂的时候，就没这么简单了。 问题一：调用方很痛，容量变化的是你，凭啥修改配置重启的是我？这是一个典型的“反向依赖”架构设计，上下游通过配置耦合，值得优化（特别是上层服务，ta依赖的服务很多的时候，可能每周都有类似的配合重启需求）。 问题二：服务方很痛，ta不知道有多少个上游调用了自己（特别是底层基础服务，像用户中心这种，调用ta的上游很多），往往只能通过以下方式来定位上游：a）群里吼b）发邮件询问c）通过连接找到ip，通过ip问运维，找到机器负责人，再通过机器负责人找到对应调用服务 不管哪种方式，都很有可能遗漏，导致ip1一直有流量难以下线，ip4/ip5的流量难以均匀迁移过来。该如何优化呢？ 三、全局配置架构的升级并不是一步到位的，先来用最低的成本来解决上述“修改配置重启”的问题一。 “全局配置”法：对于通用的服务，建立全局配置文件，消除配置私藏：1）运维层面制定规范，新建全局配置文件，例如/opt/globalconf/global.conf，如果配置较多，注意做好配置的垂直拆分2）对于服务方，如果是通用的服务，集群信息配置在global.conf里3）对于调用方，调用方禁止配置私藏，必须从global.conf里读取通用下游配置 这么做的好处：1）如果下游容量变化，只需要修改一处配置global.conf，而不需要各个上游修改2）调用方下一次重启的时候，自动迁移到扩容后的集群上来了3）修改成本非常小，读取配置文件目录变了 不足：如果调用方一直不重启，就没有办法将流量迁移到新集群上去了 有没有方面实现自动流量迁移呢？ 答案是肯定的，只需要实现两个并不复杂的组件，就能实现调用方的流量自动迁移：1）文件监控组件FileMonitor 作用是监控文件的变化，起一个timer，定期监控文件的ModifyTime或者md5就能轻松实现，当文件变化后，实施回调。2）动态连接池组件DynamicConnectionPooll“连接池组件”是RPC-client中的一个子组件l，用来维护与多个RPC-server节点之间的连接。所谓“动态连接池”，是指连接池中的连接可以动态增加和减少（用锁来互斥或者线程安全的数据结构很容易实现）。 这两个组件完成后：1）一旦全局配置文件变化，文件监控组件实施回调2）如果动态连接池组件发现配置中减少了一些节点，就动态的将对应连接销毁，如果增加了一些节点，就动态建立连接，自动完成下游节点的增容与缩容。 四、配置中心全局配置文件是一个能够快速落地的，解决“修改配置重启”问题的方案，但它仍然解决不了，服务提供方“不知道有多少个上游调用了自己”这个问题。 如果不知道多少上游调用了自己，“按照调用方限流”,“绘制全局架构依赖图”等需求便难以实现，怎么办，可以采用“配置中心”来解决。 对比“全局配置”与“配置中心”的架构图，会发现配置由静态的文件 升级为 动态的服务：1）整个配置中心子系统由zk、conf-center服务，DB配置存储与，conf-web配置后台组成2）所有下游服务的配置，通过后台设置在配置中心里3）所有上游需要拉取配置，需要去配置中心注册，拉取下游服务配置信息（ip1/ip2/ip3） 当下游服务需要扩容缩容时：4）conf-web配置后台进行设置，新增ip4/ip5，减少ip15）conf-center服务将变更的配置推送给已经注册关注相关配置的调用方6）结合动态连接池组件，完成自动的扩容与缩容 配置中心的好处：1）调用方不需要再重启2）服务方从配置中心中很清楚的知道上游依赖关系，从而实施按照调用方限流3）很容易从配置中心得到全局架构依赖关系痛点一、痛点二同时解决。 不足：系统复杂度相对较高，对配置中心的可靠性要求较高，一处挂全局挂。 五、总结解决什么问题？配置导致系统耦合，架构反向依赖。 什么痛点？上游痛：扩容的是下游，改配置重启的是上游下游痛：不知道谁依赖于自己 配置架构如何演进？一、配置私藏二、全局配置文件三、配置中心]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构师之路</tag>
        <tag>高并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedHashMap分析(Java8)]]></title>
    <url>%2F2016%2F12%2F15%2F2016-12-15-linkedhashmap%2F</url>
    <content type="text"><![CDATA[LinkedHashMap的实现在Java8中都有了不少的变化，本文将介绍LinkedHashMap在Java8中的内部结构和实现原理 一、初识LinkedHashMapHashMap是无序的，迭代HashMap的顺序并不是HashMap放置的顺序。HashMap的这一缺点往往会带来困扰，因为有些场景，我们期待一个有序的Map。这个时候，LinkedHashMap就闪亮登场了，它虽然增加了时间和空间上的开销，但是通过维护一个运行于所有条目的双向链表，LinkedHashMap保证了元素迭代的顺序。该迭代顺序可以是插入顺序或者是访问顺序。 二、四个关注点在LinkedHashMap上的答案 关 注 点 结 论 LinkedHashMap是否允许空 Key和Value都允许空 LinkedHashMap是否允许重复数据 Key重复会覆盖、Value允许重复 LinkedHashMap是否有序 有序 LinkedHashMap是否线程安全 非线程安全 三、LinkedHashMap基本结构关于LinkedHashMap，先提两点： 1、LinkedHashMap可以认为是HashMap+LinkedList，即它既使用HashMap操作数据结构，又使用LinkedList维护插入元素的先后顺序。 2、LinkedHashMap的基本实现思想就是—-多态。可以说，理解多态，再去理解LinkedHashMap原理会事半功倍；反之也是，对于LinkedHashMap原理的学习，也可以促进和加深对于多态的理解。 为什么可以这么说，首先看一下，LinkedHashMap的定义：123456public class LinkedHashMap&lt;K,V&gt; extends HashMap&lt;K,V&gt; implements Map&lt;K,V&gt;&#123; ...&#125; 看到，LinkedHashMap是HashMap的子类，自然LinkedHashMap也就继承了HashMap中所有非private的方法。再看一下LinkedHashMap中本身的方法： 看到LinkedHashMap中并没有什么操作数据结构的方法，也就是说LinkedHashMap操作数据结构（比如put一个数据），和HashMap操作数据的方法完全一样，无非就是细节上有一些的不同罢了。 LinkedHashMap只定义了三个属性： 12345678910111213141516171819/** * The head (eldest) of the doubly linked list. * 双向链表的头节点(最旧节点) */transient LinkedHashMap.Entry&lt;K,V&gt; head;/** * The tail (youngest) of the doubly linked list. * 双向链表的头节点(最旧节点) */transient LinkedHashMap.Entry&lt;K,V&gt; tail;/** * The iteration ordering method for this linked hash map: &lt;tt&gt;true&lt;/tt&gt; * for access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order. *true表示最近最少访问次序，false表示插入顺序 * @serial */final boolean accessOrder; 值得注意的是，tail属性是JDK1.8才加入的，在之前JDK版本中只有head属性。 LinkedHashMap一共提供了五个构造方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/** * Constructs an empty insertion-ordered &lt;tt&gt;LinkedHashMap&lt;/tt&gt; instance * with the default initial capacity (16) and load factor (0.75). * 构造方法1，用默认的初始化容量(16)和负载因子(0.75)创建一个LinkedHashMap，取得键值对的顺序是插入顺序 */public LinkedHashMap() &#123; super(); accessOrder = false;&#125;/** * Constructs an empty insertion-ordered &lt;tt&gt;LinkedHashMap&lt;/tt&gt; instance * with the specified initial capacity and a default load factor (0.75). * 构造方法2，构造一个指定初始容量的LinkedHashMap，取得键值对的顺序是插入顺序 * @param initialCapacity the initial capacity * @throws IllegalArgumentException if the initial capacity is negative */ public LinkedHashMap(int initialCapacity) &#123; super(initialCapacity); accessOrder = false; &#125; /** * Constructs an empty insertion-ordered &lt;tt&gt;LinkedHashMap&lt;/tt&gt; instance * with the specified initial capacity and load factor. * 构造方法3，构造一个指定初始容量和负载因子的、按照插入顺序的LinkedList * @param initialCapacity the initial capacity * @param loadFactor the load factor * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public LinkedHashMap(int initialCapacity, float loadFactor) &#123; super(initialCapacity, loadFactor); accessOrder = false; &#125;/** * Constructs an empty &lt;tt&gt;LinkedHashMap&lt;/tt&gt; instance with the * specified initial capacity, load factor and ordering mode. *构造方法4，根据指定容量、装载因子和键值对保持顺序创建一个LinkedHashMap * @param initialCapacity the initial capacity * @param loadFactor the load factor * @param accessOrder the ordering mode - &lt;tt&gt;true&lt;/tt&gt; for * access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) &#123; super(initialCapacity, loadFactor); this.accessOrder = accessOrder; &#125;/** * Constructs an insertion-ordered &lt;tt&gt;LinkedHashMap&lt;/tt&gt; instance with * the same mappings as the specified map. The &lt;tt&gt;LinkedHashMap&lt;/tt&gt; * instance is created with a default load factor (0.75) and an initial * capacity sufficient to hold the mappings in the specified map. * 构造方法5，通过传入的map创建一个LinkedHashMap * @param m the map whose mappings are to be placed in this map * @throws NullPointerException if the specified map is null */ public LinkedHashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; super(); accessOrder = false; putMapEntries(m, false); &#125; 从构造方法中可以看出，默认都采用插入顺序来维持取出键值对的次序（accessOrder=false）。所有构造方法都是通过调用父类的构造方法来创建对象的，LinkedHashMap的初始化并没有做什么额外的工作。 LinkedHashMap和HashMap的区别在于它们的基本数据结构上，看一下LinkedHashMap的基本数据结构，也就是Entry：123456789/** * HashMap.Node subclass for normal LinkedHashMap entries. */static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; super(hash, key, value, next); &#125;&#125; 列一下Entry里面有的一些属性吧： 1、K key 2、V value 3、Entry next 4、int hash 5、Entry before 6、Entry after 其中前面四个，也就是红色部分是从HashMap.Entry中继承过来的；后面两个，也就是蓝色部分是LinkedHashMap独有的。不要搞错了next和before、After，next是用于维护HashMap指定table位置上连接的Entry的顺序的，before、After是用于维护Entry插入的先后顺序的。 LinkedHashMap的属性如下： 第一张图为LinkedHashMap整体结构图，第二张图专门把双向链表（注意:不是循环链表）抽取出来。顾名思义，head和tail分别指向链表头结点和尾节点。代器遍历方向是从链表的头部开始到链表尾部结束，对于两种迭代方式节点的存储情况：&gt; 插入顺序(accessOrder=false)双向链表的头部存放最先插入的节点，尾部是最近插入的节点。 访问顺序(accessOrder=false)访问是指修改(覆盖)元素或者获取元素(注意:插入不算哦)。双向链表的头部存放最久没有被访问的元素，尾部为最近访问或最近插入的元素。 四、LinkedHashMap存储数据LinkedHashMap并未重写父类HashMap的put方法，而是重写了put方法调用的newNode，afterNodeAccess和afterNodeInsertion，以提供自己特有的双向链接列表的实现。 HashMap的put方法：12345678910111213141516171819202122232425262728293031323334353637383940414243444546public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 可以看到，在put方法中，在新增元素时调用newNode方法，下面是LinkedHashMap中重写的newNode方法：123456789101112131415161718Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = new LinkedHashMap.Entry&lt;K,V&gt;(hash, key, value, e); linkNodeLast(p); return p;&#125;// link at the end of listprivate void linkNodeLast(LinkedHashMap.Entry&lt;K,V&gt; p) &#123; LinkedHashMap.Entry&lt;K,V&gt; last = tail; tail = p; if (last == null) head = p; else &#123; p.before = last; last.after = p; &#125; &#125; newNode方法中创建Entry节点，并把其插入到双向链表的尾部。 123456789101112131415161718192021222324void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; if (accessOrder &amp;&amp; (last = tail) != e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else &#123; p.before = last; last.after = p; &#125; tail = p; ++modCount; &#125; &#125; 当需要key值已存在，需要覆盖旧元素时会调用afterNodeAccess方法，这个方法会把覆盖元素移动到链表最尾。 1234567void afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true); &#125;&#125; 在插入新元素后会调用afterNodeInsertion，它的作用是存储元素实现先进先出(插入顺序)或LRU（访问顺序）功能。我们可以通过实现预留的removeEldestEntry方法，定义移除元素的条件。当removeEldestEntry返回true的时候，将删除头节点(最先插入或最久没被访问的节点)。例如下面的代码就是实现当map的size达到100时，每次有新元素插入时删掉最旧或最少访问的元素，使map保存的元素维持在100个。12345678public class LRUCache extends LinkedHashMap&#123; private static final int MAX_ENTRIES = 100; protected boolean removeEldestEntry(Map.Entry eldest) &#123; return size() &gt; MAX_ENTRIES; &#125;&#125; 五、LinkedHashMap获取数据LinkedHashMap重写了get方法,如果是按访问循序迭代，则调用afterNodeAccess，把当前访问节点移到链表最尾。12345678public V get(Object key) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return null; if (accessOrder) afterNodeAccess(e); return e.value;&#125; 六、LinkedHashMap删除数据LinkedHashMap没有重写remove方法，而是重写了afterNodeRemoval方法,每当删除元素的时候，都会同时把元素从双向链表中删除。12345678910111213void afterNodeRemoval(Node&lt;K,V&gt; e) &#123; // unlink LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.before = p.after = null; if (b == null) head = a; else b.after = a; if (a == null) tail = b; else a.before = b;&#125;]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>JAVA集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring framework模块maven依赖关系]]></title>
    <url>%2F2016%2F12%2F06%2F2016-12-06-spring-jar%2F</url>
    <content type="text"><![CDATA[很多人都在用spring开发java项目，但是配置maven依赖的时候并不能明确要配置哪些spring的jar，经常是胡乱添加一堆，编译或运行报错就继续配置jar依赖，导致spring依赖混乱，甚至下一次创建相同类型的工程时也不知道要配置哪些spring的依赖，只有拷贝，其实，当初我就是这么干的！ spring的jar包只有20个左右，每个都有相应的功能，一个jar还可能依赖了若干其他jar，所以，搞清楚它们之间的关系，配置maven依赖就可以简洁明了，下面举个例子，要在普通java工程使用spring框架，需要哪些jar呢？其实只要一个:12345&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;3.2.17.RELEASE&lt;/version&gt;&lt;/dependency&gt; 那要在web工程中引入spring mvc呢？也只要配置一个依赖:12345&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;3.2.17.RELEASE&lt;/version&gt;&lt;/dependency&gt; 为什么可以这样配置？接下来我们以spring 3.2.17.RELEASE版本为例，介绍spring框架结构，spring 4稍有不同，将在最后介绍。spring官网给出了一张spring3的结构图: 图中将spring分为5个部分：core、aop、data access、web、test，图中每个圆角矩形都对应一个jar，如果在maven中配置，所有这些jar的“groupId”都是“org.springframework”，每个jar有一个不同的“artifactId”，另外，“instrumentation”有两个jar，还有一个“spring-context-support”图中没有列出，所以spring3的jar包一共是19个。 Spring3 jar包依赖关系。corecore部分包含4个模块 spring-core：依赖注入IoC与DI的最基本实现 spring-beans：Bean工厂与bean的装配 spring-context：spring的context上下文即IoC容器 spring-expression：spring表达式语言 它们的完整依赖关系: 因为spring-core依赖了commons-logging，而其他模块都依赖了spring-core，所以整个spring框架都依赖了commons-logging，如果有自己的日志实现如log4j，可以排除对commons-logging的依赖，没有日志实现而排除了commons-logging依赖，编译会报错1234567891011&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;3.2.17.RELEASE&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; aopaop部分包含4个模块 spring-aop：面向切面编程 spring-aspects：集成AspectJ spring-instrument：提供一些类级的工具支持和ClassLoader级的实现，用于服务器 spring-instrument-tomcat：针对tomcat的instrument实现 它们的依赖关系 data accessdata access部分包含5个模块 spring-jdbc：jdbc的支持 spring-tx：事务控制 spring-orm：对象关系映射，集成orm框架 spring-oxm：对象xml映射 spring-jms：java消息服务 它们的依赖关系 webweb部分包含4个模块 spring-web：基础web功能，如文件上传 spring-webmvc：mvc实现 spring-webmvc-portlet：基于portlet的mvc实现 spring-struts：与struts的集成，不推荐，spring4不再提供 testtest部分只有一个模块，我将spring-context-support也放在这吧 spring-test：spring测试，提供junit与mock测试功能 spring-context-support：spring额外支持包，比如邮件服务、视图解析等 到这里，spring3的介绍就完了，看着这些图我相信你在maven中配置spring依赖时不会再混乱了 Spring4 jar包依赖关系。下面介绍spring4，与spring3结构基本相同，下面是官网给出的结构图 可以看到，图中去掉了spring3的struts，添加了messaging和websocket，其他模块保持不变，因此，spring4的jar有20个。 spring-websocket：为web应用提供的高效通信工具 spring-messaging：用于构建基于消息的应用程序]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>spring framework</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有感]]></title>
    <url>%2F2016%2F11%2F03%2F2016-11-03-blog%2F</url>
    <content type="text"><![CDATA[建这个博客初衷是记录自己技术成长的点滴，但生活中总有一些事一些情，会让我在某一瞬间或脑洞大开，或有所顿悟，或不吐不快（他喵不就是想吐槽?) 于是乎决定增加一个&lt;有感&gt;的栏目分类，来聊聊自己生活中的一些感悟和看法。这个名字是模仿现在流行的二字命名法，如一条、二更、咪蒙等，以达到最佳装逼效果。 至于内容，我的定位是不求字数质量，但求记录(没人看干嘛让自己那么累)，等日后老了聊以自慰(不是你想的那个自慰)。 话说这年头，还写blog记录生活的人都是人中龙凤了。这个追求短平快，娱乐至死的年代，能发一句话、一张图片、一段视频表达的事，谁又愿意写或者看一大段文字呢？ 另一个关键是，内容越少，剩下的想象空间就越大，越容易完成不经意装逼的终极目标。这里面的技巧和例子很多，详情参见你的朋友圈。 （完）]]></content>
      <categories>
        <category>有感</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[微服务架构多“微”才合适？]]></title>
    <url>%2F2016%2F08%2F11%2F2016-08-11-micro-service%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 一、互联网架构为什么要进行服务化-总结上一篇和大伙交流了一下，随着数据量、并发量、业务复杂度的增长，互联网架构会出现以下问题：（1）代码到处拷贝（2）底层复杂性扩散（3）基础库（so/jar/dll）耦合（4）SQL质量得不到保障，业务相互影响（5）数据库耦合 “服务化”是一个很好的解决上述痛点的方案。 不少评论也提出了不少有建设性的观点，汇总出来分享给大伙： @田卫 同学提到：服务化之后，可能会引发分布式事务的问题，“没人愿意引入分布式事务，当基于业务水平拆分的时候，要业务专家介入，合理拆分服务化，以后就服务内高内聚，事务可以保证，对于跨服务调用，通过补偿等手段，只要最终一致性就行，毕竟连现在的银行转账都不是强一致性。” 如@田卫所说，分布式事务是业界没有彻底解决的难题，任何架构设计都是一个折衷，吞吐量？时延？一致性？哪个是主要矛盾，优先解决哪个问题。大数据、高并发、业务复杂性是主要矛盾的时候，或许“最终一致性”是一个替代“事务”更好的，或者说业务能够接受的方案。 @侯滇滇 同学提到：多了一层服务层，架构实际上是更复杂了，需要引入一系列机制对服务进行管理，RPC服务化中需要注意：（1）RPC服务超时，服务调用者应有一些应对策略，比如重发（2）关键服务例如支付，要注意幂等性，因为重发会导致重复操作（3）多服务要考虑并发操作，相当单服务的锁机制比如JAVA中的synchronized @黄明 同学提到：服务化之后，随着规模的扩大，一定要考虑“服务治理”，否则服务之间的依赖关系会乱成麻 二、互联网微服务架构多“微”才适合大家也都认可，随着数据量、流量、业务复杂度的提升，服务化架构是架构演进中的必由之路，今天要讨论的话题是：微服务架构多“微”才合适？ 【粗粒度：一个服务层】 最粗犷的玩法，所有基础数据的访问，都通过一个service访问，在业务不是特别复杂的时候还好，一旦业务变复杂了，这个service层会变得非常重，成为耦合点之一，以微信场景为例，假设有一个通用的服务层来访问基础数据，这个服务层可能是这样的： 有一个统一的service层，用户信息，好友信息，群组信息，消息信息都通过这个service层来走。细节：微信单对单消息是一个写多读少的业务，故没有缓存。 【一个子业务一个service】如果所有的信息存储都在一个service里，那么一个地方出bug，就将影响整个业务，所以更合理的做法是在服务层进行细分，架构如何细分？ 垂直拆分是个好的方案，将子业务一个个拆出来 ，那么微信的服务化架构或许会变成这个样子： （1）用户相关的子业务有user-service（2）好友相关的子业务有friend-service（3）群组相关的子业务有group-service（4）消息相关的子业务有msg-service 这样的话，一个service出问题也不会影响其他service，同时数据层也按照业务垂直拆分开了。服务粒度变细之后，出现一个新的问题，业务与服务的连接关系变复杂了，有什么好的优化方案么？ 常见的，加入一个高可用服务分发层集群，并在协议设计时加入服务号，可以减少蜘蛛网状的依赖关系：（1）调用方依赖分发层，传入服务号（2）分发层依赖服务层，通过服务号参数分发 【一个数据库表对应一个service】数据访问service最初是从DAO/ORM的数据访问需求过来的，所以有些公司也有一个数据表一个service的玩法。一个子业务对应一个service的玩法是： （1）服务层，整个群业务是一个service（2）存储层，实际可能对应了群信息、群成员、群消息等多个数据表 拆分成一个数据表一个service，则架构会变成这样： 群信息表，群成员表，群消息表等各个数据表之间也解耦开了，不会相互影响了。 【一个接口对应一个service】微服务架构中更极端的，甚至一个接口对应一个微服务，这样的话，架构就从： 演化为：（1）修改群信息服务（2）增加群信息服务（3）获取群信息服务多个服务操纵同一个数据表，使用同一片缓存，每个接口出问题，都不会影响其他接口。 三、粒度粗细的优劣上文中谈到的服务化与微服务，不同粒度的服务化各有什么优劣呢？总的来说，细粒度拆分的优点有：（1）服务都能够独立部署（2）扩容和缩容方便，有利于提高资源利用率（3）拆得越细，耦合相对会减小（4）拆得越细，容错相对会更好，一个服务出问题不影响其他服务（5）扩展性更好（6）… 细粒度拆分的不足也很明显：（1）拆得越细，系统越复杂（2）系统之间的依赖关系也更复杂（3）运维复杂度提升（4）监控更加复杂（5）出问题时定位问题更难（6）… 四、结束的话聊了许多，有网友问，笔者对待服务化以及微服务粒度的看法，个人觉得，以“子业务系统”粒度作为微服务的单位是比较合适的： 末了，讨论完微服务架构的粒度，后续文章和大家聊一聊微服务的最佳实践，需要什么样的框架、组件、技术能够将服务化在较短的时间内开展起来，下周和大伙再聊。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>服务化</tag>
        <tag>架构师之路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[互联网架构为什么要做服务化？]]></title>
    <url>%2F2016%2F08%2F10%2F2016-08-10-service%2F</url>
    <content type="text"><![CDATA[转自: 架构师之路 近期参加一些业界的技术大会，“微服务架构”的话题非常之火，也在一些场合聊过服务化架构实践，最近几期文章期望用通俗易懂的语言聊聊了个人对服务化以及微服务架构的理解，希望能给大伙一些启示。如果有遗漏，也欢迎大家补充。 一、互联网高可用架构，为什么要服务化？【服务化之前高可用架构】在服务化之前，互联网的高可用架构大致是这样一个架构： （1）用户端是浏览器browser，APP客户端（2）后端入口是高可用的nginx集群，用于做反向代理（3）中间核心是高可用的web-server集群，研发工程师主要编码工作就是在这一层（4）后端存储是高可用的db集群，数据存储在这一层 更典型的，web-server层是通过DAO/ORM等技术来访问数据库的。 可以看到，最初都是没有服务层的，此时架构会碰到一些什么痛点呢？ 【架构痛点一：代码到处拷贝】举一个最常见的业务的例子-&gt;用户数据的访问，绝大部分公司都有一个数据库存储用户数据，各个业务都有访问用户数据的需求： 在有用户服务之前，如何让数据的获取更加高效快捷呢？ 各个业务线都是自己通过DAO写SQL访问user库来存取用户数据，这无形中就导致了代码的拷贝。 【架构痛点二：复杂性扩散】随着并发量的越来越高，用户数据的访问数据库成了瓶颈，需要加入缓存来降低数据库的读压力，于是架构中引入了缓存，由于没有统一的服务层，各个业务线都需要关注缓存的引入导致的复杂性： 对于用户数据的写请求，所有业务线都要升级代码：（1）先淘汰cache（2）再写数据 对于用户数据的读请求，所有业务线也都要升级代码：（1）先读cache，命中则返回（2）没命中则读数据库（3）再把数据放入cache 这个复杂性是典型的“业务无关”的复杂性，业务方需要被迫升级。 随着数据量的越来越大，数据库需要进行水平拆分，于是架构中又引入了分库分表，由于没有统一的服务层，各个业务线都需要关注分库分表的引入导致的复杂性： 这个复杂性也是典型的“业务无关”的复杂性，业务方需要被迫升级。包括bug的修改，发现一个bug，多个地方都需要修改。 【架构痛点三：库的复用与耦合】服务化并不是唯一的解决上述两痛点的方法，抽象出统一的“库”是最先容易想到的解决：（1）代码拷贝（2）复杂性扩散的方法。抽象出一个user.so，负责整个用户数据的存取，从而避免代码的拷贝。至于复杂性，也只有user.so这一个地方需要关注了。 解决了旧的问题，会引入新的问题，库的版本维护与业务线之间代码的耦合：业务线A将user.so由版本1升级至版本2，如果不兼容业务线B的代码，会导致B业务出现问题；业务线A如果通知了业务线B升级，则是的业务线B会无故做一些“自身业务无关”的升级，非常郁闷。当然，如果各个业务线都是拷贝了一份代码则不存在这个问题。 【架构痛点四：SQL质量得不到保障，业务相互影响】业务线通过DAO访问数据库： 本质上SQL语句还是各个业务线拼装的，资深的工程师写出高质量的SQL没啥问题，经验没有这么丰富的工程师可能会写出一些低效的SQL，假如业务线A写了一个全表扫描的SQL，导致数据库的CPU100%，影响的不只是一个业务线，而是所有的业务线都会受影响。 【架构痛点五：疯狂的DB耦合】业务线不至访问user数据，还会结合自己的业务访问自己的数据： 典型的，通过join数据表来实现各自业务线的一些业务逻辑。这样的话，业务线A的table-user与table-A耦合在了一起，业务线B的table-user与table-B耦合在了一起，业务线C的table-user与table-C耦合在了一起，结果就是：table-user，table-A，table-B，table-C都耦合在了一起。 随着数据量的越来越大，业务线ABC的数据库是无法垂直拆分开的，必须使用一个大库（疯了，一个大库300多个业务表 =_=）。 二、服务化解决什么问题？为了解决上面的诸多问题，互联网高可用分层架构演进的过程中，引入了“服务层”。 以上文中的用户业务为例，引入了user-service，对业务线响应所用用户数据的存取。引入服务层有什么好处，解决什么问题呢？ 【好处一：调用方爽】有服务层之前：业务方访问用户数据，需要通过DAO拼装SQL访问有服务层之后：业务方通过RPC访问用户数据，就像调用一个本地函数一样，非常之爽 User = UserService::GetUserById(uid);传入一个uid，得到一个User实体，就像调用本地函数一样，不需要关心序列化，网络传输，后端执行，网络传输，范序列化等复杂性。 【好处二：复用性，防止代码拷贝】这个不展开叙述，所有user数据的存取，都通过user-service来进行，代码只此一份，不存在拷贝。升级一处升级，bug修改一处修改 。 【好处三：专注性，屏蔽底层复杂度】 在没有服务层之前，所有业务线都需要关注缓存、分库分表这些细节。 【好处四：SQL质量得到保障】 原来是业务向上游直接拼接SQL访问数据库。 有了服务层之后，所有的SQL都是服务层提供的，业务线不能再为所欲为了 。底层服务对于稳定性的要求更好的话，可以由更资深的工程师维护，而不是像原来SQL难以收口，难以控制。 【好处五：数据库解耦】 原来各个业务的数据库都混在一个大库里，相互join，难以拆分。 服务化之后，底层的数据库被隔离开了，可以很方便的拆分出来，进行扩容。 【好处六：提供有限接口，无限性能】在服务化之前，各业务线上游想怎么操纵数据库都行，遇到了性能瓶颈，各业务线容易扯皮，相互推诿。服务化之后，服务只提供有限的通用接口，理论上服务集群能够提供无限性能，性能出现瓶颈，服务层一处集中优化。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>服务化</tag>
        <tag>架构师之路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[缓存之旅(一)]]></title>
    <url>%2F2016%2F05%2F15%2F2016-05-15-cache-1%2F</url>
    <content type="text"><![CDATA[缓存真的有效？真的。嗯，根据计算机访问数据经常会呈现出的局部性原理。局部性原理又包括空间局部性和时间局部性。空间局部性就是说，计算机访问数据，而其存储在邻近的数据也经常会被访问。时间局部性就是说，在相对的一小段时间内，计算机经常会访问相同的数据。实际中是怎么运用局部性原理的呢，比如说，计算机从硬盘中读块，计算机不会只读你要的特定块，附近的快很有可能接下来要被访问，他会把这些块也一起预读出来。接下来要读附近的快的时候，就不需要再访问硬盘了。这样，运用局部性原理就减少了访问磁盘的次数。附近的块就被缓存了起来，加快了运行速度。 缓存什么？所有处理需要相对较长时间的内容都可以缓存，比如说，将图像显示到屏幕上，图像解码相对于渲染所需时间较长，我们就会缓存图像解码。再比如，客户端从服务器获取数据相对计算所需时间较长，我们就会缓存从服务器获取的数据。这样最终达到速度匹配，让整个处理过程中，没有哪步处理时间太长。 若想理解缓存，莫过于动手实现一个缓存。下面就以最常见的，客户端从服务器获取数据为引子，来聊一聊缓存。 当然，我们很想在客户端缓存下服务器端所有的东西。 但我们知道缓存下所有东西必定不可能，不仅你没有那么大的存储器，而且缓存会大大增加你编程的复杂性，所以缓存必定就是一个trade-Off的存在，权衡各种利弊，无法量化的时候甚至就靠直觉和经验了。好，我们有了一个Basic Idea，如何开始呢？ 缓存这个用空间换时间的概念存在着计算机的各个领域，cpu、操作系统、计算机网络、数据库。从这些领域我们可以借鉴他们是如何实现缓存的，然后再来考虑实现自己的缓存。缓存是分层次的，下面是计算机缓存山 每一层实际上都可以看做下一层的高速缓存，从山顶到山脚，计算机访问到的时间递增，而每一层的物理硬件造价递减，cpu计算数据先从山顶开始找数据，如果本层没有找到就去下层找，每向下找一层，找的层数越多，计算所需的时间自然就越多。 如何找到对应的缓存？索引+映射。为缓存的内容增加一个索引。对于cpu运算的数据，索引是按地址划分出来的，对于应用层来说索引就是缓存的key值。索引可以分为一对一相联、组相联、全相联。索引构成了一个的集合，缓存构成另一个集合，这两个集合之间有映射关系，直接从索引集合查找就可以找到对应的缓存了。那为什么不直接从缓存集合找呢？假设缓存容量有4KB，每个缓存大小为16B，那么一共有256个缓存。缓存的索引范围就是0到255，索引集合占256B。如果从索引集合查找，只需遍历256B的空间，从缓存集合查找需要遍历4KB的空间，明显索引集合可以加快查找的速度。这也就是为什么用一个小的空间去映射大的空间。 cpu缓存策略cpu在寄存器中计算数据，而数据存储在内存中，由于cpu和内存之间的性能差距逐渐增大，系统设计者在cpu和内存之间插入了3层的高速缓存。高速缓存有三个层级，就是整个计算机缓存系统的一个小缩影。 缓存涉及到，读操作、写操作和层级之间如何协调、缓存容量满了后的淘汰算法。淘汰下面会讲，这里关注一下读写操作和层级之间的协调。 高速缓存的读很简单，先从高层读数据，如果缓存命中了就返回数据。如果不命中就去低层读，如果从低层命中，返回数据的同时将低层的数据写入高层。 高速缓存的写复杂一点，先直接向高层写入数据，但是何时向低层写入呢？一种是直写（write-through），就是立即向低层写入。另一种是写回（write-back），等到算法淘汰的时候再向底层写入。直写实现起来简单，但是每次写入都会有更多的总线流量。第二种，减少了总线流量，增加了复杂度，他必须为每个缓存对象保存是否修改（dirty位），即是否写入低层。向低层写，时间消耗主要在访问的时间上，每次写的量多少，差别不大。高速缓存就是使用的写回，Mongo也是写回。本文推荐缓存使用写回。 抽象块理解操作系统的缓存策略之前，有一个重要的概念就是抽象块。抽象块呢，主要就在抽象两字上。而抽象主要的目的是为了隐藏不同层次的细节。比如，硬盘传输数据给内存，硬盘传输的是一个块（block），这个块就是对于硬盘的抽象，硬盘要想找到数据，必须进行磁盘的旋转和寻道，内存根本不关心，硬盘旋转了几圈还是数据在那条道上，内存只关心数据是什么，所以，硬盘只给内存一个块（block），硬盘向内存隐藏如何存取的细节。同样的思想也在网络五层协议中，每层都给高层或底层一个“块”(实际上叫包，packet)，本层不关心其他层的细节，本层直接在块上头部和尾部加上自己层做的事，然后传给高层或者低层，应用层管本层的块叫报文，传输层叫报文段，网络层叫数据报。 操作系统缓存策略在操作系统中，硬盘给内存的抽象块就是页（page）。从磁盘上读取页导致的I/O是很耗时间的，所以页就被缓存在内存中，这就是页的缓存。操作系统调用文件系统就使用这种页缓存。简单来说，内存中的页也就成了文件系统的缓存。 接下来看一下linux的cache 图中主要有三个关键部分，内存管理系统、虚拟文件系统（VFS）、文件系统，页实际上将他们联系在一起，文件系统负责将页从硬盘读出或写入硬盘，虚拟文件系统负责将页传递给内存管理系统和从内存管理系统接收页，内存负责管理页的分配或回收和置换策略。内存管理系统如何管理就是我们需要关注的。 页表不一定只映虚拟页，还有可能是文件本身，这样就相当于将文件直接载入了内存，直接访问内存就直接访问文件了，这个过程叫mmap(Memory Map)。MongDB使用的就是mmap，详见:https://docs.mongodb.org/manual/faq/storage/ linux的文件cache分为page cache和buffer cache。实际上，一个page cache对应多个buffer cache。 这两个东西容易傻傻分不清楚，将他们分开很有必要。首先，page cache和buffer cache在不同层次。page cache在vfs和内存管理系统数据交换，这一文件系统逻辑层次上。而buffer cache在具体文件系统，这一文件系统物理层次上。其次page cache和buffer cache不是为了同一个目的，page cache就是页，作为文件系统在内存上的缓存。buffer cache是缓冲区，读写磁盘的时候为了减少I/O的数量，攒齐一定量的数据再写和攒齐一定数量读出的数据再返回。 如何取出页的物理地址。他会先看虚拟页是否有映射物理页，如果有映射，就直接返回，如果没有映射，就会发生缺页中断。将相应的物理页缓存在内存，并与虚拟页映射。再进行一次查找，这次查找虚拟页就会映射物理页。但是，内存容量有限，不能让所有物理页都缓存，需要用算法淘汰掉不需要的物理页。 FIFO：先进先出算法，每次淘汰掉停留时间最长的算法，这种算法并不常用，因为他很可能淘汰掉经常使用的页面。 LRU(Least Recently Used)：选择最近一段时间内未使用过的页面置换掉。这种算法非常优秀，但是操作系统实现它还需要硬件支持，实现起来相当费时，所以又涌现了很多模仿LRU的算法，更加实用，NRU、NFU、2Q、MQ、Clock等。 数据库缓存策略和操作系统缓存策略相似，数据库将块缓存在内存上，叫做缓冲区（buffer），由缓存区管理器管理，大多数数据库使用的算法为近似LRU算法。 数据库缓存为了在崩溃后，也要保持一致性，有时会将块强制写回，有时会限制块的写回。 让Idea不仅仅是Idea现在着手实现一个缓存，根据具体缓存的东西，我们可以有不同的策略，在不同的语言下，有不同的实现方式，不过，思想是一致的。 我们实际能调用的缓存只有两个层级，内存和硬盘。那么就实现一个二级缓存。 首先，我们选择一个内存缓存的数据结构，最简单的就是对哈希表的封装—字典，拥有常数级别的查询复杂度。但是如果要再加上淘汰算法，字典的复杂度就不理想了。而iOS中有系统级已实现好的NSCache，但是其效率不高，虽说提供了更多的功能，但你也可以自己实现这些功能。本文推荐大家使用LinkedHashMap，等到下文聊到淘汰算法的时候会细讲。最终，根据你要存储的内容、复杂性和性能之间的权衡，选择不同的实现方式。 首先，我们选择一个内存缓存的数据结构，最简单的就是对哈希表的封装—字典，拥有常数级别的查询复杂度。但是如果要再加上淘汰算法，字典的复杂度就不理想了。而iOS中有系统级已实现好的NSCache，但是其效率不高，虽说提供了更多的功能，但你也可以自己实现这些功能。本文推荐大家使用LinkedHashMap，等到下文聊到淘汰算法的时候会细讲。最终，根据你要存储的内容、复杂性和性能之间的权衡，选择不同的实现方式。 我们还要确定缓存的容量，这个容量可以是缓存内容的数量或者是缓存内容的大小。取决于你实现的方式，内存中缓存的容量是不容易确定的，如果你计算出容量要花费很长的时间，那么就不要去计算，缓存本来目的就是省时间，如果要花费很长时间去做额外操作，那么就得不偿失了。 读写和两层的同步，直接参照高速缓存的思想就可以了，并且建议使用写回。什么时候写回，你可以通过两个指标来触发写回，一个是还未写回的数量和两次发生写回的时间间隔。这两个指标是或的关系，任意一个达标，都可以触发写回。每次写入缓存的时候，检查一下未写回的数量，如果超标了就写回。再用一个计时器，每隔一个时间间隔触发写回。还有当app将要退出的时候也要写回。 淘汰的算法呢，幸运的是应用层，实现LRU很简单。只要用一个上文提到的LinkedHashMap就可以了，java中有系统实现的LinkedHashMap可以参考。先用一个链表来实现最近最少使用，每次插入数据，插入到表头，读取数据也把数据插入到表头，删除数据从表尾删除，越常用的会越靠近表头，表尾就是最不常用的了。但是，链表查询复杂度是线性的，搞不好要访问的数据在表尾，就得遍历一次表。解决这个问题就是引入哈希表。利用哈希表常数级的查询复杂度。最后才叫做linkedHashMap。 注意多线程！缓存时常在多线程上进行操作，那么共享变量就一定要加锁。建议内存缓存使用自旋锁，磁盘缓存使用信号量。自旋锁，会一直询问锁是否开了，会占用大量的资源，只适合等待时间很短就能进行的操作。信号量会在问完是否开以后，睡眠一段时间，更适合长时间等待的操作。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 实用命令(持续更新)]]></title>
    <url>%2F2016%2F03%2F06%2F2018-03-06-linux-command%2F</url>
    <content type="text"><![CDATA[查看系统信息cat /etc/os-release 123456789$ cat /etc/os-releasePRETTY_NAME=&quot;Debian GNU/Linux 8 (jessie)&quot;NAME=&quot;Debian GNU/Linux&quot;VERSION_ID=&quot;8&quot;VERSION=&quot;8 (jessie)&quot;ID=debianHOME_URL=&quot;http://www.debian.org/&quot;SUPPORT_URL=&quot;http://www.debian.org/support&quot;BUG_REPORT_URL=&quot;https://bugs.debian.org/&quot; 查找指定目录某种类型的文件find . -type f -name &quot;*.rar&quot; ! -path &quot;./res.nie/*&quot; -print &gt;&gt; temp2/rar_file !path: 查找忽略目录 查找指定目录中包含文本的文件grep -rn &quot;10537&quot; *]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机原理：(八) JVM类加载器机制与类加载过程]]></title>
    <url>%2F2016%2F01%2F12%2F2016-01-23-jvm-theory-8%2F</url>
    <content type="text"><![CDATA[原文链接: https://blog.csdn.net/luanlouis/article/details/50529868 为什么说Java语言是跨平台的？Java语言之所以说它是跨平台的、可以在当前绝大部分的操作系统平台下运行，是因为Java语言的运行环境是在Java虚拟机中。Java虚拟机消除了各个平台之间的差异，只要操作系统平台下安装了Java虚拟机，那么使用Java开发的东西都能在其上面运行。如下图所示： Java虚拟机对各个平台而言，实质上是各个平台上的一个可执行程序。例如在windows平台下，java虚拟机对于windows而言，就是一个java.exe进程而已。 Java虚拟机启动、加载类过程分析下面我将定义一个非常简单的java程序并运行它，来逐步分析java虚拟机启动的过程。12345678910111213141516package org.luanlouis.jvm.load; import sun.security.pkcs11.P11Util; /** * Created by louis on 2016/1/16. */ public class Main&#123; public static void main(String[] args) &#123; System.out.println("Hello,World!"); ClassLoader loader = P11Util.class.getClassLoader(); System.out.println(loader); &#125; &#125; 在windows命令行下输入：1java org.luanlouis.jvm.load.Main 当输入上述的命令时：windows开始运行{JRE_HOME}/bin/java.exe程序，java.exe 程序将完成以下步骤： 根据JVM内存配置要求，为JVM申请特定大小的内存空间； 创建一个引导类加载器实例，初步加载系统类到内存方法区区域中； 创建JVM启动器实例 Launcher,并取得类加载器ClassLoader； 使用上述获取的ClassLoader实例加载我们定义的 org.luanlouis.jvm.load.Main类； 加载完成时候JVM会执行Main类的main方法入口，执行Main类的main方法； 结束，java程序运行结束，JVM销毁。 Step 1.根据JVM内存配置要求，为JVM申请特定大小的内存空间 为了降低本文的理解难度，这里就不详细介绍JVM内存配置要求的话题，只概括地介绍一下内存的功能划分。 JVM启动时，按功能划分，其内存应该由以下几部分组成： 如上图所示，JVM内存按照功能上的划分，可以粗略地划分为方法区(Method Area) 和堆(Heap),而所有的类的定义信息都会被加载到方法区中。 Step 2. 创建一个引导类加载器实例，初步加载系统类到内存方法区区域中； JVM申请好内存空间后，JVM会创建一个引导类加载器（Bootstrap Classloader）实例，引导类加载器是使用C++语言实现的，负责加载JVM虚拟机运行时所需的基本系统级别的类，如java.lang.String, java.lang.Object等等。引导类加载器(Bootstrap Classloader)会读取 {JRE_HOME}/lib 下的jar包和配置，然后将这些系统类加载到方法区内。 本例中，引导类加载器是用 {JRE_HOME}/lib加载类的，不过，你也可以使用参数 -Xbootclasspath 或 系统变量sun.boot.class.path来指定的目录来加载类。 一般而言，{JRE_HOME}/lib下存放着JVM正常工作所需要的系统类，如下表所示： 文件名 描述 rt.jar 运行环境包，rt即runtime，J2SE 的类定义都在这个包内 charsets.jar 字符集支持包 jce.jar 是一组包，它们提供用于加密、密钥生成和协商以及 Message Authentication Code（MAC）算法的框架和实现 jsse.jar 安全套接字拓展包Java(TM) Secure Socket Extension classlist 该文件内表示是引导类加载器应该加载的类的清单 net.properties JVM 网络配置信息 引导类加载器(Bootstrap ClassLoader） 加载系统类后，JVM内存会呈现如下格局： 引导类加载器将类信息加载到方法区中，以特定方式组织，对于某一个特定的类而言，在方法区中它应该有运行时常量池、类型信息、字段信息、方法信息、类加载器的引用，对应class实例的引用等信息。 类加载器的引用,由于这些类是由引导类加载器(Bootstrap Classloader)进行加载的，而引导类加载器是有C++语言实现的，所以是无法访问的，故而该引用为NULL 对应class实例的引用 ， 类加载器在加载类信息放到方法区中后，会创建一个对应的Class 类型的实例放到堆(Heap)中, 作为开发人员访问方法区中类定义的入口和切入点。 当我们在代码中尝试获取系统类如java.lang.Object的类加载器时，你会始终得到NULL：1234System.out.println(String.class.getClassLoader());//null System.out.println(Object.class.getClassLoader());//null System.out.println(Math.class.getClassLoader());//null System.out.println(System.class.getClassLoader());//null Step 3. 创建JVM 启动器实例 Launcher,并取得类加载器ClassLoader 上述步骤完成，JVM基本运行环境就准备就绪了。接着，我们要让JVM工作起来了：运行我们定义的程序 org.luanlouis,jvm.load.Main。 此时，JVM虚拟机调用已经加载在方法区的类sun.misc.Launcher的静态方法getLauncher(), 获取sun.misc.Launcher 实例：12sun.misc.Launcher launcher = sun.misc.Launcher.getLauncher(); //获取Java启动器 ClassLoader classLoader = launcher.getClassLoader(); //获取类加载器ClassLoader用来加载class到内存来 sun.misc.Launcher 使用了单例模式设计，保证一个JVM虚拟机内只有一个sun.misc.Launcher实例。在Launcher的内部，其定义了两个类加载器(ClassLoader),分别是sun.misc.Launcher.ExtClassLoader和sun.misc.Launcher.AppClassLoader，这两个类加载器分别被称为拓展类加载器(Extension ClassLoader) 和 应用类加载器(Application ClassLoader).如下图所示： 除了引导类加载器(Bootstrap Class Loader )的所有类加载器，都有一个能力，就是判断某一个类是否被引导类加载器加载过，如果加载过，可以直接返回对应的Class instance，如果没有，则返回null. 图上的指向引导类加载器的虚线表示类加载器的这个有限的访问 引导类加载器的功能。 此时的launcher.getClassLoader()方法将会返回AppClassLoader实例，AppClassLoader将ExtClassLoader作为自己的父加载器。 当AppClassLoader加载类时，会首先尝试让父加载器ExtClassLoader进行加载，如果父加载器ExtClassLoader加载成功，则AppClassLoader直接返回父加载器ExtClassLoader加载的结果；如果父加载器ExtClassLoader加载失败，AppClassLoader则会判断该类是否是引导的系统类(即是否是通过Bootstrap类加载器加载，这会调用Native方法进行查找)；若要加载的类不是系统引导类，那么ClassLoader将会尝试自己加载，加载失败将会抛出“ClassNotFoundException”。 具体AppClassLoader的工作流程如下所示： 双亲委派模型(parent-delegation model)： 上面讨论的应用类加载器AppClassLoader的加载类的模式就是我们常说的双亲委派模型(parent-delegation model). 对于某个特定的类加载器而言，应该为其指定一个父类加载器，当用其进行加载类的时候： 委托父类加载器帮忙加载； 父类加载器加载不了，则查询引导类加载器(Bootstrap classLoader)有没有加载过该类； 如果引导类加载器没有加载过该类，则当前的类加载器应该自己加载该类； 若加载成功，返回 对应的Class 对象；若失败，抛出异常“ClassNotFoundException”。 请注意：双亲委派模型中的”双亲”并不是指它有两个父类加载器的意思，一个类加载器只应该有一个父加载器。上面的步骤中，有两个角色： 父类加载器(parent classloader)：它可以替子加载器尝试加载类 引导类加载器（bootstrap classloader）: 子类加载器只能判断某个类是否被引导类加载器加载过，而不能委托它加载某个类；换句话说，就是子类加载器不能接触到引导类加载器，引导类加载器对其他类加载器而言是透明的。 一般情况下，双亲加载模型如下所示： Step 4. 使用类加载器ClassLoader加载Main类 通过 launcher.getClassLoader()方法返回AppClassLoader实例，接着就是AppClassLoader加载 org.luanlouis.jvm.load.Main类的时候了。12ClassLoader classloader = launcher.getClassLoader();//取得AppClassLoader类 classLoader.loadClass("org.luanlouis.jvm.load.Main");//加载自定义类 上述定义的org.luanlouis.jvm.load.Main类被编译成org.luanlouis.jvm.load.Main class二进制文件，这个class文件中有一个叫常量池(Constant Pool)的结构体来存储该class的常量信息。常量池中有CONSTANT_CLASS_INFO类型的常量，表示该class中声明了要用到那些类： 当AppClassLoader要加载 org.luanlouis.jvm.load.Main类时，会去查看该类的定义，发现它内部声明使用了其它的类： sun.security.pkcs11.P11Util、java.lang.Object、java.lang.System、java.io.PrintStream、java.lang.Class；org.luanlouis.jvm.load.Main类要想正常工作，首先要能够保证这些其内部声明的类加载成功。所以AppClassLoader要先将这些类加载到内存中。（注：为了理解方便，这里没有考虑懒加载的情况，事实上的JVM加载类过程比这复杂的多） 加载顺序： 加载java.lang.Object、java.lang.System、java.io.PrintStream、java,lang.Class AppClassLoader尝试加载这些类的时候，会先委托ExtClassLoader进行加载；而ExtClassLoader发现不是其加载范围，抛出异常；AppClassLoader发现父类加载器ExtClassLoader无法加载，则会查询这些类是否已经被BootstrapClassLoader加载过，结果表明这些类已经被BootstrapClassLoader加载过，则无需重复加载，直接返回对应的Class实例； 加载sun.security.pkcs11.P11Util 此在{JRE_HOME}/lib/ext/sunpkcs11.jar包内，属于ExtClassLoader负责加载的范畴。AppClassLoader尝试加载这些类的时候，会先委托ExtClassLoader进行加载；而ExtClassLoader发现其正好属于加载范围，故ExtClassLoader负责将其加载到内存中。ExtClassLoader在加载sun.security.pkcs11.P11Util时也分析这个类内都使用了哪些类，并将这些类先加载内存后，才开始加载sun.security.pkcs11.P11Util，加载成功后直接返回对应的Class实例； 加载org.luanlouis.jvm.load.Main AppClassLoader尝试加载这些类的时候，会先委托ExtClassLoader进行加载；而ExtClassLoader发现不是其加载范围，抛出异常；AppClassLoader发现父类加载器ExtClassLoader无法加载，则会查询这些类是否已经被BootstrapClassLoader加载过。而结果表明BootstrapClassLoader 没有加载过它，这时候AppClassLoader只能自己动手负责将其加载到内存中，然后返回对应的Class实例引用； 以上三步骤都成功，才表示classLoader.loadClass(“org.luanlouis.jvm.load.Main”)完成，上述操作完成后，JVM内存方法区的格局会如下所示： 如上图所示： JVM方法区的类信息区是按照类加载器进行划分的，每个类加载器会维护自己加载类信息； 某个类加载器在加载相应的类时，会相应地在JVM内存堆（Heap）中创建一个对应的Class，用来表示访问该类信息的入口 Step 5. 使用Main类的main方法作为程序入口运行程序 Step 6. 方法执行完毕，JVM销毁，释放内存 三、类加载器有哪些？其组织结构是怎样的？类加载器(Class Loader)：顾名思义，指的是可以加载类的工具。JVM自身定义了三个类加载器：引导类加载器(Bootstrap Class Loader)、拓展类加载器(Extension Class Loader )、应用加载器(Application Class Loader)。当然，我们有时候也会自己定义一些类加载器来满足自身的需要。 引导类加载器(Bootstrap Class Loader): 该类加载器使JVM使用C/C++底层代码实现的加载器，用以加载JVM运行时所需要的系统类，这些系统类在{JRE_HOME}/lib目录下。由于类加载器是使用平台相关的底层C/C++语言实现的， 所以该加载器不能被Java代码访问到。但是，我们可以查询某个类是否被引导类加载器加载过。我们经常使用的系统类如：java.lang.String,java.lang.Object,java.lang.……. 这些都被放在 {JRE_HOME}/lib/rt.jar包内， 当JVM系统启动的时候，引导类加载器会将其加载到 JVM内存的*方法区中。 拓展类加载器(Extension Class Loader): 该加载器是用于加载 java 的拓展类 ，拓展类一般会放在 {JRE_HOME}/lib/ext/ 目录下，用来提供除了系统类之外的额外功能。拓展类加载器是是整个JVM加载器的Java代码可以访问到的类加载器的最顶端，即是超级父加载器，拓展类加载器是没有父类加载器的。 应用类加载器(Applocatoin Class Loader): 该类加载器是用于加载用户自定义的类，是用户代码的入口。我经常执行指令java xxx.x.xxx.x.x.XClass, 实际上，JVM就是使用的AppClassLoader加载 xxx.x.xxx.x.x.XClass 类的。应用类加载器将拓展类加载器当成自己的父类加载器，当其尝试加载类的时候，首先尝试让其父加载器-拓展类加载器加载；如果拓展类加载器加载成功，则直接返回加载结果Class instance,加载失败，则会询问是否引导类加载器已经加载了该类；只有没有加载的时候，应用类加载器才会尝试自己加载。由于xxx.x.xxx.x.x.XClass是整个用户代码的入口，在Java虚拟机规范中，称其为初始类(Initial Class). 用户自定义类加载器（Customized Class Loader）：用户可以自己定义类加载器来加载类。所有的类加载器都要继承java.lang.ClassLoader类。 双亲加载模型的逻辑和底层代码实现是怎样的？上面已经不厌其烦地讲解什么是双亲加载模型，以及其机制是什么，这些东西都是可以通过底层代码查看到的。 我们也可以通过JDK源码看java.lang.ClassLoader的核心方法 loadClass()的实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748//提供class类的二进制名称表示，加载对应class，加载成功，则返回表示该类对应的Class&lt;T&gt; instance 实例 public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException &#123; return loadClass(name, false); &#125; protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // 首先，检查是否已经被当前的类加载器记载过了，如果已经被加载，直接返回对应的Class&lt;T&gt;实例 Class&lt;?&gt; c = findLoadedClass(name); //初次加载 if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; //如果有父类加载器，则先让父类加载器加载 c = parent.loadClass(name, false); &#125; else &#123; // 没有父加载器，则查看是否已经被引导类加载器加载，有则直接返回 c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; // 父加载器加载失败，并且没有被引导类加载器加载，则尝试该类加载器自己尝试加载 if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); // 自己尝试加载 c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; //是否解析类 if (resolve) &#123; resolveClass(c); &#125; return c; &#125; &#125; 相对应地，我们可以整理出双亲模型的工作流程图： 相信读者看过这张图后会对双亲加载模型有了非常清晰的脉络。当然，这是JDK自身默认的加载类的行为，我们可以通过继承复写该方法，改变其行为。 类加载器与Class 实例的关系 线程上下文加载器Java 任何一段代码的执行，都有对应的线程上下文。如果我们在代码中，想看当前是哪一个线程在执行当前代码，我们经常是使用如下方法：1Thread thread = Thread.currentThread();//返回对当当前运行线程的引用 相应地，我们可以为当前的线程指定类加载器。在上述的例子中， 当执行java org.luanlouis.jvm.load.Main的时候，JVM会创建一个Main线程，而创建应用类加载器AppClassLoader的时候，会将AppClassLoader设置成Main线程的上下文类加载器：1234567891011121314151617public Launcher() &#123; Launcher.ExtClassLoader var1; try &#123; var1 = Launcher.ExtClassLoader.getExtClassLoader(); &#125; catch (IOException var10) &#123; throw new InternalError("Could not create extension class loader", var10); &#125; try &#123; this.loader = Launcher.AppClassLoader.getAppClassLoader(var1); &#125; catch (IOException var9) &#123; throw new InternalError("Could not create application class loader", var9); &#125; //将AppClassLoader设置成当前线程的上下文加载器 Thread.currentThread().setContextClassLoader(this.loader); //....... &#125; 线程上下文类加载器是从线程的角度来看待类的加载，为每一个线程绑定一个类加载器，可以将类的加载从单纯的 双亲加载模型解放出来，进而实现特定的加载需求。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>字节码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[译]厌倦了NullPointException？Optional拯救你！]]></title>
    <url>%2F2015%2F12%2F08%2F2015-12-08-use-optional-avoid-nullpointexception%2F</url>
    <content type="text"><![CDATA[原文链接: http://weishu.me/2015/12/08/use-optional-avoid-nullpointexception/ 笔记 原文是Oracle官网上的文章，译者翻译得很好。看完能基本了解Java8中Optional的基本用法，当然前提是需要对lambda已有一定的了解。看完文章，我的疑问是Optional是否支持IOC框架的依赖注入？网上查阅了一下,发现Spring4.x 已经能完美支持Optional。 正文 I call it my billion-dollar mistake. It was the invention of the null reference in 1965. I couldn’t resist the temptation to put in a null reference, simply because it was so easy to implement.—Tony Hoare 有人说，当你处理过了空指针异常才真正成为一个Java开发者。抛开玩笑话不谈，空指针确实是很多bug的根源。Java SE 8引入了一个新的叫做java.util.Optional 的类来缓解这个问题。 我们首先看看空指针有什么危险，Computer是一个嵌套的对象，如图： Comtuper对象 下面的代码有什么潜在的问题呢？1String version = computer.getSoundcard().getUSB().getVersion(); 貌似可行，但是，很多电脑（比如 Raspberry Pi）并没有Soundcard，因此调用getSoundcard会发生什么？毫无疑问，结果自然是在运行时给你抛出一个NullPointException，然后终止程序的执行。 如何避免上面的空指针异常呢？一般的做法就是在调用方法之前进行检测：12345678910String version = "UNKNOWN";if(computer != null)&#123; Soundcard soundcard = computer.getSoundcard(); if(soundcard != null)&#123; USB usb = soundcard.getUSB(); if(usb != null)&#123; version = usb.getVersion(); &#125; &#125;&#125; 但是，上面嵌套if检测的代码确实不怎么好看。但是没办法，我们需要很多这样死板的没什么意义的代码来避免碰到NullPointException。更恼火的是，这部分代码成了我们业务逻辑的一部分，还降低了代码的可读性。 万一我们忘记对某个可能为null的对象进行非空检测怎么办？使用null来说明某个值缺失是一种错误的方式, 下文将说明这个问题并给出更好的解决办法。 先看看别的编程语言是如何处理这个问题的。 Null的替代物Grovvy语言有一个?.的操作符，可以安全地处理潜在可能的空引用（C#即将包含这个特性，Java7曾被建议引入这个但是并没有发布。）它是这么用的：1String version = computer?.getSoundcard()?.getUSB()?.getVersion(); 如果getSoundcard(),getUSB(),getVersion任意一个返回null，变量version就被赋值为null，不需要额外的复杂的嵌套检测。更好的是，Grovvy还有一个Elvis操作符:?:，可以给类似上面的表达式提供默认值。下面的表达式如果?. 返回了null那么变量version会被赋值为”UNKNOW”:1String version = computer?.getSoundcard()?.getUSB()?.getVersion() ?: "UNKNOWN"; 其他的一些函数式编程语言，比如Haskell, Scala，使用了一种别的方式。Haskell有一个Maybe型态，这个型态代表了一种有可选值的类型。Maybe形态的值可能包含一个给定类型的值或者是Nothing(译者注：代表没有值)，完全没有空指针的概念。Scala有一种类似的叫做Option[T]的东西来代表类型T的某一个值存在或者没有。因此，你必须显式检测这个值是否存在，如果不存在就不能使用任何Option类型的操作符；这样由于Scala的类型系统，你永远也不会忘记对于空指针的检测。貌似有点扯远了，那么，Java8给我们提供了什么呢？ 果壳里的Optional受到Haskell和Scala的启发，Java8引入了一个叫做java.util.Optional的类，这一个包含一个可选值的类型，你可以把它当作包含单个值的容器——这个容器要么包含一个值要么什么都没有，如下图： Optional表示 我们在数据模型里面引入Optional：123456789101112131415public class Computer &#123; private Optional&lt;Soundcard&gt; soundcard; public Optional&lt;Soundcard&gt; getSoundcard() &#123; ... &#125; ...&#125;public class Soundcard &#123; private Optional&lt;USB&gt; usb; public Optional&lt;USB&gt; getUSB() &#123; ... &#125;&#125;public class USB&#123; public String getVersion()&#123; ... &#125;&#125; 用上面的代码，我们一眼就可以看出来一个computer有没有soundcard（他们是optioal，可选的），更进一步，一个声卡也有一个可选的USB端口；新的模型能清晰地反映出一个给定的值是有可能不存在的。这种做法在某些库里面也存在，比如Guava(译：Java5之后就可以使用，不过有局限) 我们能用Optional对象干什么？Optional对象包含了一些方法来显式地处理某个值是存在还是缺失，Optional类强制你思考值不存在的情况，这样就能避免潜在的空指针异常。 值得一提的是，设计Optional类的目的并不是完全取代null, 它的目的是设计更易理解的API。通过Optional，可以从方法签名就知道这个函数有可能返回一个缺失的值，这样强制你处理这些缺失值的情况。 Optional的正确打开方式废话扯了这么多，来点实际的例子吧！首先来看看如何使用Optional类来实现传统的空指针检测：1234String name = computer.flatMap(Computer::getSoundcard) .flatMap(Soundcard::getUSB) .map(USB::getVersion) .orElse("UNKNOWN"); 如果无法理解这段代码，可以复习Java8的lambda和方法引用,见Java8 Lambdas 以及stream pipelining概念,见Processing Data with Java SE 8 Steams 创建Optional对象如何创建Optional对象呢，有下面几种方式： 空的Optional 1Optional&lt;Soundcard&gt; sc = Optional.empty(); 包含非空值的Optional 12SoundCard soundcard = new Soundcard();Optional&lt;Soundcard&gt; sc = Optional.of(soundcard); 一旦soundcard是null，这段代码会立即抛出一个NullPointException（而不是等你以后你访问这个空的soundcard对象的时候) 可能为空的Optional1Optional&lt;Soundcard&gt; sc = Optional.ofNullable(soundcard); 如果soundcard是null那么这个Optional将会是empty. 值存在的时候进行进一步的操作现在你有了一个Optional对象，你可以显式地处理值存在或者不存在的情况，再也不用想这样如履薄冰地进行空指针检测了：1234SoundCard soundcard = ...;if(soundcard != null)&#123; System.out.println(soundcard);&#125; 现在，可以使用ifPresent()方法，如下：12Optional&lt;Soundcard&gt; soundcard = ...;soundcard.ifPresent(System.out::println); 现在，你再也不用显示地进行非空检测了，类型系统帮你干了这件事。如果Optional是empty,上面的代码就不会执行打印了。 你也可以使用isPresent()方法检查某个值是否存在，另外，get 方法可以返回Optional容器里面包含的那个对象，如果没有这个对象，get方法会立即抛出一个NoSuchElementException，这两个方法可以结合起来：123if(soundcard.isPresent())&#123; System.out.println(soundcard.get());&#125; 但是，并不提倡这样使用Optional。（这么做跟null检测有什么区别？），下面有一些惯用手法，我们来看一下。 默认值和默认操作在某个操作返回空的时候给出一个默认值也是一个典型的场景，通常的做法是使用三目运算符(?):12Soundcard soundcard = maybeSoundcard != null ? maybeSoundcard : new Soundcard("basic_sound_card"); 如果你想在空值的时候抛出一个异常，可以使用ifElseThrow方法：12Soundcard soundcard = maybeSoundCard.orElseThrow(IllegalStateException::new); 使用filter过滤特定值很多时候你需要调用某个对象的方法并且检查它的一些属性。例如：你可能需要检测一个USB的端口是否是一个特定的版本；如果需要避免空指针异常，通畅的方式是检测非空然后调用getVersion方法，如下：1234USB usb = ...;if(usb != null &amp;&amp; "3.0".equals(usb.getVersion()))&#123; System.out.println("ok");&#125; 使用Optional的filter可以这么干：123Optional&lt;USB&gt; maybeUSB = ...;maybeUSB.filter(usb -&gt; "3.0".equals(usb.getVersion()) .ifPresent(() -&gt; System.out.println("ok")); filter方法带有一个Predicate的参数，如果Optional容器里面的对象存在并且满足这个predicate,那么filter返回那个对象，否则就返回empty的Optional。（跟Stream接口的filter类似） 使用map转换值另外一个比较常见的场景是需要从某个对象里面提取出特定的值。例如：从一个Soundcard对象里面取出一个USB对象然后检测这个usb对象是否是正确的版本。通常可以这么写：123456if(soundcard != null)&#123; USB usb = soundcard.getUSB(); if(usb != null &amp;&amp; "3.0".equals(usb.getVersion())&#123; System.out.println("ok"); &#125;&#125; 使用Optional的map方法，如下：1Optional&lt;USB&gt; usb = maybeSoundcard.map(Soundcard::getUSB); Optional容器里面的值被某个函数（这里是USB的方法引用）作为参数“转换”了，如果Optional是empty那么就什么也不会发生。结合使用map和filter可以检测某个声卡是否有USB 3.0的接口：123maybeSoundcard.map(Soundcard::getUSB) .filter(usb -&gt; "3.0".equals(usb.getVersion()) .ifPresent(() -&gt; System.out.println("ok")); 现在我们的代码看起来比较像是在描述问题了！而且没有任何非空检测，太酷了！ 使用flatMap级联Optional我们已经有一些常见的模式可以通过Optional重构了，那么我们如何用一种安全的方式重构下面的代码呢？结合使用map和filter可以检测某个声卡是否有USB 3.0的接口：1String version = computer.getSoundcard().getUSB().getVersion(); 上面的代码都是从一个对象里面取出另外一个对象， 这不正是上文介绍的map吗？我们改写Computer模型对象，让它拥有一个Optional和一个Optional，然后就可以把代码改成这样：1234String version = computer.map(Computer::getSoundcard) .map(Soundcard::getUSB) .map(USB::getVersion) .orElse("UNKNOWN"); 但是，这段代码并不能通过编译。为什么？ computer变量类型是Optional，因此它调用map方法没有任何问题；但是，getSoundcard()方法的返回类型是Optional这意味着map操作结果的类型是Optional&lt;Optional&gt;,因此getUsb这个调用是非法的：外面的那个Optional包含的值是另外一个Optional，自然就没有getUsb方法，下图是这个调用的结构： two level Optional 如何解决这个问题呢？Optional类提供了一个flapMap的方法。这个方法可以对一个Optional使用一个函数转换为一个Optional然后把结果（两个Optional)flatten为一个单个Optional，下图给出了map和flatMap的区别： map and flatMap 用flatMap重写我们的代码：1234String version = computer.flatMap(Computer::getSoundcard) .flatMap(Soundcard::getUSB) .map(USB::getVersion) .orElse("UNKNOWN"); 第一个flatMap确保返回一个Optioan而不是Optional&lt;&lt;Optional&gt;，第二个flatMap确保返回一个Optional；接着第三次调用着需要一个map即可，因为getVersion返回一个String而非Optional方法。 Cool！现在我们可以抛弃痛苦的嵌套非空检测了，使用Optional可以写出声明式的，更可读的代码，并且永远不会有空指针异常！ 总结本文介绍了如何使用Java SE 8的java.util.Optional。Optional的目的不是替换你代码里面的每个null，它可以帮助你设计出更好的API，使用者通过方法签名就能知道是否有一个可选的值。另外，Optional通过强迫主动处理空指针情况，可以保护代码不出现NullPointException。 译后感嵌套的非空检测确实是个很头大的问题，虽然有一些静态代码检测工具可以检测到这些异常，但是这样无聊的检测代码很是让人失望。Java 8引入的Optional确实可以部分缓解这部分问题；但是依然存在局限性，比如，如果某个特定的方法调用出了别的运行时异常怎么办？对于?Haskell Maybe Monad只吸收了一部分，不过已经很不错了，期待什么时候能引入Grovvy的?.操作符，在处理空指针问题上，?.更加简洁有力。 Optional虽好，但是Java 8目前并不普及，Android 就不用想了。虽然有retrolambda项目支持在Java 6里面使用lambda，但是它更多地是提供了语法糖： lambda的实现使用的是匿名内部类而不是invokedynamic, 见深入探索Java 8 Lambda表达式 方法引用是lambda的语法糖，实现相同 接口默认方法实际上给接口生成了一个抽象方法，然后给所有接口的实现者添加了这个默认实现 接口静态方法，实际上把静态方法放在另外一个类里面，然后把所有对接口静态方法的调用更换为对新生成类里面方法的调用 鉴于以上种种原因，在生产环境基本上不可能使用retrolambda了，大型系统还是老实一点吧。 虽然Grava项目也有一个Optional类，但是没有函数式接口，我们所能做的不过是把if (obj == null)替换为if (opt.isPresend())罢了；虽说能提高类型安全性，但是还是得写一堆shit一样的嵌套检测。 对于Android开发，想使用这个是没有希望了。但愿Kotlin能给我们惊喜。 参考 Chapter 9, “Optional: a better alternative to null,” from Java 8 in Action: Lambdas, Streams, and Functional-style Programming “Monadic Java“ by Mario Fusco Processing Data with Java SE 8 Streams 致谢 Thanks to Alan Mycroft and Mario Fusco for going through the adventure of writing Java 8 in Action: Lambdas, Streams, and Functional-style Programming with me. 关于作者 Raoul-Gabriel Urma (@raoulUK) is currently completing a PhD in computer science at the University of Cambridge, where he does research in programming languages. He’s a coauthor of the upcoming book Java 8 in Action: Lambdas, Streams, and Functional-style Programming, published by Manning. He is also a regular speaker at major Java conferences (for example, Devoxx and Fosdem) and an instructor. In addition, he has worked at several well-known companies—including Google’s Python team, Oracle’s Java Platform group, eBay, and Goldman Sachs—as well as for several startup projects. 原文：http://www.oracle.com/technetwork/articles/java/java8-optional-2175753.html]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>Java语言</tag>
        <tag>Java8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入探索Java 8 Lambda表达式]]></title>
    <url>%2F2015%2F11%2F27%2F2015-11-27-Lambda%2F</url>
    <content type="text"><![CDATA[原文链接: http://www.infoq.com/cn/articles/Java-8-Lambdas-A-Peek-Under-the-Hood 2014年3月，Java 8发布，Lambda表达式作为一项重要的特性随之而来。或许现在你已经在使用Lambda表达式来书写简洁灵活的代码。比如，你可以使用Lambda表达式和新增的流相关的API，完成如下的大量数据的查询处理：1234int total = invoices.stream() .filter(inv -&gt; inv.getMonth() == Month.JULY) .mapToInt(Invoice::getAmount) .sum(); 上面的示例代码描述了如何从一打发票中计算出7月份的应付款总额。其中我们使用Lambda表达式过滤出7月份的发票，使用方法引用来提取出发票的金额。 到这里，你可能会对Java编译器和JVM内部如何处理Lambda表达式和方法引用比较好奇。可能会提出这样的问题，Lambda表达式会不会就是匿名内部类的语法糖呢？毕竟上面的示例代码可以使用匿名内部类实现，将Lambda表达式的方法体实现移到匿名内部类对应的方法中即可，但是我们并不赞成这样做。如下为匿名内部类实现版本：1234567891011121314int total = invoices.stream() .filter(new Predicate&lt;Invoice&gt;() &#123; @Override public boolean test(Invoice inv) &#123; return inv.getMonth() == Month.JULY; &#125; &#125;) .mapToInt(new ToIntFunction&lt;Invoice&gt;() &#123; @Override public int applyAsInt(Invoice inv) &#123; return inv.getAmount(); &#125; &#125;) .sum(); 本文将会介绍为什么Java编译器没有采用内部类的形式处理Lambda表达式，并解密Lambda表达式和方法引用的内部实现。接着介绍字节码生成并简略分析Lambda表达式理论上的性能。最后，我们将讨论一下实践中Lambda表达式的性能问题。 为什么匿名内部类不好？实际上，匿名内部类存在着影响应用性能的问题。 首先，编译器会为每一个匿名内部类创建一个类文件。创建出来的类文件的名称通常按照这样的规则 ClassName$1,其中ClassName就是匿名内部类定义所属的类的名称，ClassName后面需要接上符号和数字。生成如此多的文件就会带来问题，因为类在使用之前需要加载类文件并进行验证，这个过程则会影响应用的启动性能。类文件的加载很有可能是一个耗时的操作，这其中包含了磁盘IO和解压JAR文件。 假设Lambda表达式翻译成匿名内部类，那么每一个Lambda表达式都会有一个对应的类文件。随着匿名内部类进行加载，其必然要占用JVM中的元空间（从Java 8开始永久代的一种替代实现）。如果匿名内部类的方法被JIT编译成机器代码，则会存储到代码缓存中。同时，匿名内部类都需要实例化成独立的对象。以上关于匿名内部类的种种会使得应用的内存占用增加。因此我们有必要引入新的缓存机制减少过多的内存占用，这也就意味着我们需要引入某种抽象层。 最重要的，一旦Lambda表达式使用了匿名内部类实现，就会限制了后续Lambda表达式实现的更改，降低了其随着JVM改进而改进的能力。 我们看一下下面的这段代码：12345678import java.util.function.Function;public class AnonymousClassExample &#123; Function&lt;String, String&gt; format = new Function&lt;String, String&gt;() &#123; public String apply(String input)&#123; return Character.toUpperCase(input.charAt(0)) + input.substring(1); &#125; &#125;;&#125; 使用这个命令我们可以检查任何类文件生成的字节码1javap -c -v ClassName 示例中使用Function创建的匿名内部类对应的字节码如下：1234567890: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V4: aload_0 5: new #2 // class AnonymousClassExample$18: dup 9: aload_0 10: invokespecial #3 // Method AnonymousClass$1.&quot;&lt;init&gt;&quot;:(LAnonymousClassExample;)V13: putfield #4 // Field format:Ljava/util/function/Function;16: return 上述字节码的含义如下： 第5行，使用字节码操作new创建了类型AnonymousClassExample$1的一个对象，同时将新创建的对象的的引用压入栈中。 第8行，使用dup操作复制栈上的引用。 第10行，上面的复制的引用被指令invokespecial消耗使用，用来初始化匿名内部类实例。 第13行，栈顶依旧是创建的对象的引用，这个引用通过putfield指令保存到AnonymousClassExample类的format属性中。 AnonymousClassExample$1就是由编译器生成的匿名内部类的名称。如果想更加验证的话，，你可以检查AnonymousClassExample$1这个类文件，你会发现这个类就是Function接口的实现。 将Lambda表达式翻译成匿名内部类会限制以后可能进行的优化（比如缓存）。因为一旦使用了翻译成匿名内部类形式，那么Lambda表达式则和匿名内部类的字节码生成机制绑定。因而，Java语言和JVM工程师需要设计一个稳定并且具有足够信息的二进制表示形式来支持以后的JVM实现策略。下面的部分将介绍不使用匿名内部类机制，Lambda表达式是如何工作的。 Lambdas表达式和invokedynamic为了解决前面提到的担心，Java语言和JVM工程师决定将翻译策略推迟到运行时。利用Java 7引入的invokedynamic字节码指令我们可以高效地完成这一实现。将Lambda表达式转化成字节码只需要如下两步：1.生成一个invokedynamic调用点，也叫做Lambda工厂。当调用时返回一个Lambda表达式转化成的函数式接口实例。 2.将Lambda表达式的方法体转换成方法供invokedynamic指令调用。 为了阐明上述的第一步，我们这里举一个包含Lambda表达式的简单类：12345import java.util.function.Function;public class Lambda &#123; Function&lt;String, Integer&gt; f = s -&gt; Integer.parseInt(s);&#125; 需要注意的是，方法引用的编译稍微有点不同，因为javac不需要创建一个合成的方法，javac可以直接访问该方法。 Lambda表达式转化成字节码的第二步取决于Lambda表达式是否为对变量捕获。Lambda表达式方法体需要访问外部的变量则为对变量捕获，反之则为对变量不捕获。 对于不进行变量捕获的Lambda表达式，其方法体实现会被提取到一个与之具有相同签名的静态方法中，这个静态方法和Lambda表达式位于同一个类中。比如上面的那段Lambda表达式会被提取成类似这样的方法：123static Integer lambda$1(String s) &#123; return Integer.parseInt(s);&#125; 需要注意的是，这里的$1并不是代表内部类，这里仅仅是为了展示编译后的代码而已。 对于捕获变量的Lambda表达式情况有点复杂，同前面一样Lambda表达式依然会被提取到一个静态方法中，不同的是被捕获的变量同正常的参数一样传入到这个方法中。在本例中，采用通用的翻译策略预先将被捕获的变量作为额外的参数传入方法中。比如下面的示例代码：12int offset = 100;Function&lt;String, Integer&gt; f = s -&gt; Integer.parseInt(s) + offset; 对应的翻译后的实现方法为：123static Integer lambda$1(int offset, String s) &#123; return Integer.parseInt(s) + offset;&#125; 需要注意的是编译器对于Lambda表达式的翻译策略并非固定的，因为这样invokedynamic可以使编译器在后期使用不同的翻译实现策略。比如，被捕获的变量可以放入数组中。如果Lambda表达式用到了类的实例的属性，其对应生成的方法可以是实例方法，而不是静态方法，这样可以避免传入多余的参数。 性能分析Lambda表达式最主要的优势表现在性能方面，虽然使用它很轻松的将很多行代码缩减成一句，但是其内部实现却不这么简单。下面对内部实现的每一步进行性能分析。 第一步就是连接，对应的就是我们上面提到的Lambda工厂。这一步相当于匿名内部类的类加载过程。来自Oracle的Sergey Kuksenko发布过相关的性能报告，并且他也在2013 JVM语言大会就该话题做过分享。报告表明，Lambda工厂的预热准备需要消耗时间，并且这个过程比较慢。伴随着更多的调用点连接，代码被频繁调用后（比如被JIT编译优化）性能会提升。另一方面如果连接处于不频繁调用的情况，那么Lambda工厂方式也会比匿名内部类加载要快，最高可达100倍。 第二步就是捕获变量。正如我们前面提到的，如果是不进行捕获变量，这一步会自动进行优化，避免在基于Lambda工厂实现下额外创建对象。对于匿名内部类而言，这一步对应的是创建外部类的实例，为了优化内部类这一步的问题，我们需要手动的修改代码，如创建一个对象，并将它设置给一个静态的属性。如下述代码：123456789// Hoisted Functionpublic static final Function&lt;String, Integer&gt; parseInt = new Function&lt;String, Integer&gt;() &#123; public Integer apply(String arg) &#123; return Integer.parseInt(arg); &#125;&#125;;// Usage:int result = parseInt.apply(“123”); 第三步就是真实方法的调用。在这一步中匿名内部类和Lambda表达式执行的操作相同，因此没有性能上的差别。不进行捕获的Lambda表达式要比进行static优化过的匿名内部类较优。进行变量捕获的Lambda表达式和匿名内部类表达式性能大致相同。 在这一节中，我们明显可以看到Lambda表达式的实现表现良好，匿名内部类通常需要我们手动的进行优化来避免额外对象生成，而对于不进行变量捕获的Lambda表达式，JVM已经为我们做好了优化。 实践中的性能分析理解了Lambda的性能模型很是重要，但是实际应用中的总体性能如何呢？我们在使用Java 8 编写了一些软件项目，一般都取得了很好的效果。非变量捕获的Lambda表达式给我们带来了很大的帮助。这里有一个很特殊的例子描述了关于优化方向的一些有趣的问题。 这个例子的场景是代码需要运行在一个要求GC暂定时间越少越好的系统上。因而我们需要避免创建大量的对象。在这个工程中，我们使用了大量的Lambda表达式来实现回调处理。然而在这些使用Lambda实现的回调中很多并没有捕获局部变量，而是需要引用当前类的变量或者调用当前类的方法。然而目前仍需要对象分配。下面就是我们提到的例子的代码：12345678910public MessageProcessor() &#123;&#125;public int processMessages() &#123; return queue.read(obj -&gt; &#123; if (obj instanceof NewClient) &#123; this.processNewClient((NewClient) obj); &#125; ... &#125;);&#125; 有一个简单的办法解决这个问题，我们将Lambda表达式的代码提前到构造方法中，并将其赋值给一个成员属性。在调用点我们直接引用这个属性即可。下面就是修改后的代码：1234567891011121314private final Consumer&lt;Msg&gt; handler;public MessageProcessor() &#123; handler = obj -&gt; &#123; if (obj instanceof NewClient) &#123; this.processNewClient((NewClient) obj); &#125; ... &#125;;&#125;public int processMessages() &#123; return queue.read(handler);&#125; 然而上面的修改后代码给却给整个工程带来了一个严重的问题：性能分析表明，这种修改产生很大的对象申请，其产生的内存申请在总应用的60%以上。 类似这种无关上下文的优化可能带来其他问题。 纯粹为了优化的目的，使用了非惯用的代码写法，可读性会稍差一些。 内存分配方面的问题，示例中为MessageProcessor增加了一个成员属性，使得MessageProcessor对象需要申请更大的内存空间。Lambda表达式的创建和捕获位于构造方式中，使得MessageProcessor的构造方法调用缓慢一些。 我们遇到这种情况，需要进行内存分析，结合合理的业务用例来进行优化。有些情况下，我们使用成员属性确保为经常调用的Lambda表达式只申请一个对象，这样的缓存策略大有裨益。任何性能调优的科学的方法都可以进行尝试。 上述的方法也是其他程序员对Lambda表达式进行优化应该使用的。书写整洁，简单，函数式的代码永远是第一步。任何优化，如上面的提前代码作为成员属性，都必须结合真实的具体问题进行处理。变量捕获并申请对象的Lambda表达式并非不好，就像我们我们写出new Foo()代码并非一无是处一样。 除此之外，我们想要写出最优的Lambda表达式，常规书写很重要。如果一个Lambda表达式用来表示一个简单的方法，并且没有必要对上下文进行捕获，大多数情况下，一切以简单可读即可。 总结在这片文章中，我们研究了Lambda表达式不是简单的匿名内部类的语法糖，为什么匿名内部类不是Lambda表达式的内部实现机制以及Lambda表达式的具体实现机制。对于大多数情况来说，Lambda表达式要比匿名内部类性能更优。然而现状并非完美，基于测量驱动优化，我们仍然有很大的提升空间。 Lambda表达式的这种实现形式并非Java 8 所有。Scala曾经通过生成匿名内部类的形式支持Lambda表达式。在Scala 2.12版本，Lambda的实现形式替换为Java 8中的Lambda 工厂机制。后续其他可以在JVM上运行的语言也可能支持Lambda的这种机制。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>Java8 Lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的自动装箱与拆箱]]></title>
    <url>%2F2015%2F04%2F07%2F2015-04-07-boxing-and-unboxing%2F</url>
    <content type="text"><![CDATA[原文链接：http://www.cnblogs.com/dolphin0520/p/3778589.html 笔记 Java自动装箱与拆箱是开发中极常用和简单的功能，甚至大部分开发人员都觉得理所当然了，很少去思考它的历史和实现，更枉论使用的弊端和注意事项。这篇文章把这一切都解释得非常清楚，值得推荐！ 自动装箱和拆箱从Java 1.5开始引入，目的是将原始类型值转自动地转换成对应的对象。自动装箱与拆箱的机制可以让我们在Java的变量赋值或者是方法调用等情况下使用原始类型或者对象类型更加简单直接。 如果你在Java1.5以前进行过编程的话，你一定不会陌生这一点，你不能直接地向集合(Collections)中放入原始类型值，因为集合只接收对象。通常这种情况下你的做法是，将这些原始类型的值转换成对象，然后将这些转换的对象放入集合中。使用Integer,Double,Boolean等这些类我们可以将原始类型值转换成对应的对象，但是从某些程度可能使得代码不是那么简洁精炼。为了让代码简练，Java 1.5引入了具有在原始类型和对象类型自动转换的装箱和拆箱机制。但是自动装箱和拆箱并非完美，在使用时需要有一些注意事项，如果没有搞明白自动装箱和拆箱，可能会引起难以察觉的bug。 什么是自动装箱和拆箱自动装箱就是Java自动将原始类型值转换成对应的对象，比如将int的变量转换成Integer对象，这个过程叫做装箱，反之将Integer对象转换成int类型值，这个过程叫做拆箱。因为这里的装箱和拆箱是自动进行的非人为转换，所以就称作为自动装箱和拆箱。原始类型byte,short,char,int,long,float,double和boolean对应的封装类为Byte,Short,Character,Integer,Long,Float,Double,Boolean。 自动装箱拆箱要点自动装箱时编译器调用valueOf()将原始类型值转换成对象，同时自动拆箱时，编译器通过调用类似intValue(),doubleValue()这类的方法将对象转换成原始类型值。自动装箱是将boolean值转换成Boolean对象，byte值转换成Byte对象，char转换成Character对象，float值转换成Float对象，int转换成Integer，long转换成Long，short转换成Short，自动拆箱则是相反的操作。 何时发生自动装箱和拆箱自动装箱和拆箱在Java中很常见，比如我们有一个方法，接受一个对象类型的参数，如果我们传递一个原始类型值，那么Java会自动讲这个原始类型值转换成与之对应的对象。最经典的一个场景就是当我们向ArrayList这样的容器中增加原始类型数据时或者是创建一个参数化的类，比如下面的ThreadLocal。 123456789ArrayList&lt;Integer&gt; intList = new ArrayList&lt;Integer&gt;();intList.add(1); //autoboxing - primitive to objectintList.add(2); //autoboxingThreadLocal&lt;Integer&gt; intLocal = new ThreadLocal&lt;Integer&gt;();intLocal.set(4); //autoboxingint number = intList.get(0); // unboxingint local = intLocal.get(); // unboxing in Java 举例说明上面的部分我们介绍了自动装箱和拆箱以及它们何时发生，我们知道了自动装箱主要发生在两种情况，一种是赋值时，另一种是在方法调用的时候。为了更好地理解这两种情况，我们举例进行说明。 赋值时这是最常见的一种情况，在Java 1.5以前我们需要手动地进行转换才行，而现在所有的转换都是由编译器来完成。1234567//before autoboxingInteger iObject = Integer.valueOf(3);int iPrimitive = iObject.intValue()//after java5Integer iObject = 3; //autobxing - primitive to wrapper conversionint iPrimitive = iObject; //unboxing - object to primitive conversion 方法调用时这是另一个常用的情况，当我们在方法调用时，我们可以传入原始数据值或者对象，同样编译器会帮我们进行转换12345678public static Integer show(Integer iParam)&#123; System.out.println("autoboxing example - method invocation i: " + iParam); return iParam;&#125;//autoboxing and unboxing in method invocationshow(3); //autoboxingint result = show(3); //unboxing because return type of method is Integer show方法接受Integer对象作为参数，当调用show(3)时，会将int值转换成对应的Integer对象，这就是所谓的自动装箱，show方法返回Integer对象，而int result = show(3);中result为int类型，所以这时候发生自动拆箱操作，将show方法的返回的Integer对象转换成int值。 自动装箱的弊端自动装箱有一个问题，那就是在一个循环中进行自动装箱操作的情况，如下面的例子就会创建多余的对象，影响程序的性能。1234Integer sum = 0; for(int i=1000; i&lt;5000; i++)&#123; sum+=i;&#125; 上面的代码123```JAVAint result = sum.intValue() + i;Integer sum = Integer.valueOf(result); 其中Integer.valueOf(int)的实现如下：12345public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); &#125; 由于我们这里声明的sum为Integer类型，在上面的循环中会创建将近4000个无用的Integer对象，在这样庞大的循环中，会降低程序的性能并且加重了垃圾回收的工作量。因此在我们编程时，需要注意到这一点，正确地声明变量类型，避免因为自动装箱引起的性能问题。 重载与自动装箱当重载遇上自动装箱时，情况会比较有些复杂，可能会让人产生有些困惑。在1.5之前，value(int)和value(Integer)是完全不相同的方法，开发者不会因为传入是int还是Integer调用哪个方法困惑，但是由于自动装箱和拆箱的引入，处理重载方法时稍微有点复杂。一个典型的例子就是ArrayList的remove方法，它有remove(index)和remove(Object)两种重载，我们可能会有一点小小的困惑，其实这种困惑是可以验证并解开的，通过下面的例子我们可以看到，当出现这种情况时，不会发生自动装箱操作。 1234567891011121314151617181920public void test(int num)&#123; System.out.println("method with primitive argument");&#125;public void test(Integer num)&#123; System.out.println("method with wrapper argument");&#125;public static void main(String[] args) &#123; //calling overloaded method AutoboxingTest autoTest = new AutoboxingTest(); int value = 3; autoTest.test(value); //no autoboxing Integer iValue = value; autoTest.test(iValue); //no autoboxing&#125;Output:method with primitive argumentmethod with wrapper argument 重载的方法参数既有原始类型，又有对象类型版本的时候，调用时候jvm会根据实际参数优先选择最匹配的方法，如果没有才会进行自动装箱或拆箱。 要注意的事项自动装箱和拆箱可以使代码变得简洁,但是其也存在一些问题和极端情况下的问题，以下几点需要我们加强注意。 对象相等比较这是一个比较容易出错的地方，”==“可以用于原始值进行比较，也可以用于对象进行比较，当用于对象与对象之间比较时，比较的不是对象代表的值，而是检查两个对象是否是同一对象，这个比较过程中没有自动装箱发生。进行对象值比较不应该使用”==“，而应该使用对象对应的equals方法。看一个能说明问题的例子。12345678910111213141516171819202122232425262728293031323334353637383940public class AutoboxingTest &#123; public static void main(String args[]) &#123; // Example 1: == comparison pure primitive – no autoboxing int i1 = 1; int i2 = 1; System.out.println("i1==i2 : " + (i1 == i2)); // true // Example 2: equality operator mixing object and primitive Integer num1 = 1; // autoboxing int num2 = 1; System.out.println("num1 == num2 : " + (num1 == num2)); // true // Example 3: special case - arises due to autoboxing in Java Integer obj1 = 1; // autoboxing will call Integer.valueOf() Integer obj2 = 1; // same call to Integer.valueOf() will return same // cached Object System.out.println("obj1 == obj2 : " + (obj1 == obj2)); // true // Example 4: equality operator - pure object comparison Integer one = new Integer(1); // no autoboxing Integer anotherOne = new Integer(1); System.out.println("one == anotherOne : " + (one == anotherOne)); // false // Example 5 Integer largeObj1 = 1000; Integer largeObj2 = 1000; System.out.println("largeObj1 == largeObj2 : " + (largeObj1 == largeObj2)); // false &#125;&#125;Output:i1==i2 : truenum1 == num2 : trueobj1 == obj2 : trueone == anotherOne : falselargeObj1 == largeObj2 : false 值得注意的是第三个小例子，这是一种极端情况。obj1和obj2的初始化都发生了自动装箱操作。但是处于节省内存的考虑，JVM会缓存-128到127的Integer对象。因为obj1和obj2实际上是同一个对象。所以使用”==“比较返回true。 容易混乱的对象和原始数据值另一个需要避免的问题就是混乱使用对象和原始数据值，一个具体的例子就是当我们在一个原始数据值与一个对象进行比较时，如果这个对象没有进行初始化或者为Null，在自动拆箱过程中obj.xxxValue，会抛出NullPointerException,如下面的代码123456private static Integer count;//NullPointerException on unboxingif( count &lt;= 0)&#123; System.out.println("Count is not started yet");&#125; 缓存的对象这个问题就是我们上面提到的极端情况，在Java中，会对-128到127的Integer对象进行缓存，当创建新的Integer对象时，如果符合这个这个范围，并且已有存在的相同值的对象，则返回这个对象，否则创建新的Integer对象。 在Java中另一个节省内存的例子就是字符串常量池,感兴趣的同学可以了解一下。 生成无用对象增加GC压力因为自动装箱会隐式地创建对象，像前面提到的那样，如果在一个循环体中，会创建无用的中间对象，这样会增加GC压力，拉低程序的性能。所以在写循环时一定要注意代码，避免引入不必要的自动装箱操作。 总的来说，自动装箱和拆箱着实为开发者带来了很大的方便，但是在使用时也是需要格外留意，避免引起出现文章提到的问题。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>Java语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单例这种设计模式]]></title>
    <url>%2F2015%2F01%2F11%2F2015-01-11-looking-into-singleton%2F</url>
    <content type="text"><![CDATA[随着我们编写代码的深入，我们或多或少都会接触到设计模式，其中单例(Singleton)模式应该是我们耳熟能详的一种模式。本文将比较特别的介绍一下Java设计模式中的单例模式。 概念单例模式，又称单件模式或者单子模式，指的是一个类只有一个实例，并且提供一个全局访问点。 实现思路 在单例的类中设置一个private静态变量sInstance，sInstance类型为当前类，用来持有单例唯一的实例。 将（无参数）构造器设置为private，避免外部使用new构造多个实例。 提供一个public的静态方法，如getInstance，用来返回该类的唯一实例sInstance。 其中上面的单例的实例可以有以下几种创建形式，每一种实现都需要保证实例的唯一性。 饿汉式饿汉式指的是单例的实例在类装载时进行创建。如果单例类的构造方法中没有包含过多的操作处理，饿汉式其实是可以接受的。 饿汉式的常见代码如下,当SingleInstance类加载时会执行 private static SingleInstance sInstance = new SingleInstance() 初始化了唯一的实例，然后getInstance()直接返回sInstance即可。12345678910public class SingleInstance &#123; private static SingleInstance sInstance = new SingleInstance(); private SingleInstance() &#123; &#125; public static SingleInstance getInstance() &#123; return sInstance; &#125;&#125; 饿汉式的问题 如果构造方法中存在过多的处理，会导致加载这个类时比较慢，可能引起性能问题。 如果使用饿汉式的话，只进行了类的装载，并没有实质的调用，会造成资源的浪费。 懒汉式懒汉式指的是单例实例在第一次使用时进行创建。这种情况下避免了上面饿汉式可能遇到的问题。 但是考虑到多线程的并发操作，我们不能简简单单得像下面代码实现。 123456789101112public class SingleInstance &#123; private static SingleInstance sInstance; private SingleInstance() &#123; &#125; public static SingleInstance getInstance() &#123; if (null == sInstance) &#123; sInstance = new SingleInstance(); &#125; return sInstance; &#125;&#125; 上述的代码在多个线程密集调用getInstance时，存在创建多个实例的可能。比如线程A进入null == sInstance这段代码块，而在A线程未创建完成实例时，如果线程B也进入了该代码块，必然会造成两个实例的产生。 synchronized修饰方法使用synchrnozed修饰getInstance方法可能是最简单的一个保证多线程保证单例唯一性的方法。synchronized修饰的方法后，当某个线程进入调用这个方法，该线程只有当其他线程离开当前方法后才会进入该方法。所以可以保证getInstance在任何时候只有一个线程进入。 123456789101112public class SingleInstance &#123; private static SingleInstance sInstance; private SingleInstance() &#123; &#125; public static synchronized SingleInstance getInstance() &#123; if (null == sInstance) &#123; sInstance = new SingleInstance(); &#125; return sInstance; &#125;&#125; 但是使用synchronized修饰getInstance方法后必然会导致性能下降，而且getInstance是一个被频繁调用的方法。虽然这种方法能解决问题，但是不推荐。 双重检查加锁使用双重检查加锁，首先进入该方法时进行12345678910111213141516171819双重检查加锁保证了多线程下只创建一个实例，并且加锁代码块只在实例创建的之前进行同步。如果实例已经创建后，进入该方法，则不会执行到同步块的代码。```Javapublic class SingleInstance &#123; private static volatile SingleInstance sInstance; private SingleInstance() &#123; &#125; public static SingleInstance getInstance() &#123; if (null == sInstance) &#123; synchronized (SingleInstance.class) &#123; if (null == sInstance) &#123; sInstance = new SingleInstance(); &#125; &#125; &#125; return sInstance; &#125;&#125; volatile是什么Volatile是轻量级的synchronized，它在多处理器开发中保证了共享变量的“可见性”。可见性的意思是当一个线程修改一个共享变量时，另外一个线程能读到这个修改的值。使用volatile修饰sInstance变量之后，可以确保多个线程之间正确处理sInstance变量。关于volatile，可以访问深入分析Volatile的实现原理了解更多。 利用static机制在Java中，类的静态初始化会在类被加载时触发，我们利用这个原理，可以实现利用这一特性，结合内部类，可以实现如下的代码，进行懒汉式创建实例。123456789101112public class SingleInstance &#123; private SingleInstance() &#123; &#125; public static SingleInstance getInstance() &#123; return SingleInstanceHolder.sInstance; &#125; private static class SingleInstanceHolder &#123; private static SingleInstance sInstance = new SingleInstance(); &#125;&#125; 关于这种机制，可以具体了解双重检查锁定与延迟初始化。 好奇问题真的只有一个对象么其实，单例模式并不能保证实例的唯一性，只要我们想办法的话，还是可以打破这种唯一性的。以下几种方法都能实现。 使用反射，虽然构造器为非公开，但是在反射面前就不起作用了。 如果单例的类实现了cloneable，那么还是可以拷贝出多个实例的。 Java中的对象序列化也有可能导致创建多个实例。避免使用readObject方法。 使用多个类加载器加载单例类，也会导致创建多个实例并存的问题。 单例可以继承么单例类能否被继承需要分情况而定。 可以继承的情况当子类是父类单例类的内部类时，继承是可以的。1234567891011121314151617181920212223public class BaseSingleton &#123; private static volatile BaseSingleton sInstance; private BaseSingleton() &#123; &#125; public static BaseSingleton getInstance() &#123; if (null == sInstance) &#123; synchronized(BaseSingleton.class) &#123; if (null == sInstance) &#123; sInstance = new BaseSingleton(); &#125; &#125; &#125; return sInstance; &#125; public static class MySingleton extends BaseSingleton &#123; &#125;&#125; 但是上面仅仅是编译和执行上允许的，但是继承单例没有实际的意义，反而会变得更加事倍功半，其代价要大于新写一个单例类。感兴趣的童鞋可以尝试折腾一下。 不可以继承的情况如果子类为单独的类，非单例类的内部类的话，那么在编译时就会出错Implicit super constructor BaseSingleton() is not visible for default constructor. Must define an explicit constructor，主要原因是单例类的构造器是private，解决方法是讲构造器设置为可见，但是这样做就无法保证单例的唯一性。所以这种方式不可以继承。 总的来说，单例类不要继承。 单例 vs static变量全局静态变量也可以实现单例的效果，但是使用全局变量无法保证只创建一个实例，而且使用全局变量的形式，需要团队的约束，执行起来可能会出现问题。 关于GC因为单例类中又一个静态的变量持有单例的实例，所以相比普通的对象，单例的对象更不容易被GC回收掉。单例对象的回收应该发生在其类加载器被GC回收掉之后，一般不容易出现。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>单例模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的字符串常量池]]></title>
    <url>%2F2014%2F12%2F21%2F2014-12-21-string-literal-pool%2F</url>
    <content type="text"><![CDATA[原文链接:https://droidyue.com/blog/2014/12/21/string-literal-pool-in-java/ Java中字符串对象创建有两种形式，一种为字面量形式，如str 123456789101112***## 工作原理当代码中出现**字面量形式**创建字符串对象时，JVM首先会对这个字面量进行检查，如果字符串常量池中存在相同内容的字符串对象的引用，则将这个引用返回，否则新的字符串对象被创建，然后将这个引用放入字符串常量池，并返回该引用。***## 举例说明### 字面量创建形式```JAVAString str1 = &quot;droid&quot;; JVM检测这个字面量，这里我们认为没有内容为droid的对象存在。JVM通过字符串常量池查找不到内容为1234如果接下来有这样一段代码```JAVAString str2 = &quot;droid&quot;; 同样JVM还是要检测这个字面量，JVM通过查找字符串常量池，发现内容为”droid”字符串对象存在，于是将已经存在的字符串对象的引用返回给变量str2。注意这里不会重新创建新的字符串对象。 验证是否为str1和str2是否指向同一对象，我们可以通过这段代码1System.out.println(str1 == str2); 结果为true。 使用new创建当我们使用了new来构造字符串对象的时候，不管字符串常量池中有没有相同内容的对象的引用，新的字符串对象都会创建。因此我们使用下面代码测试一下。12String str3 = new String("droid");System.out.println(str1 == str3); intern对于上面使用new创建的字符串对象，如果想将这个对象的引用加入到字符串常量池，可以使用intern方法。 调用intern后，首先检查字符串常量池中是否有该对象的引用(两个对象equal)，如果存在，则将这个引用返回给变量，否则将引用加入并返回给变量。123456789String str4 = str3.intern();System.out.println(str4 == str1);String str5 = new String("abc");String str6 = str5.intern();System.out.println(str5 == str6);Output:truefalse JDK中对intern方法的注释如下： When the intern method is invoked, if the pool already contains astring equal to this String object as determined bythe equals(Object) method, then the string from the pool isreturned. Otherwise, this String object is added to thepool and a reference to this String object is returned. 疑难问题前提条件？字符串常量池实现的前提条件就是Java中String对象是不可变的，这样可以安全保证多个变量共享同一个对象。如果Java中的String对象可变的话，一个引用操作改变了对象的值，那么其他的变量也会受到影响，显然这样是不合理的。 引用 or 对象字符串常量池中存放的时引用还是对象，这个问题是最常见的。字符串常量池存放的是对象引用，不是对象。在Java中，对象都创建在堆内存中。 更新验证，收到的很多评论也在讨论这个问题，我简单的进行了验证。 验证环境1234567891011121322:18:54-androidyue~/Videos$ cat /etc/os-releaseNAME=FedoraVERSION=&quot;17 (Beefy Miracle)&quot;ID=fedoraVERSION_ID=17PRETTY_NAME=&quot;Fedora 17 (Beefy Miracle)&quot;ANSI_COLOR=&quot;0;34&quot;CPE_NAME=&quot;cpe:/o:fedoraproject:fedora:17&quot;22:19:04-androidyue~/Videos$ java -versionjava version &quot;1.7.0_25&quot;OpenJDK Runtime Environment (fedora-2.3.12.1.fc17-x86_64)OpenJDK 64-Bit Server VM (build 23.7-b01, mixed mode) 验证思路：以下的Java程序读取一个大小为82M的视频文件，以字符串形式进行intern操作。1222:01:17-androidyue~/Videos$ ll -lh | grep why_to_learn.mp4-rw-rw-r--. 1 androidyue androidyue 82M Oct 20 2013 why_to_learn.mp4 验证代码12345678910111213141516171819202122232425262728293031323334353637383940414243import java.io.BufferedReader;import java.io.FileNotFoundException;import java.io.FileReader;import java.io.IOException;public class TestMain &#123; private static String fileContent; public static void main(String[] args) &#123; fileContent = readFileToString(args[0]); if (null != fileContent) &#123; fileContent = fileContent.intern(); System.out.println("Not Null"); &#125; &#125; private static String readFileToString(String file) &#123; BufferedReader reader = null; try &#123; reader = new BufferedReader(new FileReader(file)); StringBuffer buff = new StringBuffer(); String line; while ((line = reader.readLine()) != null) &#123; buff.append(line); &#125; return buff.toString(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (null != reader) &#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; return null; &#125;&#125; 由于字符串常量池存在于堆内存中的永久代，适用于Java8之前(Java8是怎么样？待查)。我们通过设置永久代一个很小的值来进行验证。如果字符串对象存在字符串常量池中，那么必然抛出permgen space```错误。12```JAVAjava -XX:PermSize=6m TestMain ~/Videos/why_to_learn.mp4 运行证明程序没有抛出OOM，其实这个不能很好的证明存储的是对象还是引用。 但是这个至少证明了字符串的实际内容对象char[]不存放在字符串常量池中。既然这样的话，其实字符串常量池存储字符串对象还是字符串对象的引用反而不是那么重要。但个人还是倾向于存储的为引用。(根据前intern的官方注释：this String object is added to thepool and a reference to this String object is returned，可以认为字符串常量池里存储的是String对象，但不包含char[]，char[]的内容存放在堆中) 优缺点字符串常量池的好处就是减少相同内容字符串的创建，节省内存空间。 如果硬要说弊端的话，就是牺牲了CPU计算时间来换空间。CPU计算时间主要用于在字符串常量池中查找是否有内容相同对象的引用。不过其内部实现为HashTable，所以计算成本较低。 关于hashtable。In the Hotspot JVM interned string are held in the string table, which is a Hashtable mapping object pointers to symbols (i.e. Hashtable), and is held in the permanent generation. For both the symbol table (see above) and the string table all entries are held in a canonicalized form to improve efficiency and ensure each entry only appears once. 具体可以了解一下这篇文章http://blog.jamesdbloom.com/JVMInternals.html#string_table GC回收？因为字符串常量池中持有了共享的字符串对象的引用，这就是说是不是会导致这些对象无法回收？ 首先问题中共享的对象一般情况下都比较小。据我查证了解，在早期的版本中确实存在这样的问题，但是随着弱引用的引入，目前这个问题应该没有了。 关于这个问题，可以具体了解这片文章interned Strings : Java Glossary intern使用？关于使用intern的前提就是你清楚自己确实需要使用。比如，我们这里有一份上百万的记录，其中记录的某个值多次为美国加利福尼亚州，我们不想创建上百万条这样的字符串对象，我们可以使用intern只在内存中保留一份即可。关于intern更深入的了解请参考深入解析String#intern。 总有例外？你知道下面的代码，会创建几个字符串对象，在字符串常量池中保存几个引用么？1String test = "a" + "b" + "c"; 答案是只创建了一个对象，在常量池中也只保存一个引用。我们使用javap反编译看一下即可得知。1234567891011121314$ javap -c TestInternedPoolGCCompiled from &quot;TestInternedPoolGC.java&quot;public class TestInternedPoolGC extends java.lang.Object&#123;public TestInternedPoolGC(); Code: 0: aload_0 1: invokespecial #1; //Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: returnpublic static void main(java.lang.String[]) throws java.lang.Exception; Code: 0: ldc #2; //String abc 2: astore_1 3: return 看到了么，实际上在编译期间，已经将这三个字面量合成了一个。这样做实际上是一种优化，避免了创建多余的字符串对象，也没有发生字符串拼接问题。关于字符串拼接，可以查看Java细节：字符串的拼接。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>String</tag>
        <tag>Java语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是HTTP长轮询]]></title>
    <url>%2F2014%2F12%2F01%2F2014-12-01-http-long-polling%2F</url>
    <content type="text"><![CDATA[Web应用程序从诞生起，都是围绕着client/server模型开发的，Web客户端始终是事务的发起者，从服务器请求数据。因此，没有客户端首先提出请求的情况下，服务器没有独立发送或推送数据给客户端的机制。 什么是HTTP长轮询?为了克服以上缺陷，Web应用程序开发人员可以通过一种称为HTTP长轮询的技术，客户端轮询请求获取服务器的新信息。服务器保持请求连通，直到有新数据可用。一旦可用，服务器响应并返回新信息。当客户端收到新信息时，立即发送另一个新请求，并一直重复此操作。这有效地模拟了服务器推送功能。 有关Python，Ruby和JavaScript的HTTP长轮询实现，可以参考这里 使用HTTP长轮询的注意事项在开发和扩展方面，使用HTTP长轮询在构建实时交互性时有几件事需要考虑。 随着使用量的增长，您将如何编排您的实时后端？ 当移动设备在WiFi和蜂窝网络之间快速切换或丢失连接并且IP地址发生变化时，长时间轮询会自动重新建立连接吗？ 使用长轮询，您可以管理消息队列并追踪错过的消息吗？ 长轮询是否提供跨多个服务器的负载均衡或故障转移支持？ 当使用HTTP长轮询进行服务器推送构建实时应用程序时，您必须开发自己的通信管理系统。这意味着您将负责更新，维护和扩展您的后端基础架构。]]></content>
      <categories>
        <category>网络通信</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机原理：(六) JVM运行时数据区]]></title>
    <url>%2F2014%2F11%2F12%2F2014-10-13-jvm-theory-6%2F</url>
    <content type="text"><![CDATA[原文链接: https://blog.csdn.net/luanlouis/article/details/40043991 JVM运行时数据区(JVM Runtime Area)其实就是指JVM在运行期间，其对计算机内存空间的划分和分配。本文将通过以下几个话题来讨论JVM运行时数据区。 JVM运行时数据区里有什么？ 虚拟机栈是什么？虚拟机栈里有什么？ 栈帧是什么？栈帧里有什么？ 方法区是什么？方法区里有什么？]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>字节码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机原理：(五) class文件中的方法表集合]]></title>
    <url>%2F2014%2F11%2F12%2F2014-11-17-jvm-theory-5%2F</url>
    <content type="text"><![CDATA[原文链接: https://blog.csdn.net/luanlouis/article/details/41113695 概述方法表集合是指由若干个方法表（method_info）组成的集合。对于在类中定义的若干个，经过JVM编译成class文件后，会将相应的method方法信息组织到一个叫做方法表集合的结构中，方法表集合是一个类数组结构，如下图所示： method方法的描述-方法表集合在class文件中的位置method方法的描述-方法表集合紧跟在字段表集合的后面，如下图所示： 接下来让我们看看Method_info 结构体是怎么组织method方法信息的: 一个类中的method方法应该包含哪些信息？—-method_info结构体的定义对于一个方法的表示，我们根据我们可以概括的信息如下所示： 实际上JVM还会对method方法的描述添加其他信息，我们将在后面详细讨论。如上图中的method_info结构体的定义，该结构体的定义跟描述field字段 的field_info结构体的结构几乎完全一致,如下图所示。 方法表的结构体由：访问标志(access_flags)、名称索引(name_index)、描述索引(descriptor_index)、属性表(attribute_info)集合组成。 访问标志(access_flags)： method_info结构体最前面的两个字节表示的访问标志（access_flags），记录这这个方法的作用域、静态or非静态、可变性、是否可同步、是否本地方法、是否抽象等信息，实际上不止这些信息，我们后面会详细介绍访问标志这两个字节的每一位具体表示什么意思。 名称索引(name_index)： 紧跟在访问标志（access_flags）后面的两个字节称为名称索引，这两个字节中的值指向了常量池中的某一个常量池项，这个方法的名称以UTF-8格式的字符串存储在这个常量池项中。如public void methodName(),很显然，“methodName”则表示着这个方法的名称，那么在常量池中会有一个CONSTANT_Utf8_info格式的常量池项，里面存储着“methodName”字符串，而mehodName()方法的方法表中的名称索引则指向了这个常量池项。 描述索引(descriptor_index)： 描述索引表示的是这个方法的特征或者说是签名，一个方法会有若干个参数和返回值，而若干个参数的数据类型和返回值的数据类型构成了这个方法的描述，其基本格式为：(参数数据类型描述列表)返回值数据类型。我们将在后面继续讨论。 属性表(attribute_info)集合： 这个属性表集合非常重要，方法的实现被JVM编译成JVM的机器码指令，机器码指令就存放在一个Code类型的属性表中；如果方法声明要抛出异常，那么异常信息会在一个Exceptions类型的属性表中予以展现。Code类型的属性表可以说是非常复杂的内容，也是本文最难的地方。 接下来，我们将一一击破它们，看看它们到底是怎么表示的。 访问标志(access_flags)—记录着method方法的访问信息访问标志（access_flags）共占有2 个字节，分为 16 位，这 16位 表示的含义如下所示： 举例：某个类中定义了如下方法：12public static synchronized final void greeting()&#123; &#125; greeting()方法的修饰符有：public、static、synchronized、final 这几个修饰符修饰，那么相对应地，greeting()方法的访问标志中的ACC_PUBLIC、ACC_STATIC、ACC_SYNCHRONIZED、ACC_FINAL标志位都应该是1，即： 从上图中可以看出访问标志的值应该是二进制00000000 00111001,即十六进制0x0039。我们将在文章的最后一个例子中证实这里点。 名称索引和描述符索引—-一个方法的签名紧接着访问标志（access_flags）后面的两个字节，叫做名称索引(name_index)，这两个字节中的值是指向了常量池中某个常量池项的索引，该常量池项表示这这个方法名称的字符串。 方法描述符索引(descrptor_index)是紧跟在名称索引后面的两个字节，这两个字节中的值跟名称索引中的值性质一样，都是指向了常量池中的某个常量池项。这两个字节中的指向的常量池项，是表示了方法描述符的字符串。 所谓的方法描述符，实质上就是指用一个什么样的字符串来描述一个方法，方法描述符的组成如下图所示： 举例：对于如下定义的的greeting()方法，我们来看一下对应的method_info结构体中的名称索引和描述符索引信息是怎样组织的。12public static synchronized final void greeting()&#123; &#125; 如下图所示,method_info结构体的名称索引中存储了一个索引值x，指向了常量池中的第x项，第 x项表示的是字符串”greeting”,即表示该方法名称是”greeting”；描述符索引中的y 值指向了常量池的第y项，该项表示字符串”()V”，即表示该方法没有参数，返回值是void类型。 属性表集合–记录方法的机器指令和抛出异常等信息属性表集合记录了某个方法的一些属性信息，这些信息包括： 这个方法的代码实现，即方法的*可执行的机器指令 这个方法声明的要抛出的异常信息 这个方法是否被@deprecated注解表示 这个方法是否是编译器自动生成的 属性表（attribute_info）结构体的一般结构如下所示： Code类型的属性表–method方法中的机器指令的信息Code类型的属性表(attribute_info)可以说是class文件中最为重要的部分，因为它包含的是JVM可以运行的机器码指令，JVM能够运行这个类，就是从这个属性中取出机器码的。除了要执行的机器码，它还包含了一些其他信息，如下所示： Code属性表的组成部分： 机器指令—-code： 目前的JVM使用一个字节表示机器操作码，即对JVM底层而言，它能表示的机器操作码不多于2的 8 次方，即 256个。class文件中的机器指令部分是class文件中最重要的部分，并且非常复杂，本文的重点不止介绍它，我将专门在一片博文中讨论它，敬请期待。 异常处理跳转信息—exception_table： 如果代码中出现了try{}catch{}块，那么try{}块内的机器指令的地址范围记录下来，并且记录对应的catch{}块中的起始机器指令地址，当运行时在try块中有异常抛出的话，JVM会将catch{}块对应机器指令地址传递给PC寄存器，从而实现指令跳转； Java源码行号和机器指令的对应关系—LineNumberTable属性表： 编译器在将java源码编译成class文件时，会将源码中的语句行号跟编译好的机器指令关联起来，这样的class文件加载到内存中并运行时，如果抛出异常，JVM可以根据这个对应关系，抛出异常信息，告诉我们我们的源码的多少行有问题，方便我们定位问题。这个信息不是运行时必不可少的信息，但是默认情况下，编译器会生成这一项信息，如果你项取消这一信息，你可以使用-g:none或-g:lines来取消或者要求设置这一项信息。如果使用了-g:none来生成class文件，class文件中将不会有LineNumberTable属性表，造成的影响就是 将来如果代码报错，将无法定位错误信息报错的行，并且如果项调试代码，将不能在此类中打断点（因为没有指定行号。） 局部变量表描述信息—-LocalVariableTable属性表： 局部变量表信息会记录栈帧局部变量表中的变量和java源码中定义的变量之间的关系，这个信息不是运行时必须的属性，默认情况下会生成到class文件中。你可以根据javac指令的-g:none或者-g:vars选项来取消或者设置这一项信息。 它有什么作用呢？ 当我们使用IDE进行开发时，最喜欢的莫过于它们的代码提示功能了。如果在项目中引用到了第三方的jar包，而第三方的包中的class文件中有无LocalVariableTable属性表的区别如下所示： Code属性表结构体的解释： attribute_name_index,属性名称索引，占有2个字节，其内的值指向了常量池中的某一项，该项表示字符串“Code”; attribute_length,属性长度，占有 4个字节，其内的值表示后面有多少个字节是属于此Code属性表的； max_stack,操作数栈深度的最大值，占有 2 个字节，在方法执行的任意时刻，操作数栈都不应该超过这个值，虚拟机的运行的时候，会根据这个值来设置该方法对应的栈帧(Stack Frame)中的操作数栈的深度； max_local,最大局部变量数目，占有 2个字节，其内的值表示局部变量表所需要的存储空间大小； code_length,机器指令长度，占有 4 个字节，表示跟在其后的多少个字节表示的是机器指令； code,机器指令区域，该区域占有的字节数目由 code_length中的值决定。JVM最底层的要执行的机器指令就存储在这里； exception_table_length,异常表长度，占有2个字节，如果在方法代码中出现了try{} catch()形式的结构，该值不会为空，紧跟其后会跟着若干个exception_table结构体，以表示异常捕获情况； exception_table，显式异常表，占有8 个字节，start_pc,end_pc,handler_pc中的值都表示的是PC计数器中的指令地址。exception_table表示的意思是：如果字节码从第start_pc行到第end_pc行之间出现了catch_type所描述的异常类型，那么将跳转到handler_pc行继续处理。 attribute_count,属性计数器，占有 2 个字节，表示Code属性表的其他属性的数目 attribute_info,表示Code属性表具有的属性表，它主要分为两个类型的属性表：“LineNumberTable”类型和“LocalVariableTable”类型。“LineNumberTable”类型的属性表记录着Java源码和机器指令之间的对应关系“LocalVariableTable”类型的属性表记录着局部变量描述 举例：如下定义Simple类，使用javac -g:none Simple.java 编译出Simple.class 文件，并使用javap -v Simple &gt; Simple.txt查看反编译的信息，然后看Simple.class文件中的方法表集合是怎样组织的： 123456public class Simple &#123; public static synchronized final void greeting()&#123; int a = 10; &#125; &#125; Simple.class文件组织信息如下所示： 如上所示，方法表集合使用了蓝色线段圈了起来。 请注意：方法表集合的头两个字节，即方法表计数器（method_count）的值是0x0002，它表示该类中有2 个方法。细心的读者会注意到，我们的Simple.java中就定义了一个greeting()方法，为什么class文件中会显示有两个方法呢？？ 在Simple.classz中出现了两个方法表，分别代表构造方法&lt; init &gt;()和 greeting()方法，现在让我们分别来讨论这两个方法： Simple.class 中的() 方法: 方法访问标志(access_flags)： 占有 2个字节，值为0x0001,即标志位的第 16 位为 1，所以该()方法的修饰符是：ACC_PUBLIC; 名称索引(name_index)： 占有 2 个字节，值为 0x0004，指向常量池的第 4项，该项表示字符串“”，即该方法的名称是“”; 描述符索引(descriptor_index): 占有 2 个字节，值为0x0005,指向常量池的第 5 项，该项表示字符串“()V”，即表示该方法不带参数，并且无返回值（构造函数确实也没有返回值）； 属性计数器（attribute_count): 占有 2 个字节，值为0x0001,表示该方法表中含有一个属性表，后面会紧跟着一个属性表； 属性表的名称索引(attribute_name_index)：占有 2 个字节，值为0x0006,指向常量池中的第6 项，该项表示字符串“Code”，表示这个属性表是Code类型的属性表； 属性长度（attribute_length）：占有4个字节，值为0x0000 0011，即十进制的 17，表明后续的 17 个字节可以表示这个Code属性表的属性信息； 操作数栈的最大深度（max_stack）：占有2个字节，值为0x0001,表示栈帧中操作数栈的最大深度是1； 局部变量表的最大容量（max_variable）：占有2个字节，值为0x0001, JVM在调用该方法时，根据这个值设置栈帧中的局部变量表的大小； 机器指令数目(code_length)：占有4个字节，值为0x0000 0005,表示后续的5 个字节 0x2A 、0xB7、 0x00、0x01、0xB1表示机器指令; 机器指令集(code[code_length])：这里共有 5个字节，值为0x2A 、0xB7、 0x00、0x01、0xB1； 显式异常表集合（exception_table_count）： 占有2 个字节，值为0x0000,表示方法中没有需要处理的异常信息； Code属性表的属性表集合（attribute_count）： 占有2 个字节，值为0x0000，表示它没有其他的属性表集合，因为我们使用了-g:none 禁止编译器生成Code属性表的 LineNumberTable 和LocalVariableTable; Simple.class 中的greeting() 方法: 方法访问标志(access_flags)： 占有 2个字节，值为 0x0039 ,即二进制的00000000 00111001,即标志位的第11、12、13、16位为1，根据上面讲的方法标志位的表示，可以得到该greeting()方法的修饰符有：ACC_SYNCHRONIZED、ACC_FINAL、ACC_STATIC、ACC_PUBLIC; 名称索引(name_index)： 占有 2 个字节，值为 0x0007，指向常量池的第 7 项，该项表示字符串“greeting”，即该方法的名称是“greeting”; 描述符索引(descriptor_index): 占有 2 个字节，值为0x0005,指向常量池的第 5 项，该项表示字符串“()V”，即表示该方法不带参数，并且无返回值； 属性计数器（attribute_count): 占有 2 个字节，值为0x0001,表示该方法表中含有一个属性表，后面会紧跟着一个属性表； 属性表的名称索引(attribute_name_index)：占有 2 个字节，值为0x0006,指向常量池中的第6 项，该项表示字符串“Code”，表示这个属性表是Code类型的属性表； 属性长度（attribute_length）：占有4个字节，值为0x0000 0010，即十进制的16，表明后续的16个字节可以表示这个Code属性表的属性信息； 操作数栈的最大深度（max_stack）：占有2个字节，值为0x0001,表示栈帧中操作数栈的最大深度是1； 局部变量表的最大容量（max_variable）：占有2个字节，值为0x0001, JVM在调用该方法时，根据这个值设置栈帧中的局部变量表的大小； 机器指令数目(code_length)：占有4 个字节，值为0x0000 0004,表示后续的4个字节0x10、 0x0A、 0x3B、0xB1的是表示机器指令; 机器指令集(code[code_length])：这里共有4 个字节，值为0x10、 0x0A、 0x3B、0xB1 ； 显式异常表集合（exception_table_count）： 占有2 个字节，值为0x0000,表示方法中没有需要处理的异常信息； Code属性表的属性表集合（attribute_count）： 占有2 个字节，值为0x0000，表示它没有其他的属性表集合，因为我们使用了-g:none 禁止编译器生成Code属性表的 LineNumberTable 和LocalVariableTable; Exceptions类型的属性表—-method方法声明的要抛出的异常信息 有些方法在定义的时候，会声明该方法会抛出什么类型的异常，如下定义一个Interface接口，它声明了sayHello()方法，抛出Exception异常：1234public interface Interface &#123; public void sayHello() throws Exception; &#125; 现在让我们看一下Exceptions类型的属性表(attribute_info)结构体是怎样组织的： 如上图所示，Exceptions类型的属性表(attribute_info)结构体由一下元素组成： 属性名称索引(attribute_name_index)：占有 2个字节，其中的值指向了常量池中的表示”Exceptions“字符串的常量池项； 属性长度(attribute_length)：它比较特殊，占有4个字节，它的值表示跟在其后面多少个字节表示异常信息； 异常数量(number_of_exceptions)：占有2 个字节，它的值表示方法声明抛出了多少个异常，即表示跟在其后有多少个异常名称索引； 异常名称索引(exceptions_index_table)：占有2个字节，它的值指向了常量池中的某一项，该项是一个CONSTANT_Class_info类型的项，表示这个异常的完全限定名称； Exceptions类型的属性表的长度计算 如果某个方法定义中，没有声明抛出异常，那么，表示该方法的方法表(method_info)结构体中的属性表集合中不会有Exceptions类型的属性表；换句话说，如果方法声明了要抛出的异常，方法表(method_info)结构体中的属性表集合中必然会有Exceptions类型的属性表，并且该属性表中的异常数量不小于1。 我们假设异常数量中的值为 N，那么后面的异常名称索引的数量就为N，它们总共占有的字节数为N*2，而异常数量占有2个字节，那么将有下面的这个关系式： 属性长度(attribute_length)中的值= 2 + 2异常数量(number_of_exceptions)中的值*Exceptions类型的属性表（attribute_info）的长度=2+4+属性长度(attribute_length)中的值 举例：将上面定义的Interface接口类编译成class文件，然后我们查看Interface.class文件，找出方法表集合所在位置和相应的数据，并辅助javap -v Inerface查看常量池信息，如下图所示： 由于sayHello()方法是在的Interface接口类中声明的，它没有被实现，所以它对应的方法表(method_info)结构体中的属性表集合中没有Code类型的属性表。 方法计数器（methods_count）中的值为0x0001，表明其后的方法表(method_info)就一个,即我们就定义了一个方法，其后会紧跟着一个方法表(method_info)结构体； 方法的访问标志（access_flags）的值是0x0401，二进制是00000100 00000001,第6位和第16位是1，对应上面的标志位信息，可以得出它的访问标志符有：ACC_ABSTRACT、ACC_PUBLIC。细心的读者可能会发现，在上面声明的sayHello()方法中并没有声明为abstract类型啊。确实如此，这是因为编译器对于接口内声明的方法自动加上ACC_ABSTRACT标志。 名称索引（name_index）中的值为0x0005，0x0005指向了常量池的第5项，第五项表示的字符串为“sayHello”，即表示的方法名称是sayHello 描述符索引(descriptor_index)中的值为0x0006,0x0006指向了常量池中的第6项，第6项表示的字符串为“()V” 表示这个方法的无入参，返回值为void类型 属性表计数器(attribute_count)中的值为0x0001,表示后面的属性表的个数就1个，后面紧跟着一个attribute_info结构体； 属性表（attribute_info）中的属性名称索引(attribute_name_index)中的值为0x0007，0x0007指向了常量池中的第7 项，第 7项指向字符串“Exceptions”，即表示该属性表表示的异常信息； 属性长度（attribute_length）中的值为：0x00000004,即后续的4个字节将会被解析成属性值； 异常数量（number_of_exceptions）中的值为0x0001,表示这个方法声明抛出的异常个数是1个； 异常名称索引(exception_index_table)中的值为0x0008,指向了常量池中的第8项，第8项表示的是CONSTANT_Class_info类型的常量池项，表示“java/lang/Exception”，即表示此方法抛出了java.lang.Exception异常。 IDE代码提示功能实现的基本原理现在对于企业级的开发，开发者们越来越依赖IDE如Intellij IDEA、Eclipse、MyEclipse、NetBeans等，利用他们提供的高级功能，可以极大地提高编码的速度和效率。 每个IDE都提供了代码提示功能，它们实现的基本原理其实就是IDE针对它们项目下的包中所有的class文件进行建模，解析出它们的方法信息，当我们一定的条件时，IDE会自动地将合适条件的方法列表展示给开发者，供开发者使用。 在上面将Code属性表的时候也讲了，如果编译的第三方包，没有LocalVariableTable属性表信息，IDE的提示信息会稍有不同： 写在后面以上就是Class文件的方法表集合的全部内容。 读者可能觉得本文关于方法表的Code属性表讨论的不够深入，在讨论Code属性表的时候，我简单介绍了它的两个属性表LineNumberTable 和LocalVariableTable这两个在有什么实际作用，但是没有详细第介绍它们，并且在列举的例子中，刻意地使用了 -g:none 选项 ，以使生成的class文件没有这两项信息，这么做是因为Code 属性太过复杂，而本文主要是想让读者了解的是 方法表集合，所以就生成了最精简的Code属性表，以减少读者的负担。 接下来的一篇文章，我打算专门来讨论Code属性表，揭开Code属性表的所有秘密，敬请关注~~ 本文还引出了一个需要讨论的话题：就是Code属性表中的机器指令,机器指令的运行要依赖于JVM体系结构的设计机制，理解机器指令的运行机制，这将是根非常非常难啃的骨头]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>字节码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机原理：(七) JVM机器指令集]]></title>
    <url>%2F2014%2F11%2F12%2F2016-01-08-jvm-theory-7%2F</url>
    <content type="text"><![CDATA[原文链接: https://blog.csdn.net/luanlouis/article/details/50412126 Java虚拟机对运行时虚拟机栈（JVM Stack）的组织Java虚拟机在运行时会为每一个线程在内存中分配了一个虚拟机栈(VM Stack) ，来表示线程的运行状态和信息，虚拟机栈中的元素称之为栈帧（JVM stack frame）,每一个栈帧表示这对一个方法的调用信息。如下所示： 上述的描述可能会有点抽象，为了给读者一个直观的感受，我们定义一个简单的Java类，然后执行这个运行这个类，逐步分析整个Java虚拟机的运行时信息的组织的。 方法调用过程在JVM中是如何表示的我们将定义如下带有main方法的简单类org.louis.jvm.codeset.Bootstrap.java ，逐步分析该类在JVM中是如何表示的，方法是如何一步步运行的：123456789public static void main(String[] args) &#123; String name = "Louis"; greeting(name); &#125; public static void greeting(String name) &#123; System.out.println("Hello,"+name); &#125; 当我们将Bootstrap.java 编译成Bootstrap.class 并运行这段程序的时候，在JVM复杂的运行逻辑中，会有以下几步： 首先JVM会先将这个Bootstrap.class 信息加载到 内存中的方法区(Method Area)中。 Bootstrap.class 中包含了常量池信息，方法的定义以及编译后的方法实现的二进制形式的机器指令，所有的线程共享一个方法区，从中读取方法定义和方法的指令集。 接着，JVM会在Heap堆上为Bootstrap.class 创建一个Class实例用来表示Bootstrap.class 的类实例。 JVM开始执行main方法，这时会为main方法创建一个栈帧，以表示main方法的整个执行过程（我会在后面章节中详细展开这个过程）； main方法在执行的过程之中，调用了greeting静态方法，则JVM会为greeting方法创建一个栈帧，推到虚拟机栈顶（我会在后面章节中详细展开这个过程）。 当greeting方法运行完成后，则greeting方法出栈，main方法继续运行； JVM方法调用的过程是通过栈帧来实现的，那么，方法的指令是如何运行的呢？弄清楚这个之前，我们要先了解对于JVM而言，方法的结构是什么样的。 我们知道，class 文件时 JVM能够识别的二进制文件，其中通过特定的结构描述了每个方法的定义。 JVM在编译Bootstrap.java 的过程中，类在将源代码编译成二进制机器码的同时，会判断其中的每一个方法的三个信息类： 在运行时会使用到的局部变量的数量（作用是：当JVM为方法创建栈帧的时候，在栈帧中为该方法创建一个局部变量表，来存储方法指令在运算时的局部变量值） 其机器指令执行时所需要的最大的操作数栈的大小（当JVM为方法创建栈帧的时候，在栈帧中为方法创建一个操作数栈，保证方法内指令可以完成工作） 方法的参数的数量 经过编译之后，我们可以得到main方法和greeting方法的信息如下： 注： 上述编译后的信息全部都存储在Bootstrap.class 文件中，并按照这Class文件格式的形式存储，关于Class文件格式的定义，我在前几篇文章中已经做了非常详尽的介绍，如果您全部阅读了，那么相信您已经可以“读懂” class 文件了。 JVM运行main方法的过程： 为main方法创建栈帧：JVM解析main方法，发现其 局部变量的数量为 2，操作数栈的数量为1， 则会为main方法创建一个栈帧（Frame Stack），并将其加入虚拟机栈中 完成栈帧初始化： main栈帧创建完成后，会将栈帧push 到虚拟机栈中，现在有两步重要的事情要做： a). 计算PC值。PC 是指令计数器，其内部的值决定了JVM虚拟机下一步应该执行哪一个机器指令，而机器指令存放在方法区，我们需要让PC的值指向方法区的main方法上； 初始化 PC = main方法在方法区指令的地址+0； b). 局部变量的初始化。main方法有个入参(String[] args) ，JVM已经在main所在的栈帧的局部变量表中为其空出来了一个slot ，我们需要将 args 的引用值初始化到局部变量表中； 接着JVM开始读取PC指向的机器指令。如上图所示，main方法的指令序列：12 10 4c 2b b8 20 12 b1 ，通过JVM虚拟机指令集规范，可以将这个指令序列解析成以下Java汇编语言: 机器指令 汇编语言 解释 对栈帧的影响 0x12 0x10 ldc #16 将常量池中第16个常量池项引用推到操作数栈栈顶。常量池第16项是CONSTANT_UTF-8_INFO项，表示”Louis”字符串 0x4c astore_1 操作数栈的栈顶元素出栈，将栈顶元素的值赋给index=1 的局部变量表元素上。这里等价于：name = “Louis”. 0x2b aload_1 将局部变量表中index=1的元素的值推到操作数栈栈顶 0xb8 0x20 0x12 invokestatic #18 0xb8表示机器指令invokestatic,操作数是0x20 &lt;&lt; 8 0x12 = 18，操作数18表示指向常量池第18项，该项是main方法的符号引用：org/louis/jvm/codeset/Bootstrap.greeting:(Ljava/lang/String;)V 当JVM执行这条语句的时候，会做以下几件事： a).方法符号引用校验。会校验这个方法的符号引用，按照这个符号规则 在常量池中查找是否有这个方法的定义，如果找到了此方法的定义，则表示解析成功。如果是方法greeting:(Ljava/lang/String;)V没有找到，JVM会抛出错误NoSuchMethodError b).为新的方法调用创建新的栈帧。然后JVM会为此方法greeting创建一个新的栈帧(VM stack)，并根据greeting中操作数栈的大小和局部变量的数量分别创建相应大小的操作数栈；然后将此栈帧推到虚拟机栈的栈顶。c).更新PC指令计数器的值。将当前PC程序计数器的值记录到greeting栈帧中(方法出口)，当greeting执行完成后，以便恢复PC值。更新PC的值，使下一条执行的指令地址指向greeting方法的指令开始部分。这条语句会使当前的main方法执行暂停，使JVM进入对greeting方法的执行当中当greeting方法执行完成后，才会恢复PC程序计数器的值指向当前下一条指令。 0xb1 return 返回 当main方法调用greeting()时， JVM会为greeting方法创建一个栈帧，用以表示对greeting方法的调用，具体栈帧信息如下： 具体的greeting方法的机器码表示的含义如下图所示： 机器指令 汇编语言 解释 对栈帧的影响 b2 20 1a getstatic #26 获取指定类的静态域，并将其值压入栈顶.将常量池中的第26个符号引用推到操作数栈中： #26： // Field java/lang/System.out:Ljava/io/PrintStream; bb 20 20 new #32 创建一个对象，并将其引用值压入栈顶。创建一个java/lang/StringBuider实例,将其压入栈顶。 #32: // class java/lang/StringBuilder 59 dup 复制操作数栈栈顶的值，并插入到栈顶 12 22 ldc #34 从运行时常量池中提取数据推入操作数栈,将“Hello” String引用复制到 操作数栈中 #34:// String Hello b7 20 24 invokespecial #36 调用超类构造方法，实例初始化方法，私有方法。此处调用StringBuilder(String)构造方法，并将结果推到栈顶 #36: // Method java/lang/StringBuilder.”“:(Ljava/lang/String;)V 2a aload_0 将第一个局部变量的引用推到栈顶。当前局部变量表的第一个局部变量引用是 ：“Louis”，即将Louis推到栈顶 b6 20 26 invokevirtual #38 StringBuilder实例的 append(String ) 方法，表示：”Hello,”+”Louis”，结果保留在栈顶。 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; b6 20 2a invokevirtual #42 调用StringBuilder实例的toString()方法，结果保留在栈顶。 // Method java/lang/StringBuilder.toString:()Ljava/lang/String; b6 20 2e invokevirtual #46 调用System.out.println(String)方法 // Method java/io/PrintStream.println:(Ljava/lang/String;)V b1 return 结束返回 JVM对一个方法执行的基本策略一般地，对于java方法的执行，在JVM在其某一特定线程的虚拟机栈(JVM Stack) 中会为方法分配一个局部变量表，一个操作数栈，用以存储方法的运行过程中的中间值存储。 由于JVM的指令是基于栈的，即大部分的指令的执行，都伴随着操作数的出栈和入栈。所以在学习JVM的机器指令的时候，一定要铭记一点： 每个机器指令的执行，对操作数栈和局部变量的影响，充分地了解了这个机制，你就可以非常顺畅地读懂class文件中的二进制机器指令了。 如下是栈帧信息的简化图，在分析JVM指令时，脑海中对栈帧有个清晰的认识： 机器指令的格式所谓的机器指令，就是只有机器才能够认识的二进制代码。一个机器指令分为两部分组成： a). 如上图所示JVM虚拟机的操作码是由一个字节组成的，也就是说对于JVM虚拟机而言，其指令的数量最多为 2^8,即 256个; b). 上图中的操作码如:b2,bb,59….等等都是表示某一特定的机器指令，为了方便我们识别，其分别有相应的助记符：getstatic,new,dup…. 这样方便我们理解。 机器指令的执行模式—基于操作数栈的模式对于传统的物理机而言，大部分的机器指令的设计都是寄存器的，物理机内设置若干个寄存器，用以存储机器指令运行过程中的值，寄存器的数量和支持的指令的个数决定了这个机器的处理能力。但是Java虚拟机的设计的机制并不是这样的，Java虚拟机使用操作数栈 来存储机器指令的运算过程中的值。所有的操作数的操作，都要遵循出栈和入栈的规则，所以在《Java虚拟机规范》中，你会发现有很多机器指令都是关于出栈入栈的操作。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>字节码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机原理：(三) class访问标志、类索引、父类索引、接口索引集合]]></title>
    <url>%2F2014%2F11%2F12%2F2014-11-12-jvm-theory-3%2F</url>
    <content type="text"><![CDATA[原文链接: https://blog.csdn.net/luanlouis/article/details/41039269 讲完了class文件中的常量池，我们就相当于克服了class文件中最麻烦的模块了。现在，我们来看一下class文件中紧接着常量池后面的几个东西：访问标志、类索引、父类索引、接口索引集合。 访问标志、类索引、父类索引、接口索引集合 在class文件中的位置 访问标志(access_flags)能够表示什么？访问标志（access_flags）紧接着常量池后，占有两个字节，总共16位，如下图所示： 当JVM在编译某个类或者接口的源代码时，JVM会解析出这个类或者接口的访问标志信息，然后，将这些标志设置到访问标志（access_flags）这16个位上。JVM会考虑如下设置如下访问表示信息： 我们知道，每个定义的类或者接口都会生成class文件（这里也包括内部类，在某个类中定义的静态内部类也会单独生成一个class文件）。 对于定义的类，JVM在将其编译成class文件时，会将class文件的访问标志的第11位设置为1 。第11位叫做ACC_SUPER标志位； 对于定义的接口，JVM在将其编译成class文件时，会将class文件的访问标志的第8位 设置为 1 。第8位叫做ACC_INTERFACE标志位； class文件表示的类或者接口的访问权限有public类型的和包package类型的。 如果类或者接口被声明为public类型的，那么，JVM将其编译成class文件时，会将class文件的访问标志的第16位设置为1 。第16位叫做ACC_PUBLIC标志符； 类是否为抽象类型的，即我们定义的类有没有被abstract关键字修饰，即我们定义的类是否为抽象类。若是 会将class文件的访问标志的第7位设置为1 。第7位叫做ACC_ABSTRACT标志位。 另外值得注意的是，对于定义的接口，JVM在编译接口的时候也会对class文件的访问标志上的ACC_ABSTRACT标志位设置为 1； 该类是否被声明了final类型,即表示该类不能被继承。 此时JVM会在编译class文件的过程中，会将class文件的访问标志的第12位设置为 1 。第12位叫做ACC_FINAL标志位； 如果我们这个class文件不是JVM通过java源代码文件编译而成的，而是用户自己通过class文件的组织规则生成的，那么，一般会对class文件的访问标志第4位设置为 1 。通过JVM编译源代码产生的class文件此标志位为 0，第4位叫做ACC_SYNTHETIC标志位； 枚举类，对于定义的枚举类如：public enum EnumTest{….}，JVM也会对此枚举类编译成class文件，这时，对于这样的class文件，JVM会对访问标志第2位设置为 1 ，以表示它是枚举类。第2位叫做ACC_ENUM标志位； 注解类，对于定义的注解类如：public @interface{…..},JVM会对此注解类编译成class文件，对于这样的class文件，JVM会将访问标志第3位设置为1，以表示这是个注解类，第3位叫做ACC_ANNOTATION标志位。 当JVM确定了上述标志位的值后，就可以确定访问标志（access_flags）的值了。实际上JVM上述标志会根据上述确定的标志位的值，对这些标志位的值取或，便得到了访问标志（access_flags）。如下图所示: 举例：定义一个最简单的类Simple.java，使用编译器编译成class文件，然后观察class文件中的访问标志的值，以及使用javap -v Simple查看访问标志。123public class Simple &#123; &#125; 使用UltraEdit查看编译成的class文件，如下图所示： 上述的图中黄色部分表示的是常量池部分，具体为什么是常量池部分不是本文的重点，有兴趣的读者可以参考我的《Java虚拟机原理图解》系列关于常量池的博客，你就可以很轻松地识别常量它们了。 常量池后面紧跟着就是访问标志，它的十六进制值为0x0021,二进制的值为：00000000 00100001，由二进制的1的位数可以得出第11、16位为1，分别对应ACC_SUPER标志位和ACC_PUBLIC标志位。 也可以通过一下运算： 0x0021 = 0x0001 | 0x0020, 即： 访问标志表示的标志是ACC_PUBLIC + ACC_SUPER 为了验证我们的运算，使用javap -v Simple查看反编译信息如下：（小技巧：使用javap -v Simple指令的结果展示在命令提示符下显示不友好，一般我是使用javap -v Simple &gt; temp.txt，将结果重定向到文件中，然后查看文件） 类索引(this_class)是什么？我们知道一般情况下一个Java类源文件经过JVM编译会生成一个class文件，也有可能一个Java类源文件中定义了其他类或者内部类，这样编译出来的class文件就不止一个，但每一个class文件表示某一个类，至于这个class表示哪一个类，便可以通过类索引这个数据项来确定。JVM通过类的完全限定名确定是某一个类。 类索引的作用，就是为了指出class文件所描述的这个类叫什么名字。 类索引紧接着访问标志的后面，占有两个字节，在这两个字节中存储的值是一个指向常量池的一个索引，该索引指向的是CONSTANT_Class_info常量池项， 以上面定义的Simple.class 为例，如下图所示，查看他的类索引在什么位置和取什么值。 由上可知，它的类索引值为0x0001,那么，它指向了常量池中的第一个常量池项，那我们再看一下常量池中的信息。使用javap -v Simple,常量池中有以下信息： 可以看到常量池中的第一项是CONSTANT_Class_info项，它表示一个”com/louis/jvm/Simple”的类名。即类索引是告诉我们这个class文件所表示的是哪一个类。 父类索引(super_class)是什么？Java支持单继承模式，除了java.lang.Object 类除外，每一个类都会有且只有一个父类。class文件中紧接着类索引(this_class)之后的两个字节区域表示父类索引，跟类索引一样，父类索引这两个字节中的值指向了常量池中的某个常量池项CONSTANT_Class_info，表示该class表示的类是继承自哪一个类。 接口索引集合(interfaces)是什么？一个类可以不实现任何接口，也可以实现很多个接口，为了表示当前类实现的接口信息，class文件使用了如下结构体描述某个类的接口实现信息: 由于类实现的接口数目不确定，所以接口索引集合的描述的前部分叫做接口计数器（interfaces_count），接口计数器占用两个字节，其中的值表示着这个类实现了多少个接口，紧跟着接口计数器的部分就是接口索引部分了，每一个接口索引占有两个字节，接口计数器的值代表着后面跟着的接口索引的个数。接口索引和类索引和父类索引一样，其内的值存储的是指向了常量池中的CONSTANT_Class_info的索引，表示着这个接口的完全限定名。 举例：定义一个Worker接口，然后类Programmer实现这个Worker接口，然后我们观察Programmer的接口索引集合是怎样表示的。12345678910111213public interface Worker&#123; public void work(); &#125; public class Programmer implements Worker &#123; @Override public void work() &#123; System.out.println("I'm Programmer,Just coding...."); &#125; &#125;]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>字节码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机原理：(四) class文件中的字段集合]]></title>
    <url>%2F2014%2F11%2F12%2F2014-11-14-jvm-theory-4%2F</url>
    <content type="text"><![CDATA[原文链接: https://blog.csdn.net/luanlouis/article/details/41046443 概述 字段表集合是指由若干个字段表（field_info）组成的集合。对于在类中定义的若干个字段，经过JVM编译成class文件后，会将相应的字段信息组织到一个叫做字段表集合的结构中，字段表集合是一个类数组结构，如下图所示： 这里所讲的字段是指在类中定义的静态或者非静态的变量，而不是在类中的方法内定义的变量。请注意区别。 比如，如果某个类中定义了5个字段，那么，JVM在编译此类的时候，会生成5个字段表（field_info）信息,然后将字段表集合中的字段计数器的值设置成5，将5个字段表信息依次放置到字段计数器的后面。 字段表集合在class文件中的位置字段表集合紧跟在class文件的接口索引集合结构的后面，如下图所示： Java中的一个Field字段应该包含那些信息？(字段表field_info结构体的定义) 针对上述的字段表示，JVM虚拟机规范规定了field_info结构体来描述字段，其表示信息如下： 下面我将一一讲解FIeld_info的组成元素： 访问标志（access_flags）、名称索引（name_index）、描述索引（descriptor_index）、属性表集合 field字段的访问标志如上图所示定义的field_info结构体，field字段的访问标志(access_flags)占有两个字节，它能够表述的信息如下所示： 举例：如果我们在某个类中有定义field域：private static String str;，那么在访问标志上，第15位ACC_PRIVATE和第13位ACC_STATIC标志位都应该为1。field域str的访问标志信息应该是如下所示： 如上图所示，str字段的访问标志的值为0x000A，它由两个修饰符ACC_PRIVATE和ACC_STATIC组成。 根据给定的访问标志（access_flags），我们可以通过以下运算来得到这个域有哪些修饰符： 上面列举的str字段的访问标志的值为000A，那么分别域上述的标志符的特征值取&amp;，结果为1的只有ACC_PRIVATE和ACC_STATIC，所以该字段的标志符只有有ACC_PRIVATE和ACC_STATIC。 字段的数据类型表示和字段名称表示class文件对数据类型的表示如下图所示： field字段名称，我们定义了一个形如private static String str的field字段，其中”str”就是这个字段的名称。 class文件将字段名称和field字段的数据类型表示作为字符串存储在常量池中。在field_info结构体中，紧接着访问标志的，就是字段名称索引和字段描述符索引，它们分别占有两个字节，其内部存储的是指向了常量池中的某个常量池项的索引，对应的常量池项中存储的字符串，分别表示该字段的名称和字段描述符。 属性表集合—–静态field字段的初始化在定义field字段的过程中，我们有时候会很自然地对field字段直接赋值，如下所示：12public static final int MAX=100; public int count=0; 对于虚拟机而言，上述的两个field字段赋值的时机是不同的： 对于非静态（即无static修饰）的field字段的赋值将会出现在实例构造方法&lt; init &gt;()中 对于静态的field字段，有两个选择：1、在静态构造方法()中进行；2 、使用ConstantValue属性进行赋值 Sun javac编译器对于静态field字段的初始化赋值策略 目前的Sun javac编译器的选择是：如果使用final和static同时修饰一个field字段，并且这个字段是基本类型或者String类型的，那么编译器在编译这个字段的时候，会在对应的field_info结构体中增加一个ConstantValue类型的结构体，在赋值的时候使用这个ConstantValue进行赋值；如果该field字段并没有被final修饰，或者不是基本类型或者String类型，那么将在类构造方法&lt; cinit &gt;()中赋值。 对于上述的public static final init MAX=100; javac编译器在编译此field字段构建field_info结构体时，除了访问标志、名称索引、描述符索引外，会增加一个ConstantValue类型的属性表。 实例解析：定义如下一个简单的Simple类，然后通过查看Simple.class文件内容并结合javap -v Simple生成的常量池内容，分析str field字段的结构：1234public class Simple &#123; private transient static final String str ="This is a test"; &#125; 字段计数器中的值为0x0001,表示这个类就定义了1个field字段 字段的访问标志是0x009A,二进制是00000000 10011010，即第9、12、13、15位标志位为1，这个字段的标志符有：ACC_TRANSIENT、ACC_FINAL、ACC_STATIC、ACC_PRIVATE; 名称索引中的值为0x0005,指向了常量池中的第5项，为“str”,表明这个field字段的名称是str； 描述索引中的值为0x0006,指向了常量池中的第6项，为”Ljava/lang/String;”，表明这个field字段的数据类型是java.lang.String类型； 属性表计数器中的值为0x0001,表明field_info还有一个属性表； 属性表名称索引中的值为0x0007,指向常量池中的第7项，为“ConstantValue”,表明这个属性表的名称是ConstantValue，即属性表的类型是ConstantValue类型的； 属性长度中的值为0x0002，因为此属性表是ConstantValue类型，它的值固定为2； 常量值索引 中的值为0x0008,指向了常量池中的第8项，为CONSTANT_String_info类型的项，表示“This is a test” 的常量。在对此field赋值时，会使用此常量对field赋值。 您还需要了解什么简单地说，对于一个类而言，它有两部分组成：field字段和 method方法。本文主要介绍了field字段，那还剩些一个method方法啦。method方法可是说是class文件中最为重要的一部分了，它包含了方法的实现代码，即机器指令，机器指令是整个class文件的核心。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>字节码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[译文：理解Java中的弱引用]]></title>
    <url>%2F2014%2F10%2F12%2F2014-10-12-understanding-weak-references%2F</url>
    <content type="text"><![CDATA[原文链接： https://droidyue.com/blog/2014/10/12/understanding-weakreference-in-java/ 不久之前，我面试了一些求职Java高级开发工程师的应聘者。我常常会面试他们说，“你能给我介绍一些Java中得弱引用吗？”，如果面试者这样说，“嗯，是不是垃圾回收有关的？”，我就会基本满意了，我并不期待回答是一篇诘究本末的论文描述。 然而事与愿违，我很吃惊的发现，在将近20多个有着平均5年开发经验和高学历背景的应聘者中，居然只有两个人知道弱引用的存在，但是在这两个人之中只有一个人真正了解这方面的知识。在面试过程中，我还尝试提示一些东西，来看看有没有人突然说一声“原来是这个啊”，结果很是让我失望。我开始困惑，为什么这块的知识如此不被重视，毕竟弱引用是一个很有用途的特性，况且这个特性已经在7年前 Java 1.2发布时便引入了。 好吧，这里我不期待你看完本文之后成为一个弱引用方面的专家，但是我认为至少你应该了解什么是弱引用，如何使用它们，并且什么场景使用。既然它们是一些不知名的概念，我简单就着前面的三个问题来说明一下。 强引用(Strong Reference)强引用就是我们经常使用的引用，其写法如下1StringBuffer buffer = new StringBuffer(); 上面创建了一个StringBuffer对象，并将这个对象的（强）引用存到变量buffer中。是的，就是这个小儿科的操作（请原谅我这样的说法）。强引用最重要的就是它能够让引用变得强（Strong），这就决定了它和垃圾回收器的交互。具体来说，如果一个对象通过一串强引用链接可到达(Strongly reachable)，它是不会被回收的。如果你不想让你正在使用的对象被回收，这就正是你所需要的。 但是强引用如此之强在一个程序里，将一个类设置成不可被扩展是有点不太常见的，当然这个完全可以通过类标记成final实现。或者也可以更加复杂一些，就是通过内部包含了未知数量具体实现的工厂方法返回一个接口(Interface)。举个例子，我们想要使用一个叫做Widget的类，但是这个类不能被继承，所以无法增加新的功能。 但是我们如果想追踪Widget对象的额外信息，我们该怎么办？ 假设我们需要记录每个对象的序列号，但是由于Widget类并不包含这个属性，而且也不能扩展导致我们也不能增加这个属性。其实一点问题也没有，HashMap完全可以解决上述的问题。1serialNumberMap.put(widget, widgetSerialNumber); 这表面看上去没有问题，但是widget对象的强引用很有可能会引发问题。我们可以确信当一个widget序列号不需要时，我们应该将这个条目从map中移除。如果我们没有移除的话，可能会导致内存泄露，亦或者我们手动移除时删除了我们正在使用的widgets，会导致有效数据的丢失。其实这些问题很类似，这就是没有垃圾回收机制的语言管理内存时常遇到的问题。但是我们不用去担心这个问题，因为我们使用的时具有垃圾回收机制的Java语言。 另一个强引用可能带来的问题就是缓存,尤其是像图片这样的大文件的缓存。假设你有一个程序需要处理用户提供的图片，通常的做法就是做图片数据缓存，因为从磁盘加载图片代价很大，并且同时我们也想避免在内存中同时存在两份一样的图片数据。 缓存被设计的目的就是避免我们去再次加载哪些不需要的文件。你会很快发现在缓存中会一直包含一个到已经指向内存中图片数据的引用。使用强引用会强制图片数据留在内存，这就需要你来决定什么时候图片数据不需要并且手动从缓存中移除，进而可以让垃圾回收器回收。因此你再一次被强制做垃圾回收器该做的工作，并且人为决定是该清理到哪一个对象。 弱引用(Weak Reference)弱引用简单来说就是将对象留在内存的能力不是那么强的引用。使用WeakReference，垃圾回收器会帮你来决定引用的对象何时回收并且将对象从内存移除。创建弱引用如下1WeakReference&lt;Widget&gt; weakWidget = new WeakReference&lt;Widget&gt;(widget); 使用weakWidget.get()就可以得到真实的Widget对象，因为弱引用不能阻挡垃圾回收器对其回收，你会发现（当没有任何强引用到widget对象时）使用get时突然返回null。1234567Object object =new Object();WeakReference&lt;Object&gt; reference = new WeakReference&lt;Object&gt;(object);System.out.println(reference.get());object = null;System.gc();Thread.sleep(30000);System.out.println(reference.get()); 123Output:java.lang.Object@76ccd017null 解决上述的widget序列数记录的问题，最简单的办法就是使用Java内置的WeakHashMap类。WeakHashMap和HashMap几乎一样，唯一的区别就是它的键（不是值!!!）使用WeakReference引用。当WeakHashMap的键标记为垃圾的时候，这个键对应的条目就会自动被移除。这就避免了上面不需要的Widget对象手动删除的问题。使用WeakHashMap可以很便捷地转为HashMap或者Map。1234567Object object =new Object();WeakHashMap&lt;Object, String&gt; map = new WeakHashMap&lt;&gt;();map.put(object,"test");object = null;System.gc();Thread.sleep(2000);System.out.println(map.size()); 12Output:0 引用队列(Reference Queue)一旦弱引用对象开始返回null，该弱引用指向的对象就被标记成了垃圾。而这个弱引用对象（非其指向的对象）就没有什么用了。通常这时候需要进行一些清理工作。比如WeakHashMap会在这时候移除没用的条目来避免保存无限制增长的没有意义的弱引用。 引用队列可以很容易地实现跟踪不需要的引用。当你在构造WeakReference时传入一个ReferenceQueue对象，当该引用指向的对象被标记为垃圾的时候，这个引用对象会自动地加入到引用队列里面。接下来，你就可以在固定的周期，处理传入的引用队列，比如做一些清理工作来处理这些没有用的引用对象。 四种引用Java中实际上有四种强度不同的引用，从强到弱它们分别是，强引用，软引用，弱引用和虚引用。上面部分介绍了强引用和弱引用，下面介绍剩下的两个，软引用和虚引用。 软引用（Soft Reference）软引用基本上和弱引用差不多，只是相比弱引用，它阻止垃圾回收期回收其指向的对象的能力强一些。如果一个对象是弱引用可到达，那么这个对象会被垃圾回收器接下来的回收周期销毁。但是如果是软引用可以到达，那么这个对象会停留在内存更时间上长一些。当内存不足时垃圾回收器才会回收这些软引用可到达的对象。 由于软引用可到达的对象比弱引用可达到的对象滞留内存时间会长一些，我们可以利用这个特性来做缓存。这样的话，你就可以节省了很多事情，垃圾回收器会关心当前哪种可到达类型以及内存的消耗程度来进行处理。 虚引用 （Phantom Reference）与软引用，弱引用不同，虚引用指向的对象十分脆弱，我们不可以通过get方法来得到其指向的对象。它的唯一作用就是当其指向的对象被回收之后，自己被加入到引用队列，用作记录该引用指向的对象已被销毁。 当弱引用的指向对象变得弱引用可到达，该弱引用就会加入到引用队列。这一操作发生在对象析构或者垃圾回收真正发生之前。理论上，这个即将被回收的对象是可以在一个不符合规范的析构方法里面重新复活。但是这个弱引用会销毁。虚引用只有在其指向的对象从内存中移除掉之后才会加入到引用队列中。其get方法一直返回null就是为了阻止其指向的几乎被销毁的对象重新复活。 虚引用使用场景主要有两个。它允许你知道具体何时其引用的对象从内存中移除。而实际上这是Java中唯一的方式。这一点尤其表现在处理类似图片的大文件的情况。当你确定一个图片数据对象应该被回收，你可以利用虚引用来判断这个对象回收之后在继续加载下一张图片。这样可以尽可能地避免可怕的内存溢出错误。 第二点，虚引用可以避免很多析构时的问题。finalize方法可以通过创建强引用指向快被销毁的对象来让这些对象重新复活。然而，一个重写了finalize方法的对象如果想要被回收掉，需要经历两个单独的垃圾收集周期。在第一个周期中，某个对象被标记为可回收，进而才能进行析构。但是因为在析构过程中仍有微弱的可能这个对象会重新复活。这种情况下，在这个对象真实销毁之前，垃圾回收器需要再次运行。因为析构可能并不是很及时，所以在调用对象的析构之前，需要经历数量不确定的垃圾收集周期。这就意味着在真正清理掉这个对象的时候可能发生很大的延迟。这就是为什么当大部分堆被标记成垃圾时还是会出现烦人的内存溢出错误。 使用虚引用，上述情况将引刃而解，当一个虚引用加入到引用队列时，你绝对没有办法得到一个销毁了的对象。因为这时候，对象已经从内存中销毁了。因为虚引用不能被用作让其指向的对象重生，所以其对象会在垃圾回收的第一个周期就将被清理掉。 显而易见，finalize方法不建议被重写。因为虚引用明显地安全高效，去掉finalize方法可以虚拟机变得明显简单。当然你也可以去重写这个方法来实现更多。这完全看个人选择。 总结 引用队列的作用，对于弱引用或者软引用变为垃圾时，会将这些引用对象加入队列中，便于我们了解这些引用对象指向的对象是否标记为垃圾 对于虚引用，其目的是判断某个对象是否被回收了。 当弱/软引用对象所指向的对象变得弱/软引用可达到的时候，弱/软引用对象不一定被移入引用队列，这个需要依赖于GC周期，一旦GC经过（对于软引用来说是内存吃紧下）才会对这些对象进行标记为垃圾。 虚引用的使用方法是，通过判断虚引用对象本身是否在引用队列中，来判断其指向的对象是否已经从内存移除。我想看到这里，很多人开始发牢骚了，为什么你要讲一个过去十年的老古董API呢，好吧，以我的经验看，很多的Java程序员并不是很了解这个知识，我认为有一些深入的理解是很必要的，同时我希望大家能从本文中收获一些东西。 我想看到这里，很多人开始发牢骚了，为什么你要讲一个过去十年的老古董API呢，好吧，以我的经验看，很多的Java程序员并不是很了解这个知识，我认为有一些深入的理解是很必要的，同时我希望大家能从本文中收获一些东西。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>Java语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机原理：(一)class文件结构]]></title>
    <url>%2F2014%2F10%2F09%2F2014-10-09-jvm-theory-1%2F</url>
    <content type="text"><![CDATA[原文链接: https://blog.csdn.net/luanlouis/article/details/39892027 作为Java程序猿，我们知道，我们写好的.java 源代码，最后会被Java编译器编译成后缀为.class的文件，该类型的文件是由字节组成的文件，又叫字节码文件。那么，class字节码文件里面到底是有什么呢？它又是怎样组织的呢？让我们先来大概了解一下他的组成结构吧。 NO.1 魔数(magic number)所有的由Java编译器编译而成的class文件的前4个字节都是“0xCAFEBABE” 它的作用在于：当JVM在尝试加载某个文件到内存中来的时候，会首先判断此class文件有没有JVM认为可以接受的“签名”，即JVM会首先读取文件的前4个字节，判断该4个字节是否是“0xCAFEBABE”，如果是，则JVM会认为可以将此文件当作class文件来加载并使用。 NO.2 版本号(minor_version,major_version)随着Java本身的发展，Java语言特性和JVM虚拟机也会有相应的更新和增强。目前我们能够用到的JDK版本如：1.5，1.6，1.7，还有现如今最新的1.8。发布新版本的目的在于：在原有的版本上增加新特性和相应的JVM虚拟机的优化。而随着主版本发布的次版本，则是修改相应主版本上出现的bug。我们平时只需要关注主版本就可以了。 主版本号和次版本号在class文件中各占两个字节，副版本号占用第5、6两个字节，而主版本号则占用第7，8两个字节。JDK1.0的主版本号为45，以后的每个新主版本都会在原先版本的基础上加1。若现在使用的是JDK1.7编译出来的class文件，则相应的主版本号应该是51,对应的7，8个字节的十六进制的值应该是 0x33。 一个 JVM实例只能支持特定范围内的主版本号 （Mi 至Mj） 和 0 至特定范围内 （0 至 m） 的副版本号。假设一个 Class 文件的格式版本号为 V， 仅当Mi.0 ≤ v ≤ Mj.m成立时，这个 Class 文件才可以被此 Java 虚拟机支持。不同版本的 Java 虚拟机实现支持的版本号也不同，高版本号的 Java 虚拟机实现可以支持低版本号的 Class 文件，反之则不成立。 JVM在加载class文件的时候，会读取出主版本号，然后比较这个class文件的主版本号和JVM本身的版本号，如果JVM本身的版本号 &lt; class文件的版本号，JVM会认为加载不了这个class文件，会抛出我们经常见到的”java.lang.UnsupportedClassVersionError: Bad version number in .class file “ Error 错误；反之，JVM会认为可以加载此class文件，继续加载此class文件。 小贴士： 有时候我们在运行程序时会抛出这个Error 错误：”java.lang.UnsupportedClassVersionError: Bad version number in .class file”。上面已经揭示了出现这个问题的原因，就是在于当前尝试加载class文件的JVM虚拟机的版本 低于class文件的版本。解决方法：1.重新使用当前jvm编译源代码，然后再运行代码；2.将当前JVM虚拟机更新到class文件的版本。 怎样查看class文件的版本号？ 可以借助于文本编辑工具，直接查看该文件的7，8个字节的值，确定class文件是什么版本的。 当然快捷的方式使用JDK自带的javap工具，如当前有Programmer.class 文件，进入此文件所在的目录，然后执行 ”javap -v Programmer“,结果会类似如下所示： NO3.常量池计数器(constant_pool_count)常量池是class文件中非常重要的结构，它描述着整个class文件的字面量信息。 常量池是由一组constant_pool结构体数组组成的，而数组的大小则由常量池计数器指定。常量池计数器constant_pool_count 的值 =constant_pool表中的成员数+ 1（设计者就将第0项保留出来了，以备后患）。constant_pool表的索引值只有在大于 0 且小于constant_pool_count时才会被认为是有效的。 NO4.常量池数据区(constant_pool[contstant_pool_count-1])常量池，constant_pool是一种表结构,它包含 Class 文件结构及其子结构中引用的所有字符串常量、 类或接口名、字段名和其它常量。 常量池中的每一项都具备相同的格式特征——第一个字节作为类型标记用于识别该项是哪种类型的常量，称为 “tag byte” 。常量池的索引范围是 1 至constant_pool_count−1。常量池的具体细节我们会稍后讨论。 NO5.访问标志(access_flags)访问标志，access_flags 是一种掩码标志，用于表示某个类或者接口的访问权限及基础属性。 NO6.类索引(this_class)类索引，this_class的值必须是对constant_pool表中项目的一个有效索引值。constant_pool表在这个索引处的项必须为CONSTANT_Class_info 类型常量，表示这个 Class 文件所定义的类或接口。 NO7.父类索引(super_class)父类索引，对于类来说，super_class 的值必须为 0 或者是对constant_pool 表中项目的一个有效索引值。如果它的值不为 0，那 constant_pool 表在这个索引处的项必须为CONSTANT_Class_info 类型常量，表示这个 Class 文件所定义的类的直接父类。当前类的直接父类，以及它所有间接父类的access_flag 中都不能带有ACC_FINAL 标记。对于接口来说，它的Class文件的super_class项的值必须是对constant_pool表中项目的一个有效索引值。constant_pool表在这个索引处的项必须为代表 java.lang.Object 的 CONSTANT_Class_info 类型常量 。如果 Class 文件的 super_class的值为 0，那这个Class文件只可能是定义的是java.lang.Object类，只有它是唯一没有父类的类。 NO8.接口计数器(interfaces_count)接口计数器，interfaces_count的值表示当前类或接口的直接父接口数量。 NO9.接口信息数据区(interfaces[interfaces_count])接口表，interfaces[]数组中的每个成员的值必须是一个对constant_pool表中项目的一个有效索引值， 它的长度为 interfaces_count。每个成员 interfaces[i] 必须为 CONSTANT_Class_info类型常量，其中 0 ≤ i &lt;interfaces_count。在interfaces[]数组中，成员所表示的接口顺序和对应的源代码中给定的接口顺序（从左至右）一样，即interfaces[0]对应的是源代码中最左边的接口。 NO10.字段计数器(fields_count)字段计数器，fields_count的值表示当前 Class 文件 fields[]数组的成员个数。 fields[]数组中每一项都是一个field_info结构的数据项，它用于表示该类或接口声明的类字段或者实例字段。 NO11.字段信息数据区(fields[fields_count])字段表，fields[]数组中的每个成员都必须是一个fields_info结构的数据项，用于表示当前类或接口中某个字段的完整描述。 fields[]数组描述当前类或接口声明的所有字段，但不包括从父类或父接口继承的部分。 NO12.方法计数器(methods_count)方法计数器， methods_count的值表示当前Class 文件 methods[]数组的成员个数。Methods[]数组中每一项都是一个method_info结构的数据项。 NO13.方法信息数据区(methods[methods_count])方法表，methods[] 数组中的每个成员都必须是一个method_info结构的数据项，用于表示当前类或接口中某个方法的完整描述。如果某个method_info 结构的access_flags 项既没有设置 ACC_NATIVE 标志也没有设置ACC_ABSTRACT 标志，那么它所对应的方法体就应当可以被 Java 虚拟机直接从当前类加载，而不需要引用其它类。 method_info结构可以表示类和接口中定义的所有方法，包括实例方法、类方法、实例初始化方法方法和类或接口初始化方法方法 。methods[]数组只描述当前类或接口中声明的方法，不包括从父类或父接口继承的方法。 NO14.属性计数器(attributes_count)属性计数器，attributes_count的值表示当前 Class 文件attributes表的成员个数。attributes表中每一项都是一个attribute_info 结构的数据项。 NO15.属性信息数据区(attributes[attributes_count])属性表，attributes 表的每个项的值必须是attribute_info结构。 在Java 7 规范里，Class文件结构中的attributes表的项包括下列定义的属性： InnerClasses 、 EnclosingMethod 、 Synthetic 、Signature、SourceFile，SourceDebugExtension 、Deprecated、RuntimeVisibleAnnotations 、RuntimeInvisibleAnnotations以及BootstrapMethods属性。 对于支持 Class 文件格式版本号为 49.0 或更高的 Java 虚拟机实现，必须正确识别并读取attributes表中的Signature、RuntimeVisibleAnnotations和RuntimeInvisibleAnnotations属性。对于支持Class文件格式版本号为 51.0 或更高的 Java 虚拟机实现，必须正确识别并读取 attributes表中的BootstrapMethods属性。Java 7 规范 要求任一 Java 虚拟机实现可以自动忽略 Class 文件的 attributes表中的若干 （甚至全部） 它不可识别的属性项。任何本规范未定义的属性不能影响Class文件的语义，只能提供附加的描述信息 。 根据上述的叙述，我们可以将class的文件组织结构概括成以下面这个结构体：]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>字节码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机原理：(二) class常量池]]></title>
    <url>%2F2014%2F10%2F09%2F2014-10-16-jvm-theory-2%2F</url>
    <content type="text"><![CDATA[原文链接: https://blog.csdn.net/luanlouis/article/details/40148053 了解JVM虚拟机原理 是每一个Java程序员修炼的必经之路。但是由于JVM虚拟机中有很多的东西讲述的比较宽泛，在当前接触到的关于JVM虚拟机原理的教程或者博客中，绝大部分都是充斥的文字性的描述，很难给人以形象化的认知，看完之后感觉还是稀里糊涂的。感于以上的种种，我打算把我在学习JVM虚拟机的过程中学到的东西，结合自己的理解，总结成《Java虚拟机原理图解》 这个系列，以图解的形式，将抽象的JVM虚拟机的知识具体化，希望能够对想了解Java虚拟机原理的的Java程序员 提供点帮助。 上一章节结构大致地介绍了class文件的组织结构，接下来，我们将深入每一个结构，来详细了解它们。这一章节呢，我们就来扒一扒 class文件中非常重要 的个数据区域常量池。它在JVM虚拟机中扮演了非常重要的地位。 NO1.常量池在class文件的什么位置？在class文件中的魔数、副版本号、主版本之后，紧接着就是常量池的数据区域了，如下图用红线包括的位置： NO2.常量池的里面是怎么组织的？常量池的组织很简单，前端的两个字节占有的位置叫做常量池计数器(constant_pool_count)，它记录着常量池的组成元素常量池项(cp_info)的个数。紧接着会排列着constant_pool_count-1个常量池项(cp_info)。如下图所示： NO3.常量池项 (cp_info) 的结构是什么？每个常量池项(cp_info) 都会对应记录着class文件中的某中类型的字面量。让我们先来了解一下常量池项(cp_info)的结构吧： JVM虚拟机规定了不同的tag值和不同类型的字面量对应关系如下： 所以根据cp_info中的tag 不同的值，可以将cp_info 更细化为以下结构体： 现在让我们看一下细化了的常量池的结构会是类似下图所示的样子： NO4.常量池能够表示那些信息？ NO5. int和float数据类型的常量在常量池中是怎样表示和存储的？(CONSTANT_Integer_info, CONSTANT_Float_info)Java语言规范规定了 int类型和Float 类型的数据类型占用 4 个字节的空间。那么存在于class字节码文件中的该类型的常量是如何存储的呢？相应地，在常量池中，将 int和Float类型的常量分别使用CONSTANT_Integer_info和 Constant_float_info表示，他们的结构如下所示： 举例：建下面的类 IntAndFloatTest.java，在这个类中，我们声明了五个变量，但是取值就两种int类型的10 和Float类型的11f。123456789public class IntAndFloatTest &#123; private final int a = 10; private final int b = 10; private float c = 11f; private float d = 11f; private float e = 11f; &#125; 然后用编译器编译成IntAndFloatTest.class字节码文件，我们通过javap -v IntAndFloatTest ``指令来看一下其常量池中的信息，可以看到虽然我们在代码中写了两次10和三次11f，但是常量池中，就只有一个常量10 和一个常量11f,如下图所示: 从结果上可以看到常量池第#8个常量池项(cp_info) 就是CONSTANT_Integer_info,值为10；第#23个常量池项(cp_info) 就是CONSTANT_Float_info,值为11f。(常量池中其他的东西先别纠结啦，我们会面会一一讲解的哦)。 代码中所有用到 int 类型 10 的地方，会使用指向常量池的指针值#8 定位到第#8 个常量池项(cp_info)，即值为 10的结构体CONSTANT_Integer_info，而用到float类型的11f时，也会指向常量池的指针值#23来定位到第#23个常量池项(cp_info) 即值为11f的结构体CONSTANT_Float_info。如下图所示： NO6. long和 double数据类型的常量在常量池中是怎样表示和存储的？(CONSTANT_Long_info、CONSTANT_Double_info )Java语言规范规定了 long 类型和 double类型的数据类型占用8 个字节的空间。那么存在于class 字节码文件中的该类型的常量是如何存储的呢？相应地，在常量池中，将long和double类型的常量分别使用CONSTANT_Long_info和Constant_Double_info表示，他们的结构如下所示： 举例：建下面的类 LongAndDoubleTest.java，在这个类中，我们声明了六个变量，但是取值就两种Long 类型的-6076574518398440533L 和Double 类型的10.1234567890D。123456789public class LongAndDoubleTest &#123; private long a = -6076574518398440533L; private long b = -6076574518398440533L; private long c = -6076574518398440533L; private double d = 10.1234567890D; private double e = 10.1234567890D; private double f = 10.1234567890D; &#125; 然后用编译器编译成 LongAndDoubleTest.class 字节码文件，我们通过javap -v LongAndDoubleTest指令来看一下其常量池中的信息，可以看到虽然我们在代码中写了三次-6076574518398440533L 和三次10.1234567890D，但是常量池中，就只有一个常量-6076574518398440533L 和一个常量10.1234567890D,如下图所示: 从结果上可以看到常量池第#18个常量池项(cp_info) 就是CONSTANT_Long_info,值为-6076574518398440533L ；第#26个常量池项(cp_info) 就是CONSTANT_Double_info,值为10.1234567890D。(常量池中其他的东西先别纠结啦，我们会面会一一讲解的哦)。 代码中所有用到 long 类型-6076574518398440533L 的地方，会使用指向常量池的指针值#18 定位到第 #18 个常量池项(cp_info)，即值为-6076574518398440533L 的结构体CONSTANT_Long_info，而用到double类型的10.1234567890D时，也会指向常量池的指针值#26 来定位到第 #26 个常量池项(cp_info) 即值为10.1234567890D的结构体CONSTANT_Double_info。如下图所示： NO7. String类型的字符串常量在常量池中是怎样表示和存储的？（CONSTANT_String_info、CONSTANT_Utf8_info）对于字符串而言，JVM会将字符串类型的字面量以UTF-8 编码格式存储到在class字节码文件中。这么说可能有点摸不着北，我们先从直观的Java源码中中出现的用双引号”” 括起来的字符串来看，在编译器编译的时候，都会将这些字符串转换成CONSTANT_String_info结构体，然后放置于常量池中。其结构如下所示： 如上图所示的结构体，CONSTANT_String_info结构体中的string_index的值指向了CONSTANT_Utf8_info结构体，而字符串的utf-8编码数据就在这个结构体之中。如下图所示： 请看一例，定义一个简单的StringTest.java类，然后在这个类里加一个”JVM原理” 字符串，然后，我们来看看它在class文件中是怎样组织的。123456public class StringTest &#123; private String s1 = "JVM原理"; private String s2 = "JVM原理"; private String s3 = "JVM原理"; private String s4 = "JVM原理"; &#125; 将Java源码编译成StringTest.class文件后，在此文件的目录下执行`` javap -v StringTest 命令，会看到如下的常量池信息的轮廓： 在上图中，我们可以看到CONSTANT_String_info结构体位于常量池的第#15个索引位置。而存放”Java虚拟机原理” 字符串的 UTF-8编码格式的字节数组被放到CONSTANT_Utf8_info结构体中，该结构体位于常量池的第#16个索引位置。上面的图只是看了个轮廓，让我们再深入地看一下它们的组织吧。请看下图： 由上图可见：“JVM原理”的UTF-8编码的数组是：4A564D E5 8E 9FE7 90 86，并且存入了CONSTANT_Utf8_info结构体中。 NO8. 类文件中定义的类名和类中使用到的类在常量池中是怎样被组织和存储的？(CONSTANT_Class_info)JVM会将某个Java 类中所有使用到了的类的完全限定名 以二进制形式的完全限定名 封装成CONSTANT_Class_info结构体中，然后将其放置到常量池里。CONSTANT_Class_info 的tag值为 7 。其结构如下： Tips：类的完全限定名和二进制形式的完全限定名 在某个Java源码中，我们会使用很多个类，比如我们定义了一个 ClassTest的类，并把它放到com.louis.jvm 包下，则 ClassTest类的完全限定名为com.louis.jvm.ClassTest，将JVM编译器将类编译成class文件后，此完全限定名在class文件中，是以二进制形式的完全限定名存储的，即它会把完全限定符的“.”换成”/“ ，即在class文件中存储的 ClassTest类的完全限定名称是”com/louis/jvm/ClassTest”。因为这种形式的完全限定名是放在了class二进制形式的字节码文件中，所以就称之为二进制形式的完全限定名。 举例，我们定义一个很简单的ClassTest类，来看一下常量池是怎么对类的完全限定名进行存储的。12345package com.jvm; import java.util.Date; public class ClassTest &#123; private Date date =new Date(); &#125; 将Java源码编译成ClassTest.class文件后，在此文件的目录下执行javap -v ClassTest命令，会看到如下的常量池信息的轮廓： 如上图所示，在ClassTest.class文件的常量池中，共有 3 个CONSTANT_Class_info结构体，分别表示ClassTest 中用到的Class信息。 我们就看其中一个表示com/jvm.ClassTest的CONSTANT_Class_info 结构体。它在常量池中的位置是#1，它的name_index值为#2，它指向了常量池的第2 个常量池项，如下所示: 对于某个类而言，其class文件中至少要有两个CONSTANT_Class_info常量池项，用来表示自己的类信息和其父类信息。(除了java.lang.Object类除外，其他的任何类都会默认继承自java.lang.Object）如果类声明实现了某些接口，那么接口的信息也会生成对应的CONSTANT_Class_info常量池项。 除此之外，如果在类中使用到了其他的类，只有真正使用到了相应的类，JDK编译器才会将类的信息组成CONSTANT_Class_info常量池项放置到常量池中。如下图： 12345678910import java.util.Date; public class Other&#123; private Date date; public Other() &#123; Date da; &#125; &#125; 上述的Other的类，在JDK将其编译成class文件时，常量池中并没有java.util.Date对应的CONSTANT_Class_info常量池项，为什么呢? 在Other类中虽然定义了Date类型的两个变量date、da，但是JDK编译的时候，认为你只是声明了“Ljava/util/Date”类型的变量，并没有实际使用到Ljava/util/Date类。将类信息放置到常量池中的目的，是为了在后续的代码中有可能会反复用到它。很显然，JDK在编译Other类的时候，会解析到Date类有没有用到，发现该类在代码中就没有用到过，所以就认为没有必要将它的信息放置到常量池中了。 将上述的Other类改写一下，仅使用new Date()，如下图所示：12345678import java.util.Date; public class Other&#123; public Other() &#123; new Date(); &#125; &#125; 这时候使用javap -v Other ，可以查看到常量池中有表示java/util/Date的常量池项： 总结： 对于某个类或接口而言，其自身、父类和继承或实现的接口的信息会被直接组装成CONSTANT_Class_info常量池项放置到常量池中； 类中或接口中使用到了其他的类，只有在类中实际使用到了该类时，该类的信息才会在常量池中有对应的CONSTANT_Class_info常量池项； 类中或接口中仅仅定义某种类型的变量，JDK只会将变量的类型描述信息以UTF-8字符串组成CONSTANT_Utf8_info常量池项放置到常量池中，上面在类中的private Date date;JDK编译器只会将表示date的数据类型的“Ljava/util/Date”字符串放置到常量池中。 NO9.类中引用到的field字段在常量池中是怎样描述的？(CONSTANT_Fieldref_info, CONSTANT_Name_Type_info)一般而言，我们在定义类的过程中会定义一些 field 字段，然后会在这个类的其他地方（如方法中）使用到它。有可能我们在类的方法中只使用field字段一次，也有可能我们会在类定义的方法中使用它很多很多次。 举一个简单的例子，我们定一个叫Person的简单java bean，它有name和age两个field字段，如下所示：1234567891011121314151617181920public class Person &#123; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; &#125; 在上面定义的类中，我们在Person类中的一系列方法里，多次引用到namefield字段 和agefield字段，对于JVM编译器而言，name和age只是一个符号而已，并且它在由于它可能会在此类中重复出现多次，所以JVM把它当作常量来看待，将name和age以field字段常量的形式保存到常量池中。 将它name和age封装成CONSTANT_Fieldref_info常量池项，放到常量池中，在类中引用到它的地方，直接放置一个指向field字段所在常量池的索引。 上面的Person类，使用javap -v Person指令，查看class文件的信息，你会看到，在Person类中引用到age和namefield字段的地方，都是指向了常量池中age和namefield字段对应的常量池项中。表示field字段的常量池项叫做CONSTANT_Fieldref_info。 怎样描述某一个field字段的引用？ 实例解析： 现在，让我们来看一下Person类中定义的namefield字段在常量池中的表示。通过使用javap -v Person会查看到如下的常量池信息： 请读者看上图中namefield字段的数据类型，它在#6个常量池项，以UTF-8编码格式的字符串“Ljava/lang/String;” 表示，这表示着这个field 字段是java.lang.String 类型的。关于field字段的数据类型，class文件中存储的方式和我们在源码中声明的有些不一样。请看下图的对应关系： 请注意！！！ 如果我们在类中定义了field 字段，但是没有在类中的其他地方用到这些字段或者初始赋值，它是不会被编译器放到常量池中的。读者可以自己试一下。（当然了，定义了但是没有在类中的其它地方引用到这种情况很少。）只有在类中的其他地方引用到了，才会将他放到常量池中。 NO10.类中引用到的method方法在常量池中是怎样描述的？(CONSTANT_Methodref_info, CONSTANT_Name_Type_info)还是以Person类为例。在Person类中，我们定义了setName(String name)、getName()、setAge(int age)、getAge()这些方法：1234567891011121314151617181920212223package com.louis.jvm; public class Person &#123; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; &#125; 虽然我们定义了方法，但是这些方法没有在类中的其他地方被用到（即没有在类中其他的方法中引用到），所以它们的方法引用信息并不会放到常量池中。现我们在类中加一个方法 getInfo()，调用了getName()和getAge() 方法：12345public String getInfo() &#123; return getName()+"\t"+getAge(); &#125; 这时候JVM编译器会将getName()和getAge()方法的引用信息包装成CONSTANT_Methodref_info结构体放入到常量池之中。 这里的方法调用的方式牵涉到Java非常重要的一个术语和机制，叫动态绑定。这个动态绑定问题以后在单独谈谈。 怎样表示一个方法引用？ 请看下图： getName() 方法引用在常量池中的表示 NO11.类中引用到某个接口中定义的method方法在常量池中是怎样描述的？(CONSTANT_InterfaceMethodref_info, CONSTANT_Name_Type_info)当我们在某个类中使用到了某个接口中的方法，JVM会将用到的接口中的方法信息方知道这个类的常量池中。比如我们定义了一个Worker接口，和一个Boss类，在Boss类中调用了Worker接口中的方法，这时候在Boss类的常量池中会有Worker接口的方法的引用表示。1234567891011121314public interface Worker&#123; public void work(); &#125; public class Boss &#123; public void makeMoney(Worker worker) &#123; worker.work(); &#125; &#125; 我们对Boss.class执行javap -v Boss,然后会看到如下信息： 如上图所示，在Boss类的makeMoney()方法中调用了Worker接口的work()方法，机器指令是通过invokeinterface指令完成的，invokeinterface指令后面的操作数，是指向了Boss常量池中Worker接口的work()方法描述，表示的意思就是：“我要调用Worker接口的work()方法”。 Worker接口的work()方法引用信息，JVM会使用CONSTANT_InterfaceMethodref_info结构体来描述，CONSTANT_InterfaceMethodref_info定义如下： . CONSTANT_InterfaceMethodref_info结构体和上面介绍的CONSTANT_Methodref_info 结构体很基本上相同，它们的不同点只有： CONSTANT_InterfaceMethodref_info 的tag 值为11，而CONSTANT_Methodref_info的tag值为10； CONSTANT_InterfaceMethodref_info 描述的是接口中定义的方法，而CONSTANT_Methodref_info描述的是实例类中的方法； NO12.CONSTANT_MethodType_info，CONSTANT_MethodHandle_info，CONSTANT_InvokeDynamic_info如果你从我的《常量池详解》NO1节看到了NO11节，那么恭喜你，你已经学会了几乎所有的常量池项！只要你掌握了上述的常量池项，你就可以读懂你平常所见到的任何一个class文件的常量池了。 至于NO12所列出来的三项：CONSTANT_MethodType_info,CONSTANT_MethodHandle_info,CONSTANT_InvokeDynamic_info，我想对你说，暂时先不管它吧。 这三项主要是为了让Java语言支持动态语言特性而在Java 7 版本中新增的三个常量池项，只会在极其特别的情况能用到它，在class文件中几乎不会生成这三个常量池项。 其实我花了一些时间来研究这三项，并且想通过各种方式生成这三项，不过没有成功，最后搞的还是迷迷糊糊的。从我了解到的信息来看，Java 7对动态语言的支持很笨拙，并且当前没有什么应用价值，然后就对着三项的研究先放一放了。）]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>字节码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探秘Java中的String、StringBuilder以及StringBuffer]]></title>
    <url>%2F2014%2F06%2F09%2F2014-06-09-string-learning%2F</url>
    <content type="text"><![CDATA[原文链接：http://www.cnblogs.com/dolphin0520/p/3778589.html 相信String这个类是Java中使用得最频繁的类之一，并且又是各大公司面试喜欢问到的地方，今天就来和大家一起学习一下String、StringBuilder和StringBuffer这几个类，分析它们的异同点以及了解各个类适用的场景。下面是本文的目录大纲： 一.你了解String类吗？ 二.深入理解String、StringBuffer、StringBuilder 三.不同场景下三个类的性能测试 四.常见的关于String、StringBuffer的面试题（辟谣网上流传的一些曲解String类的说法） 若有不正之处，请多多谅解和指正，不胜感激。 一. 你了解String类吗？想要了解一个类，最好的办法就是看这个类的实现源代码12345678910111213public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 /** use serialVersionUID from JDK 1.0.2 for interoperability */ private static final long serialVersionUID = -6849794470754667710L; ...&#125; 从上面可以看出几点： 1）String类是final类，也即意味着String类不能被继承，并且它的成员方法都默认为final方法。在Java中，被final修饰的类是不允许被继承的，并且该类中的成员方法都默认为final方法。在早期的JVM实现版本中，被final修饰的方法会被转为内嵌调用以提升执行效率。而从Java SE5/6开始，就渐渐摈弃这种方式了。因此在现在的Java SE版本中，不需要考虑用final去提升方法调用效率。只有在确定不想让该方法被覆盖时，才将方法设置为final。 2）上面列举出了String类中所有的成员属性，从上面可以看出String类其实是通过char数组来保存字符串的。 下面再继续看String类的一些方法实现：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public String substring(int beginIndex, int endIndex) &#123; if (beginIndex &lt; 0) &#123; throw new StringIndexOutOfBoundsException(beginIndex); &#125; if (endIndex &gt; value.length) &#123; throw new StringIndexOutOfBoundsException(endIndex); &#125; int subLen = endIndex - beginIndex; if (subLen &lt; 0) &#123; throw new StringIndexOutOfBoundsException(subLen); &#125; return ((beginIndex == 0) &amp;&amp; (endIndex == value.length)) ? this : new String(value, beginIndex, subLen); &#125; public String concat(String str) &#123; int otherLen = str.length(); if (otherLen == 0) &#123; return this; &#125; int len = value.length; char buf[] = Arrays.copyOf(value, len + otherLen); str.getChars(buf, len); return new String(buf, true); &#125; public String replace(char oldChar, char newChar) &#123; if (oldChar != newChar) &#123; int len = value.length; int i = -1; char[] val = value; /* avoid getfield opcode */ while (++i &lt; len) &#123; if (val[i] == oldChar) &#123; break; &#125; &#125; if (i &lt; len) &#123; char buf[] = new char[len]; for (int j = 0; j &lt; i; j++) &#123; buf[j] = val[j]; &#125; while (i &lt; len) &#123; char c = val[i]; buf[i] = (c == oldChar) ? newChar : c; i++; &#125; return new String(buf, true); &#125; &#125; return this; &#125; 从上面的三个方法可以看出，无论是sub操、concat还是replace操作都不是在原有的字符串上进行的，而是重新生成了一个新的字符串对象。也就是说进行这些操作后，最原始的字符串并没有被改变。 在这里要永远记住一点： “对String对象的任何改变都不影响到原对象，相关的任何change操作都会生成新的对象”。 在了解了于String类基础的知识后，下面来看一些在平常使用中容易忽略和混淆的地方。 二.深入理解String、StringBuffer、StringBuilder1. String str=”hello world”和String str=new String(“hello world”)的区别想必大家对上面2个语句都不陌生，在平时写代码的过程中也经常遇到，那么它们到底有什么区别和联系呢？下面先看几个例子：123456789101112131415161718public class Main &#123; public static void main(String[] args) &#123; String str1 = "hello world"; String str2 = new String("hello world"); String str3 = "hello world"; String str4 = new String("hello world"); System.out.println(str1==str2); System.out.println(str1==str3); System.out.println(str2==str4); &#125;&#125;Output:falsetruefalse 为什么会出现这样的结果？下面解释一下原因： 在前面一篇讲解关于JVM内存机制的一篇博文中提到 ，在class文件中有一部分 来存储编译期间生成的 字面常量以及符号引用，这部分叫做class文件常量池，在运行期间对应着方法区的运行时常量池。 因此在上述代码中，String str1 = “hello world”;和String str3 = “hello world”; 都在编译期间生成了 字面常量和符号引用，运行期间字面常量”hello world”被存储在运行时常量池（当然只保存了一份）。通过这种方式来将String对象跟引用绑定的话，JVM执行引擎会先在运行时常量池查找是否存在相同的字面常量，如果存在，则直接将引用指向已经存在的字面常量；否则在运行时常量池开辟一个空间来存储该字面常量，并将引用指向该字面常量。 众所周知，通过new关键字来生成对象是在堆区进行的，而在堆区进行对象生成的过程是不会去检测该对象是否已经存在的。因此通过new来创建对象，创建出的一定是不同的对象，即使字符串的内容是相同的。 2. String、StringBuffer以及StringBuilder的区别既然在Java中已经存在了String类，那为什么还需要StringBuilder和StringBuffer类呢？ 那么看下面这段代码：123456public static void main(String[] args) &#123; String string = ""; for(int i=0;i&lt;10000;i++)&#123; string += "hello"; &#125; &#125; 这句 string += “hello”;的过程相当于将原有的string变量指向的对象内容取出与”hello”作字符串相加操作再存进另一个新的String对象当中，再让string变量指向新生成的对象。如果大家还有疑问可以反编译其字节码文件便清楚了： 这段反编译出的字节码文件可以很清楚地看出：从第8行开始到第35行是整个循环的执行过程，并且每次循环会new出一个StringBuilder对象，然后进行append操作，最后通过toString方法返回String对象。也就是说这个循环执行完毕new出了10000个对象，试想一下，如果这些对象没有被回收，会造成多大的内存资源浪费。从上面还可以看出：string+=”hello”的操作事实上会自动被JVM优化成：12345StringBuilder str = new StringBuilder(string);str.append(&quot;hello&quot;);string = str.toString(); 再看下面这段代码：123456789public class Main &#123; public static void main(String[] args) &#123; StringBuilder stringBuilder = new StringBuilder(); for(int i=0;i&lt;10000;i++)&#123; stringBuilder.append("hello"); &#125; &#125;&#125; 反编译字节码文件得到：从这里可以明显看出，这段代码的for循环式从13行开始到27行结束，并且new操作只进行了一次，也就是说只生成了一个对象，append操作是在原有对象的基础上进行的。因此在循环了10000次之后，这段代码所占的资源要比上面小得多。 那么有人会问既然有了StringBuilder类，为什么还需要StringBuffer类？查看源代码便一目了然，事实上，StringBuilder和StringBuffer类拥有的成员属性以及成员方法基本相同，区别是StringBuffer类的成员方法前面多了一个关键字：synchronized，不用多说，这个关键字是在多线程访问时起到安全保护作用的,也就是说StringBuffer是线程安全的。 三.不同场景下三个类的性能测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class Main &#123; private static int time = 50000; public static void main(String[] args) &#123; testString(); testStringBuffer(); testStringBuilder(); test1String(); test2String(); &#125; public static void testString () &#123; String s=""; long begin = System.currentTimeMillis(); for(int i=0; i&lt;time; i++)&#123; s += "java"; &#125; long over = System.currentTimeMillis(); System.out.println("操作"+s.getClass().getName()+"类型使用的时间为："+(over-begin)+"毫秒"); &#125; public static void testStringBuffer () &#123; StringBuffer sb = new StringBuffer(); long begin = System.currentTimeMillis(); for(int i=0; i&lt;time; i++)&#123; sb.append("java"); &#125; long over = System.currentTimeMillis(); System.out.println("操作"+sb.getClass().getName()+"类型使用的时间为："+(over-begin)+"毫秒"); &#125; public static void testStringBuilder () &#123; StringBuilder sb = new StringBuilder(); long begin = System.currentTimeMillis(); for(int i=0; i&lt;time; i++)&#123; sb.append("java"); &#125; long over = System.currentTimeMillis(); System.out.println("操作"+sb.getClass().getName()+"类型使用的时间为："+(over-begin)+"毫秒"); &#125; public static void test1String () &#123; long begin = System.currentTimeMillis(); for(int i=0; i&lt;time; i++)&#123; String s = "I"+"love"+"java"; &#125; long over = System.currentTimeMillis(); System.out.println("字符串直接相加操作："+(over-begin)+"毫秒"); &#125; public static void test2String () &#123; String s1 ="I"; String s2 = "love"; String s3 = "java"; long begin = System.currentTimeMillis(); for(int i=0; i&lt;time; i++)&#123; String s = s1+s2+s3; &#125; long over = System.currentTimeMillis(); System.out.println("字符串间接相加操作："+(over-begin)+"毫秒"); &#125;&#125; 测试结果（win7，Eclipse，JDK6)： 上面提到string+=”hello”的操作事实上会自动被JVM优化，看下面这段代码：12345678910111213141516171819202122232425262728293031public class Main &#123; private static int time = 50000; public static void main(String[] args) &#123; testString(); testOptimalString(); &#125; public static void testString () &#123; String s=""; long begin = System.currentTimeMillis(); for(int i=0; i&lt;time; i++)&#123; s += "java"; &#125; long over = System.currentTimeMillis(); System.out.println("操作"+s.getClass().getName()+"类型使用的时间为："+(over-begin)+"毫秒"); &#125; public static void testOptimalString () &#123; String s=""; long begin = System.currentTimeMillis(); for(int i=0; i&lt;time; i++)&#123; StringBuilder sb = new StringBuilder(s); sb.append("java"); s=sb.toString(); &#125; long over = System.currentTimeMillis(); System.out.println("模拟JVM优化操作的时间为："+(over-begin)+"毫秒"); &#125;&#125; 执行结果： 下面对上面的执行结果进行一般性的解释： 对于直接相加字符串，效率很高，因为在编译器便确定了它的值，也就是说形如”I”+”love”+”java”; 的字符串相加，在编译期间便被优化成了”Ilovejava”。这个可以用javap -c命令反编译生成的class文件进行验证。对于间接相加（即包含字符串引用），形如s1+s2+s3; 效率要比直接相加低，因为在编译器不会对引用变量进行优化。 String、StringBuilder、StringBuffer三者的执行效率： StringBuilder &gt; StringBuffer &gt; String 当然这个是相对的，不一定在所有情况下都是这样。比如String str = “hello”+ “world”的效率就比 StringBuilder st = new StringBuilder().append(“hello”).append(“world”)要高。 因此，这三个类是各有利弊，应当根据不同的情况来进行选择使用： 当字符串相加操作或者改动较少的情况下，建议使用 String str=”hello”这种形式； 当字符串相加操作较多的情况下，建议使用StringBuilder，如果采用了多线程，则使用StringBuffer。 四.常见的关于String、StringBuffer的面试题 下面这段代码的输出结果是什么？123String a = "hello2"; String b = "hello" + 2;System.out.println((a == b)); 输出结果为：true。原因很简单，”hello”+2在编译期间就已经被优化成”hello2”，因此在运行期间，变量a和变 量b指向的是同一个对象。 下面这段代码的输出结果是什么？1234String a = "hello2"; String b = "hello"; String c = b + 2; System.out.println((a == c)); 输出结果为:false。由于有符号引用的存在，所以 String c = b + 2;不会在编译期间被优化，不会把b+2当做字面常量来处理的，因此这种方式生成的对象事实上是保存在堆上的。因此a和c指向的并不是同一个对象。javap -c得到的内容： 下面这段代码的输出结果是什么？1234String a = "hello2";final String b = "hello";String c = b + 2;System.out.println((a == c)); 输出结果为：true。对于被final修饰的变量，会在class文件常量池中保存一个副本，也就是说不会通过连接而进行访问，对final变量的访问在编译期间都会直接被替代为真实的值。那么String c = b + 2;在编译期间就会被优化成：String c = “hello” + 2; 下图是javap -c的内容： 下面这段代码输出结果为：123456789101112public class Main &#123; public static void main(String[] args) &#123; String a = "hello2"; final String b = getHello(); String c = b + 2; System.out.println((a == c)); &#125; public static String getHello() &#123; return "hello"; &#125;&#125; 输出结果为false。这里面虽然将b用final修饰了，但是由于其赋值是通过方法调用返回的，那么它的值只能在运行期间确定，因此a和c指向的不是同一个对象。 下面这段代码的输出结果是什么？12345678910111213public class Main &#123; public static void main(String[] args) &#123; String a = "hello"; String b = new String("hello"); String c = new String("hello"); String d = b.intern(); System.out.println(a==b); System.out.println(b==c); System.out.println(b==d); System.out.println(a==d); &#125;&#125; 输出结果为（JDK版本 JDK6)：输出结果为false,false,false,true。这里面涉及到的是String.intern方法的使用。在String类中，intern方法是一个本地方法，在JAVA SE6之前，intern方法会在运行时常量池中查找是否存在内容相同的字符串，如果存在则返回指向该字符串的引用，如果不存在，则会将该字符串入池，并返回一个指向该字符串的引用。因此，a和d指向的是同一个对象。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[双重检查锁定与延迟初始化]]></title>
    <url>%2F2013%2F10%2F07%2F2013-10-07-double-checked-locking-with-delay-initialization%2F</url>
    <content type="text"><![CDATA[原文链接: http://www.infoq.com/cn/articles/double-checked-locking-with-delay-initialization 在java程序中，有时候可能需要推迟一些高开销的对象初始化操作，并且只有在使用这些对象时才进行初始化。此时程序员可能会采用延迟初始化。但要正确实现线程安全的延迟初始化需要一些技巧，否则很容易出现问题。比如，下面是非线程安全的延迟初始化对象的示例代码：12345678public class UnsafeLazyInitialization &#123; private static Instance instance; public static Instance getInstance() &#123; if (instance == null) //1：A线程执行 instance = new Instance(); //2：B线程执行 return instance; &#125;&#125; 在UnsafeLazyInitialization中，假设A线程执行代码1的同时，B线程执行代码2。此时，线程A可能会看到instance引用的对象还没有完成初始化（出现这种情况的原因见后文的“问题的根源”）。 对于UnsafeLazyInitialization，我们可以对getInstance()做同步处理来实现线程安全的延迟初始化。示例代码如下：123456789public class SafeLazyInitialization &#123; private static Instance instance; public synchronized static Instance getInstance() &#123; if (instance == null) instance = new Instance(); return instance; &#125;&#125; 由于对getInstance()做了同步处理，synchronized将导致性能开销。如果getInstance()被多个线程频繁的调用，将会导致程序执行性能的下降。反之，如果getInstance()不会被多个线程频繁的调用，那么这个延迟初始化方案将能提供令人满意的性能。 在早期的JVM中，synchronized（甚至是无竞争的synchronized）存在这巨大的性能开销。因此，人们想出了一个“聪明”的技巧：双重检查锁定（double-checked locking）。人们想通过双重检查锁定来降低同步的开销。下面是使用双重检查锁定来实现延迟初始化的示例代码：12345678910111213public class DoubleCheckedLocking &#123; //1 private static Instance instance; //2 public static Instance getInstance() &#123; //3 if (instance == null) &#123; //4:第一次检查 synchronized (DoubleCheckedLocking.class) &#123; //5:加锁 if (instance == null) //6:第二次检查 instance = new Instance(); //7:问题的根源出在这里 &#125; //8 &#125; //9 return instance; //10 &#125; //11&#125; //12 如上面代码所示，如果第一次检查instance不为null，那么就不需要执行下面的加锁和初始化操作。因此可以大幅降低synchronized带来的性能开销。上面代码表面上看起来，似乎两全其美： 在多个线程试图在同一时间创建对象时，会通过加锁来保证只有一个线程能创建对象。 在对象创建好之后，执行getInstance()将不需要获取锁，直接返回已创建好的对象。 双重检查锁定看起来似乎很完美，但这是一个错误的优化！在线程执行到第4行代码读取到instance不为null时，instance引用的对象有可能还没有完成初始化。 问题的根源前面的双重检查锁定示例代码的第7行（instance = new Singleton();）创建一个对象。这一行代码可以分解为如下的三行伪代码：123memory = allocate(); //1：分配对象的内存空间ctorInstance(memory); //2：初始化对象instance = memory; //3：设置instance指向刚分配的内存地址 上面三行伪代码中的2和3之间，可能会被重排序（在一些JIT编译器上，这种重排序是真实发生的，详情见参考文献1的“Out-of-order writes”部分）。2和3之间重排序之后的执行时序如下：1234memory = allocate(); //1：分配对象的内存空间instance = memory; //3：设置instance指向刚分配的内存地址 //注意，此时对象还没有被初始化！ctorInstance(memory); //2：初始化对象 根据《The Java Language Specification, Java SE 7 Edition》（后文简称为java语言规范），所有线程在执行java程序时必须要遵守intra-thread semantics。intra-thread semantics保证重排序不会改变单线程内的程序执行结果。换句话来说，intra-thread semantics允许那些在单线程内，不会改变单线程程序执行结果的重排序。上面三行伪代码的2和3之间虽然被重排序了，但这个重排序并不会违反intra-thread semantics。这个重排序在没有改变单线程程序的执行结果的前提下，可以提高程序的执行性能。 为了更好的理解intra-thread semantics，请看下面的示意图（假设一个线程A在构造对象后，立即访问这个对象）：如上图所示，只要保证2排在4的前面，即使2和3之间重排序了，也不会违反intra-thread semantics。 下面，再让我们看看多线程并发执行的时候的情况。请看下面的示意图： 由于单线程内要遵守intra-thread semantics，从而能保证A线程的程序执行结果不会被改变。但是当线程A和B按上图的时序执行时，B线程将看到一个还没有被初始化的对象。 回到本文的主题，DoubleCheckedLocking示例代码的第7行（instance = new Singleton();）如果发生重排序，另一个并发执行的线程B就有可能在第4行判断instance不为null。线程B接下来将访问instance所引用的对象，但此时这个对象可能还没有被A线程初始化！下面是这个场景的具体执行时序： 时间 线程A 线程B t1 A1：分配对象的内存空间 t2 A3：设置instance指向内存空间 t3 B1：判断instance是否为空 t4 B2：由于instance不为null，线程B将访问instance引用的对象 t5 A2：初始化对象 t6 A4：访问instance引用的对象 这里A2和A3虽然重排序了，但java内存模型的intra-thread semantics将确保A2一定会排在A4前面执行。因此线程A的intra-thread semantics没有改变。但A2和A3的重排序，将导致线程B在B1处判断出instance不为空，线程B接下来将访问instance引用的对象。此时，线程B将会访问到一个还未初始化的对象。 在知晓了问题发生的根源之后，我们可以想出两个办法来实现线程安全的延迟初始化： 不允许2和3重排序； 允许2和3重排序，但不允许其他线程“看到”这个重排序。 后文介绍的两个解决方案，分别对应于上面这两点。 基于volatile的双重检查锁定的解决方案对于前面的基于双重检查锁定来实现延迟初始化的方案（指DoubleCheckedLocking示例代码），我们只需要做一点小的修改（把instance声明为volatile型），就可以实现线程安全的延迟初始化。请看下面的示例代码：12345678910111213public class SafeDoubleCheckedLocking &#123; private volatile static Instance instance; public static Instance getInstance() &#123; if (instance == null) &#123; synchronized (SafeDoubleCheckedLocking.class) &#123; if (instance == null) instance = new Instance();//instance为volatile，现在没问题了 &#125; &#125; return instance; &#125;&#125; 注意，这个解决方案需要JDK5或更高版本（因为从JDK5开始使用新的JSR-133内存模型规范，这个规范增强了volatile的语义）。 当声明对象的引用为volatile后，“问题的根源”的三行伪代码中的2和3之间的重排序，在多线程环境中将会被禁止。上面示例代码将按如下的时序执行： 这个方案本质上是通过禁止上图中的2和3之间的重排序，来保证线程安全的延迟初始化。 基于类初始化的解决方案JVM在类的初始化阶段（即在Class被加载后，且被线程使用之前），会执行类的初始化。在执行类的初始化期间，JVM会去获取一个锁。这个锁可以同步多个线程对同一个类的初始化。 基于这个特性，可以实现另一种线程安全的延迟初始化方案（这个方案被称之为Initialization On Demand Holder idiom）：123456789public class InstanceFactory &#123; private static class InstanceHolder &#123; public static Instance instance = new Instance(); &#125; public static Instance getInstance() &#123; return InstanceHolder.instance ; //这里将导致InstanceHolder类被初始化 &#125;&#125; 假设两个线程并发执行getInstance()，下面是执行的示意图： 这个方案的实质是：允许“问题的根源”的三行伪代码中的2和3重排序，但不允许非构造线程（这里指线程B）“看到”这个重排序。 Mark: 重点！！！初始化一个类，包括执行这个类的静态初始化和初始化在这个类中声明的静态字段。根据java语言规范，在首次发生下列任意一种情况时，一个类或接口类型T将被立即初始化： T是一个类，而且一个T类型的实例被创建； T是一个类，且T中声明的一个静态方法被调用； T中声明的一个静态字段被赋值； T中声明的一个静态字段被使用，而且这个字段不是一个常量字段； T是一个顶级类（top level class，见java语言规范的§7.6），而且一个断言语句嵌套在T内部被执行。 在InstanceFactory示例代码中，首次执行getInstance()的线程将导致InstanceHolder类被初始化（符合情况4）。 由于java语言是多线程的，多个线程可能在同一时间尝试去初始化同一个类或接口（比如这里多个线程可能在同一时刻调用getInstance()来初始化InstanceHolder类）。因此在java中初始化一个类或者接口时，需要做细致的同步处理。 Java语言规范规定，对于每一个类或接口C，都有一个唯一的初始化锁LC与之对应。从C到LC的映射，由JVM的具体实现去自由实现。JVM在类初始化期间会获取这个初始化锁，并且每个线程至少获取一次锁来确保这个类已经被初始化过了（事实上，java语言规范允许JVM的具体实现在这里做一些优化，见后文的说明）。 对于类或接口的初始化，java语言规范制定了精巧而复杂的类初始化处理过程。java初始化一个类或接口的处理过程如下（这里对类初始化处理过程的说明，省略了与本文无关的部分；同时为了更好的说明类初始化过程中的同步处理机制，笔者人为的把类初始化的处理过程分为了五个阶段）： 第一阶段：通过在Class对象上同步（即获取Class对象的初始化锁），来控制类或接口的初始化。这个获取锁的线程会一直等待，直到当前线程能够获取到这个初始化锁。 假设Class对象当前还没有被初始化（初始化状态state此时被标记为state = noInitialization），且有两个线程A和B试图同时初始化这个Class对象。下面是对应的示意图： 下面是这个示意图的说明： 时间 线程A 线程B t1 A1:尝试获取Class对象的初始化锁。这里假设线程A获取到了初始化锁 B1:尝试获取Class对象的初始化锁，由于线程A获取到了锁，线程B将一直等待获取初始化锁 t2 A2：线程A看到线程还未被初始化（因为读取到state == noInitialization），线程设置state = initializing t3 A3：线程A释放初始化锁 第二阶段：线程A执行类的初始化，同时线程B在初始化锁对应的condition上等待： 下面是这个示意图的说明： 时间 线程A 线程B t1 A1:执行类的静态初始化和初始化类中声明的静态字段 B1：获取到初始化锁 t2 B2：读取到state == initializing t3 B3：释放初始化锁 t4 B4：在初始化锁的condition中等待 第三阶段：线程A设置state = initialized，然后唤醒在condition中等待的所有线程： 下面是这个示意图的说明： 时间 线程A t1 A1：获取初始化锁 t2 A2：设置state = initialized t3 A3：唤醒在condition中等待的所有线程 t4 A4：释放初始化锁 t5 A5：线程A的初始化处理过程完成 第四阶段：线程B结束类的初始化处理： 下面是这个示意图的说明： 时间 线程B t1 B1：获取初始化锁 t2 B2：读取到state == initialized t3 B3：释放初始化锁 t4 B4：线程B的类初始化处理过程完成 线程A在第二阶段的A1执行类的初始化，并在第三阶段的A4释放初始化锁；线程B在第四阶段的B1获取同一个初始化锁，并在第四阶段的B4之后才开始访问这个类。根据java内存模型规范的锁规则，这里将存在如下的happens-before关系： 个happens-before关系将保证：线程A执行类的初始化时的写入操作（执行类的静态初始化和初始化类中声明的静态字段），线程B一定能看到。 第五阶段：线程C执行类的初始化的处理：下面是这个示意图的说明： 时间 线程C t1 C1：获取初始化锁 t2 C2：读取到state == initialized t3 C3：释放初始化锁 t4 C4：线程C的类初始化处理过程完成 在第三阶段之后，类已经完成了初始化。因此线程C在第五阶段的类初始化处理过程相对简单一些（前面的线程A和B的类初始化处理过程都经历了两次锁获取-锁释放，而线程C的类初始化处理只需要经历一次锁获取-锁释放）。 线程A在第二阶段的A1执行类的初始化，并在第三阶段的A4释放锁；线程C在第五阶段的C1获取同一个锁，并在在第五阶段的C4之后才开始访问这个类。根据java内存模型规范的锁规则，这里将存在如下的happens-before关系： 这个happens-before关系将保证：线程A执行类的初始化时的写入操作，线程C一定能看到。 ※ 注1：这里的condition和state标记是本文虚构出来的。Java语言规范并没有硬性规定一定要使用condition和state标记。JVM的具体实现只要实现类似功能即可。 ※ 注2：Java语言规范允许Java的具体实现，优化类的初始化处理过程（对这里的第五阶段做优化），具体细节参见java语言规范的12.4.2章。 通过对比基于volatile的双重检查锁定的方案和基于类初始化的方案，我们会发现基于类初始化的方案的实现代码更简洁。但基于volatile的双重检查锁定的方案有一个额外的优势：除了可以对静态字段实现延迟初始化外，还可以对实例字段实现延迟初始化。 总结延迟初始化降低了初始化类或创建实例的开销，但增加了访问被延迟初始化的字段的开销。在大多数时候，正常的初始化要优于延迟初始化。如果确实需要对实例字段使用线程安全的延迟初始化，请使用上面介绍的基于volatile的延迟初始化的方案；如果确实需要对静态字段使用线程安全的延迟初始化，请使用上面介绍的基于类初始化的方案。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>java语言</tag>
        <tag>延迟加载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL索引背后的数据结构及算法原理]]></title>
    <url>%2F2013%2F03%2F28%2F2013-03-28-index%2F</url>
    <content type="text"><![CDATA[原文链接: http://blog.jobbole.com/24006/ 摘要本文以MySQL数据库为研究对象，讨论与数据库索引相关的一些话题。特别需要说明的是，MySQL支持诸多存储引擎，而各种存储引擎对索引的支持也各不相同，因此MySQL数据库支持多种索引类型，如BTree索引，哈希索引，全文索引等等。为了避免混乱，本文将只关注于BTree索引，因为这是平常使用MySQL时主要打交道的索引，至于哈希索引和全文索引本文暂不讨论。 文章主要内容分为三个部分。 第一部分主要从数据结构及算法理论层面讨论MySQL数据库索引的数理基础。 第二部分结合MySQL数据库中MyISAM和InnoDB数据存储引擎中索引的架构实现讨论聚集索引、非聚集索引及覆盖索引等话题。 第三部分根据上面的理论基础，讨论MySQL中高性能使用索引的策略。 索引的本质MySQL官方对索引的定义为：索引（Index）是帮助MySQL高效获取数据的数据结构。提取句子主干，就可以得到索引的本质：索引是数据结构。 我们知道，数据库查询是数据库的最主要功能之一。我们都希望查询数据的速度能尽可能的快，因此数据库系统的设计者会从查询算法的角度进行优化。最基本的查询算法当然是顺序查找（linear search），这种复杂度为O(n)的算法在数据量很大时显然是糟糕的，好在计算机科学的发展提供了很多更优秀的查找算法，例如二分查找（binary search）、二叉树查找（binary tree search）等。如果稍微分析一下会发现，每种查找算法都只能应用于特定的数据结构之上，例如二分查找要求被检索数据有序，而二叉树查找只能应用于二叉查找树上，但是数据本身的组织结构不可能完全满足各种数据结构（例如，理论上不可能同时将两列都按顺序进行组织），所以，在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。 看一个例子： 图1 图1展示了一种可能的索引方式。左边是数据表，一共有两列七条记录，最左边的是数据记录的物理地址（注意逻辑上相邻的记录在磁盘上也并不是一定物理相邻的）。为了加快Col2的查找，可以维护一个右边所示的二叉查找树，每个节点分别包含索引键值和一个指向对应数据记录物理地址的指针，这样就可以运用二叉查找在O(log2n)的复杂度内获取到相应数据。 虽然这是一个货真价实的索引，但是实际的数据库系统几乎没有使用二叉查找树或其进化品种红黑树（red-black tree）实现的，原因会在下文介绍。 B-Tree和B+Tree目前大部分数据库系统及文件系统都采用B-Tree或其变种B+Tree作为索引结构，在本文的下一节会结合存储器原理及计算机存取原理讨论为什么B-Tree和B+Tree在被如此广泛用于索引，这一节先单纯从数据结构角度描述它们。 B-Tree为了描述B-Tree，首先定义一条数据记录为一个二元组[key, data]，key为记录的键值，对于不同数据记录，key是互不相同的；data为数据记录除key外的数据。那么B-Tree是满足下列条件的数据结构： d为大于1的一个正整数，称为B-Tree的度。 h为一个正整数，称为B-Tree的高度。 每个非叶子节点由n-1个key和n个指针组成，其中d&lt;=n&lt;=2d。 每个叶子节点最少包含一个key和两个指针，最多包含2d-1个key和2d个指针，叶节点的指针均为null 。 所有叶节点具有相同的深度，等于树高h。 key和指针互相间隔，节点两端是指针。 一个节点中的key从左到右非递减排列。 所有节点组成树结构。 每个指针要么为null，要么指向另外一个节点。 如果某个指针在节点node最左边且不为null，则其指向节点的所有key小于v(key1)，其中v(key1)为node的第一个key的值。 如果某个指针在节点node最右边且不为null，则其指向节点的所有key大于v(keym)，其中v(keym)为node的最后一个key的值。 如果某个指针在节点node的左右相邻key分别是keyi和keyi+1且不为null，则其指向节点的所有key小于v(keyi+1)且大于v(keyi)。 图2是一个d=2的B-Tree示意图。 由于B-Tree的特性，在B-Tree中按key检索数据的算法非常直观：首先从根节点进行二分查找，如果找到则返回对应节点的data，否则对相应区间的指针指向的节点递归进行查找，直到找到节点或找到null指针，前者查找成功，后者查找失败。B-Tree上查找算法的伪代码如下：1234567891011121314BTree_Search(node, key)&#123; if(node == null) return null; foreach(node.key) &#123; if(node.key[i] == key) return node.data[i]; if(node.key[i] &gt; key) return BTree_Search(point[i]-&gt;node); &#125; return BTree_Search(point[i+1]-&gt;node);&#125;data = BTree_Search(root, my_key); 关于B-Tree有一系列有趣的性质，例如一个度为d的B-Tree，设其索引N个key，则其树高h的上限为logd((N+1)/2)，检索一个key，其查找节点个数的渐进复杂度为O(logdN)。从这点可以看出，B-Tree是一个非常有效率的索引数据结构。 另外，由于插入删除新的数据记录会破坏B-Tree的性质，因此在插入删除时，需要对树进行一个分裂、合并、转移等操作以保持B-Tree性质，本文不打算完整讨论B-Tree这些内容，因为已经有许多资料详细说明了B-Tree的数学性质及插入删除算法，有兴趣的朋友可以在本文末的参考文献一栏找到相应的资料进行阅读。 B+TreeB-Tree有许多变种，其中最常见的是B+Tree，例如MySQL就普遍使用B+Tree实现其索引结构。 与B-Tree相比，B+Tree有以下不同点： 每个节点的指针上限为2d而不是2d+1。 内节点不存储data，只存储key；叶子节点不存储指针。 图3是一个简单的B+Tree示意。 图3 由于并不是所有节点都具有相同的域，因此B+Tree中叶节点和内节点一般大小不同。这点与B-Tree不同，虽然B-Tree中不同节点存放的key和指针可能数量不一致，但是每个节点的域和上限是一致的，所以在实现中B-Tree往往对每个节点申请同等大小的空间。 一般来说，B+Tree比B-Tree更适合实现外存储索引结构，具体原因与外存储器原理及计算机存取原理有关，将在下面讨论。 带有顺序访问指针的B+Tree一般在数据库系统或文件系统中使用的B+Tree结构都在经典B+Tree的基础上进行了优化，增加了顺序访问指针。 图4 如图4所示，在B+Tree的每个叶子节点增加一个指向相邻叶子节点的指针，就形成了带有顺序访问指针的B+Tree。做这个优化的目的是为了提高区间访问的性能，例如图4中如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。 这一节对B-Tree和B+Tree进行了一个简单的介绍，下一节结合存储器存取原理介绍为什么目前B+Tree是数据库系统实现索引的首选数据结构。 为什么使用B-Tree（B+Tree） 上文说过，红黑树等数据结构也可以用来实现索引，但是文件系统及数据库系统普遍采用B-/+Tree作为索引结构，这一节将结合计算机组成原理相关知识讨论B-/+Tree作为索引的理论基础。 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。下面先介绍内存和磁盘存取原理，然后再结合这些原理分析B-/+Tree作为索引的效率。 主存存取原理目前计算机使用的主存基本都是随机读写存储器（RAM），现代RAM的结构和存取原理比较复杂，这里本文抛却具体差别，抽象出一个十分简单的存取模型来说明RAM的工作原理。 图5 从抽象角度看，主存是一系列的存储单元组成的矩阵，每个存储单元存储固定大小的数据。每个存储单元有唯一的地址，现代主存的编址规则比较复杂，这里将其简化成一个二维地址：通过一个行地址和一个列地址可以唯一定位到一个存储单元。图5展示了一个4 x 4的主存模型。 主存的存取过程如下： 当系统需要读取主存时，则将地址信号放到地址总线上传给主存，主存读到地址信号后，解析信号并定位到指定存储单元，然后将此存储单元数据放到数据总线上，供其它部件读取。 写主存的过程类似，系统将要写入单元地址和数据分别放在地址总线和数据总线上，主存读取两个总线的内容，做相应的写操作。 这里可以看出，主存存取的时间仅与存取次数呈线性关系，因为不存在机械操作，两次存取的数据的“距离”不会对时间有任何影响，例如，先取A0再取A1和先取A0再取D3的时间消耗是一样的。 磁盘存取原理上文说过，索引一般以文件形式存储在磁盘上，索引检索需要磁盘I/O操作。与主存不同，磁盘I/O存在机械运动耗费，因此磁盘I/O的时间消耗是巨大的。 图6是磁盘的整体结构示意图。 MySQL索引背后的数据结构及算法原理 图6 一个磁盘由大小相同且同轴的圆形盘片组成，磁盘可以转动（各个磁盘必须同步转动）。在磁盘的一侧有磁头支架，磁头支架固定了一组磁头，每个磁头负责存取一个磁盘的内容。磁头不能转动，但是可以沿磁盘半径方向运动（实际是斜切向运动），每个磁头同一时刻也必须是同轴的，即从正上方向下看，所有磁头任何时候都是重叠的（不过目前已经有多磁头独立技术，可不受此限制）。 图7是磁盘结构的示意图。 盘片被划分成一系列同心环，圆心是盘片中心，每个同心环叫做一个磁道，所有半径相同的磁道组成一个柱面。磁道被沿半径线划分成一个个小的段，每个段叫做一个扇区，每个扇区是磁盘的最小存储单元。为了简单起见，我们下面假设磁盘只有一个盘片和一个磁头。 当需要从磁盘读取数据时，系统会将数据逻辑地址传给磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址，即确定要读的数据在哪个磁道，哪个扇区。为了读取这个扇区的数据，需要将磁头放到这个扇区上方，为了实现这一点，磁头需要移动对准相应磁道，这个过程叫做寻道，所耗费时间叫做寻道时间，然后磁盘旋转将目标扇区旋转到磁头下，这个过程耗费的时间叫做旋转时间。 局部性原理与磁盘预读由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理： 当一个数据被用到时，其附近的数据也通常会马上被使用。 程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 B-/+Tree索引的性能分析到这里终于可以分析B-/+Tree索引的性能了。 上文说过一般使用磁盘I/O次数评价索引结构的优劣。先从B-Tree分析，根据B-Tree的定义，可知检索一次最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧： 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为O(h)=O(logdN)。一般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小（通常不超过3）。 综上所述，用B-Tree作为索引结构效率是非常高的。 而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，效率明显比B-Tree差很多。 上文还说过，B+Tree更适合外存索引，原因和内节点出度d有关。从上面分析可以看到，d越大索引的性能越好，而出度的上限取决于节点内key和data的大小： dmax = floor(pagesize / (keysize + datasize + pointsize)) (pagesize – dmax &gt;= pointsize) 或 dmax = floor(pagesize / (keysize + datasize + pointsize)) – 1 (pagesize – dmax &lt; pointsize) floor表示向下取整。由于B+Tree内节点去掉了data域，因此可以拥有更大的出度，拥有更好的性能。 这一章从理论角度讨论了与索引相关的数据结构与算法问题，下一章将讨论B+Tree是如何具体实现为MySQL中索引，同时将结合MyISAM和InnDB存储引擎介绍非聚集索引和聚集索引两种不同的索引实现形式。 MySQL索引实现在MySQL中，索引属于存储引擎级别的概念，不同存储引擎对索引的实现方式是不同的，本文主要讨论MyISAM和InnoDB两个存储引擎的索引实现方式。 MyISAM索引实现MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址。下图是MyISAM索引的原理图： 图8 这里设表一共有三列，假设我们以Col1为主键，则图8是一个MyISAM表的主索引（Primary key）示意。可以看出MyISAM的索引文件仅仅保存数据记录的地址。在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。如果我们在Col2上建立一个辅助索引，则此索引的结构如下图所示： 同样也是一颗B+Tree，data域保存数据记录的地址。因此，MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。 MyISAM的索引方式也叫做“非聚集”的，之所以这么称呼是为了与InnoDB的聚集索引区分。 InnoDB索引实现虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。 第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。 图10是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。 第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。例如，图11为定义在Col3上的一个辅助索引： 这里以英文字符的ASCII码作为比较准则。聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。 了解不同存储引擎的索引实现方式对于正确使用和优化索引都非常有帮助，例如知道了InnoDB的索引实现后，就很容易明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。再例如，用非单调的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择。 下一章将具体讨论这些与索引有关的优化策略。. 索引使用策略及优化MySQL的优化主要分为结构优化（Scheme optimization）和查询优化（Query optimization）。本章讨论的高性能索引策略主要属于结构优化范畴。本章的内容完全基于上文的理论基础，实际上一旦理解了索引背后的机制，那么选择高性能的策略就变成了纯粹的推理，并且可以理解这些策略背后的逻辑。 示例数据库 为了讨论索引策略，需要一个数据量不算小的数据库作为示例。本文选用MySQL官方文档中提供的示例数据库之一：employees。这个数据库关系复杂度适中，且数据量较大。下图是这个数据库的E-R关系图（引用自MySQL官方手册）： MySQL官方文档中关于此数据库的页面为。里面详细介绍了此数据库，并提供了下载地址和导入方法，如果有兴趣导入此数据库到自己的MySQL可以参考文中内容。 最左前缀原理与相关优化高效使用索引的首要条件是知道什么样的查询会使用到索引，这个问题和B+Tree中的“最左前缀原理”有关，下面通过例子说明最左前缀原理。 这里先说一下联合索引的概念。在上文中，我们都是假设索引只引用了单个的列，实际上，MySQL中的索引可以以一定顺序引用多个列，这种索引叫做联合索引，一般的，一个联合索引是一个有序元组，其中各个元素均为数据表的一列，实际上要严格定义索引需要用到关系代数，但是这里我不想讨论太多关系代数的话题，因为那样会显得很枯燥，所以这里就不再做严格定义。另外，单列索引可以看成联合索引元素数为1的特例。 以employees.titles表为例，下面先查看其上都有哪些索引：123456789SHOW INDEX FROM employees.titles;+--------+------------+----------+--------------+-------------+-----------+-------------+------+------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Null | Index_type |+--------+------------+----------+--------------+-------------+-----------+-------------+------+------------+| titles | 0 | PRIMARY | 1 | emp_no | A | NULL | | BTREE || titles | 0 | PRIMARY | 2 | title | A | NULL | | BTREE || titles | 0 | PRIMARY | 3 | from_date | A | 443308 | | BTREE || titles | 1 | emp_no | 1 | emp_no | A | 443308 | | BTREE |+--------+------------+----------+--------------+-------------+----------- 从结果中可以到titles表的主索引为，还有一个辅助索引。为了避免多个索引使事情变复杂（MySQL的SQL优化器在多索引时行为比较复杂），这里我们将辅助索引drop掉：1ALTER TABLE employees.titles DROP INDEX emp_no; 情况一：全列匹配。123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND title=&apos;Senior Engineer&apos; AND from_date=&apos;1986-06-26&apos;;+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+| 1 | SIMPLE | titles | const | PRIMARY | PRIMARY | 59 | const,const,const | 1 | |+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+ 很明显，当按照索引中所有列进行精确匹配（这里精确匹配指“=”或“IN”匹配）时，索引可以被用到。这里有一点需要注意，理论上索引对顺序是敏感的，但是由于MySQL的查询优化器会自动调整where子句的条件顺序以使用适合的索引，例如我们将where中的条件顺序颠倒：123456EXPLAIN SELECT * FROM employees.titles WHERE from_date=&apos;1986-06-26&apos; AND emp_no=&apos;10001&apos; AND title=&apos;Senior Engineer&apos;;+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+| 1 | SIMPLE | titles | const | PRIMARY | PRIMARY | 59 | const,const,const | 1 | |+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+ 效果是一样的。 情况二：最左前缀匹配。123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos;;+----+-------------+--------+------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | titles | ref | PRIMARY | PRIMARY | 4 | const | 1 | |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------+ 当查询条件精确匹配索引的左边连续一个或几个列时，如或，所以可以被用到，但是只能用到一部分，即条件所组成的最左前缀。上面的查询从分析结果看用到了PRIMARY索引，但是key_len为4，说明只用到了索引的第一列前缀。 情况三：查询条件用到了索引中列的精确匹配，但是中间某个条件未提供。123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND from_date=&apos;1986-06-26&apos;;+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+| 1 | SIMPLE | titles | ref | PRIMARY | PRIMARY | 4 | const | 1 | Using where |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+ 此时索引使用情况和情况二相同，因为title未提供，所以查询只用到了索引的第一列，而后面的from_date虽然也在索引中，但是由于title不存在而无法和左前缀连接，因此需要对结果进行扫描过滤from_date（这里由于emp_no唯一，所以不存在扫描）。如果想让from_date也使用索引而不是where过滤，可以增加一个辅助索引，此时上面的查询会使用这个索引。除此之外，还可以使用一种称之为“隔离列”的优化方法，将emp_no与from_date之间的“坑”填上。 首先我们看下title一共有几种不同的值：123456789101112SELECT DISTINCT(title) FROM employees.titles;+--------------------+| title |+--------------------+| Senior Engineer || Staff || Engineer || Senior Staff || Assistant Engineer || Technique Leader || Manager |+--------------------+ 只有7种。在这种成为“坑”的列值比较少的情况下，可以考虑用“IN”来填补这个“坑”从而形成最左前缀：12345678910EXPLAIN SELECT * FROM employees.titlesWHERE emp_no=&apos;10001&apos;AND title IN (&apos;Senior Engineer&apos;, &apos;Staff&apos;, &apos;Engineer&apos;, &apos;Senior Staff&apos;, &apos;Assistant Engineer&apos;, &apos;Technique Leader&apos;, &apos;Manager&apos;)AND from_date=&apos;1986-06-26&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 59 | NULL | 7 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 这次key_len为59，说明索引被用全了，但是从type和rows看出IN实际上执行了一个range查询，这里检查了7个key。看下两种查询的性能比较：1234567SHOW PROFILES;+----------+------------+-------------------------------------------------------------------------------+| Query_ID | Duration | Query |+----------+------------+-------------------------------------------------------------------------------+| 10 | 0.00058000 | SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND from_date=&apos;1986-06-26&apos;|| 11 | 0.00052500 | SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND title IN ... |+----------+------------+-------------------------------------------------------------------------------+ “填坑”后性能提升了一点。如果经过emp_no筛选后余下很多数据，则后者性能优势会更加明显。当然，如果title的值很多，用填坑就不合适了，必须建立辅助索引。 情况四：查询条件没有指定索引第一列。123456EXPLAIN SELECT * FROM employees.titles WHERE from_date=&apos;1986-06-26&apos;;+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+| 1 | SIMPLE | titles | ALL | NULL | NULL | NULL | NULL | 443308 | Using where |+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+ 由于不是最左前缀，索引这样的查询显然用不到索引。 情况五：匹配某列的前缀字符串。123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND title LIKE &apos;Senior%&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 56 | NULL | 1 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 如果通配符%不出现在开头，则可以用到索引，但根据具体情况不同可能只会用其中一个前缀。 情况六：范围查询。1234567EXPLAIN SELECT * FROM employees.titles WHERE emp_no &lt; &apos;10010&apos; and title=&apos;Senior Engineer&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 4 | NULL | 16 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+` 范围列可以用到索引（必须是最左前缀），但是范围列后面的列无法用到索引。同时，索引最多用于一个范围列，因此如果查询条件中有两个范围列则无法全用到索引。12345678910EXPLAIN SELECT * FROM employees.titlesWHERE emp_no &lt; 10010&apos;AND title=&apos;Senior Engineer&apos;AND from_date BETWEEN &apos;1986-01-01&apos; AND &apos;1986-12-31&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 4 | NULL | 16 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 可以看到索引对第二个范围索引无能为力。这里特别要说明MySQL一个有意思的地方，那就是仅用explain可能无法区分范围索引和多值匹配，因为在type中这两者都显示为range。同时，用了“between”并不意味着就是范围查询，例如下面的查询：12345678910EXPLAIN SELECT * FROM employees.titlesWHERE emp_no BETWEEN &apos;10001&apos; AND &apos;10010&apos;AND title=&apos;Senior Engineer&apos;AND from_date BETWEEN &apos;1986-01-01&apos; AND &apos;1986-12-31&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 59 | NULL | 16 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 看起来是用了两个范围查询，但作用于emp_no上的“BETWEEN”实际上相当于“IN”，也就是说emp_no实际是多值精确匹配。可以看到这个查询用到了索引全部三个列。因此在MySQL中要谨慎地区分多值匹配和范围匹配，否则会对MySQL的行为产生困惑。 情况七：查询条件中含有函数或表达式。 很不幸，如果查询条件中含有函数或表达式，则MySQL不会为这列使用索引（虽然某些在数学意义上可以使用）。例如：123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND left(title, 6)=&apos;Senior&apos;;+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+| 1 | SIMPLE | titles | ref | PRIMARY | PRIMARY | 4 | const | 1 | Using where |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+ 虽然这个查询和情况五中功能相同，但是由于使用了函数left，则无法为title列应用索引，而情况五中用LIKE则可以。再如：123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no - 1=&apos;10000&apos;;+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+| 1 | SIMPLE | titles | ALL | NULL | NULL | NULL | NULL | 443308 | Using where |+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+ 显然这个查询等价于查询emp_no为10001的函数，但是由于查询条件是一个表达式，MySQL无法为其使用索引。看来MySQL还没有智能到自动优化常量表达式的程度，因此在写查询语句时尽量避免表达式出现在查询中，而是先手工私下代数运算，转换为无表达式的查询语句。 索引选择性与前缀索引既然索引可以加快查询速度，那么是不是只要是查询语句需要，就建上索引？答案是否定的。因为索引虽然加快了查询速度，但索引也是有代价的：索引文件本身要消耗存储空间，同时索引会加重插入、删除和修改记录时的负担，另外，MySQL在运行时也要消耗资源维护索引，因此索引并不是越多越好。一般两种情况下不建议建索引。 第一种情况是表记录比较少，例如一两千条甚至只有几百条记录的表，没必要建索引，让查询做全表扫描就好了。至于多少条记录才算多，这个个人有个人的看法，我个人的经验是以2000作为分界线，记录数不超过 2000可以考虑不建索引，超过2000条可以酌情考虑索引。 另一种不建议建索引的情况是索引的选择性较低。所谓索引的选择性（Selectivity），是指不重复的索引值（也叫基数，Cardinality）与表记录数（#T）的比值： Index Selectivity = Cardinality / #T 显然选择性的取值范围为(0, 1]，选择性越高的索引价值越大，这是由B+Tree的性质决定的。例如，上文用到的employees.titles表，如果title字段经常被单独查询，是否需要建索引，我们看一下它的选择性：123456789101112SELECT count(DISTINCT(title))/count(*) AS Selectivity FROM employees.titles; +-------------+ | Selectivity | +-------------+ | 0.0000 | +-------------+``` title的选择性不足0.0001（精确值为0.00001579），所以实在没有什么必要为其单独建索引。有一种与索引选择性有关的索引优化策略叫做**前缀索引，就是用列的前缀代替整个列作为索引key**，当前缀长度合适时，可以做到既使得前缀索引的选择性接近全列索引，同时因为索引key变短而减少了索引文件的大小和维护开销。下面以employees.employees表为例介绍前缀索引的选择和使用。从图12可以看到employees表只有一个索引&lt;emp_no&gt;，那么如果我们想按名字搜索一个人，就只能全表扫描了： EXPLAIN SELECT * FROM employees.employees WHERE first_name=’Eric’ AND last_name=’Anido’;+—-+————-+———–+——+—————+——+———+——+——–+————-+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+—-+————-+———–+——+—————+——+———+——+——–+————-+| 1 | SIMPLE | employees | ALL | NULL | NULL | NULL | NULL | 300024 | Using where |+—-+————-+———–+——+—————+——+———+——+——–+————-+1如果频繁按名字搜索员工，这样显然效率很低，因此我们可以考虑建索引。有两种选择，建&lt;first_name&gt;或&lt;first_name, last_name&gt;，看下两个索引的选择性： SELECT count(DISTINCT(first_name))/count(*) AS Selectivity FROM employees.employees;+————-+| Selectivity |+————-+| 0.0042 |+————-+ SELECT count(DISTINCT(concat(first_name, last_name)))/count(*) AS Selectivity FROM employees.employees;+————-+| Selectivity |+————-+| 0.9313 |+————-+123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824文章 – 伯乐在线首页最新文章IT 职场前端 后端 移动端 数据库运维 其他技术伯乐在线 &gt; 首页 &gt; 所有文章 &gt; IT技术 &gt; MySQL索引背后的数据结构及算法原理MySQL索引背后的数据结构及算法原理2013/03/28 · IT技术 · 17 评论 · MySQL, 数据结构, 算法分享到： 0原文出处： 张洋 摘要本文以MySQL数据库为研究对象，讨论与数据库索引相关的一些话题。特别需要说明的是，MySQL支持诸多存储引擎，而各种存储引擎对索引的支持也各不相同，因此MySQL数据库支持多种索引类型，如BTree索引，哈希索引，全文索引等等。为了避免混乱，本文将只关注于BTree索引，因为这是平常使用MySQL时主要打交道的索引，至于哈希索引和全文索引本文暂不讨论。文章主要内容分为三个部分。第一部分主要从数据结构及算法理论层面讨论MySQL数据库索引的数理基础。第二部分结合MySQL数据库中MyISAM和InnoDB数据存储引擎中索引的架构实现讨论聚集索引、非聚集索引及覆盖索引等话题。第三部分根据上面的理论基础，讨论MySQL中高性能使用索引的策略。数据结构及算法基础索引的本质MySQL官方对索引的定义为：索引（Index）是帮助MySQL高效获取数据的数据结构。提取句子主干，就可以得到索引的本质：索引是数据结构。我们知道，数据库查询是数据库的最主要功能之一。我们都希望查询数据的速度能尽可能的快，因此数据库系统的设计者会从查询算法的角度进行优化。最基本的查询算法当然是顺序查找（linear search），这种复杂度为O(n)的算法在数据量很大时显然是糟糕的，好在计算机科学的发展提供了很多更优秀的查找算法，例如二分查找（binary search）、二叉树查找（binary tree search）等。如果稍微分析一下会发现，每种查找算法都只能应用于特定的数据结构之上，例如二分查找要求被检索数据有序，而二叉树查找只能应用于二叉查找树上，但是数据本身的组织结构不可能完全满足各种数据结构（例如，理论上不可能同时将两列都按顺序进行组织），所以，在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。看一个例子：MySQL索引背后的数据结构及算法原理图1图1展示了一种可能的索引方式。左边是数据表，一共有两列七条记录，最左边的是数据记录的物理地址（注意逻辑上相邻的记录在磁盘上也并不是一定物理相邻的）。为了加快Col2的查找，可以维护一个右边所示的二叉查找树，每个节点分别包含索引键值和一个指向对应数据记录物理地址的指针，这样就可以运用二叉查找在O(log2n)的复杂度内获取到相应数据。虽然这是一个货真价实的索引，但是实际的数据库系统几乎没有使用二叉查找树或其进化品种红黑树（red-black tree）实现的，原因会在下文介绍。B-Tree和B+Tree目前大部分数据库系统及文件系统都采用B-Tree或其变种B+Tree作为索引结构，在本文的下一节会结合存储器原理及计算机存取原理讨论为什么B-Tree和B+Tree在被如此广泛用于索引，这一节先单纯从数据结构角度描述它们。B-Tree为了描述B-Tree，首先定义一条数据记录为一个二元组[key, data]，key为记录的键值，对于不同数据记录，key是互不相同的；data为数据记录除key外的数据。那么B-Tree是满足下列条件的数据结构：1. d为大于1的一个正整数，称为B-Tree的度。2. h为一个正整数，称为B-Tree的高度。3. 每个非叶子节点由n-1个key和n个指针组成，其中d&lt;=n&lt;=2d。4. 每个叶子节点最少包含一个key和两个指针，最多包含2d-1个key和2d个指针，叶节点的指针均为null 。5. 所有叶节点具有相同的深度，等于树高h。6. key和指针互相间隔，节点两端是指针。7. 一个节点中的key从左到右非递减排列。8. 所有节点组成树结构。9. 每个指针要么为null，要么指向另外一个节点。10. 如果某个指针在节点node最左边且不为null，则其指向节点的所有key小于v(key1)，其中v(key1)为node的第一个key的值。11. 如果某个指针在节点node最右边且不为null，则其指向节点的所有key大于v(keym)，其中v(keym)为node的最后一个key的值。12. 如果某个指针在节点node的左右相邻key分别是keyi和keyi+1且不为null，则其指向节点的所有key小于v(keyi+1)且大于v(keyi)。图2是一个d=2的B-Tree示意图。MySQL索引背后的数据结构及算法原理图2由于B-Tree的特性，在B-Tree中按key检索数据的算法非常直观：首先从根节点进行二分查找，如果找到则返回对应节点的data，否则对相应区间的指针指向的节点递归进行查找，直到找到节点或找到null指针，前者查找成功，后者查找失败。B-Tree上查找算法的伪代码如下：CBTree_Search(node, key)&#123; if(node == null) return null; foreach(node.key) &#123; if(node.key[i] == key) return node.data[i]; if(node.key[i] &gt; key) return BTree_Search(point[i]-&gt;node); &#125; return BTree_Search(point[i+1]-&gt;node);&#125;data = BTree_Search(root, my_key);1234567891011121314BTree_Search(node, key)&#123; if(node == null) return null; foreach(node.key) &#123; if(node.key[i] == key) return node.data[i]; if(node.key[i] &gt; key) return BTree_Search(point[i]-&gt;node); &#125; return BTree_Search(point[i+1]-&gt;node);&#125;data = BTree_Search(root, my_key);关于B-Tree有一系列有趣的性质，例如一个度为d的B-Tree，设其索引N个key，则其树高h的上限为logd((N+1)/2)，检索一个key，其查找节点个数的渐进复杂度为O(logdN)。从这点可以看出，B-Tree是一个非常有效率的索引数据结构。另外，由于插入删除新的数据记录会破坏B-Tree的性质，因此在插入删除时，需要对树进行一个分裂、合并、转移等操作以保持B-Tree性质，本文不打算完整讨论B-Tree这些内容，因为已经有许多资料详细说明了B-Tree的数学性质及插入删除算法，有兴趣的朋友可以在本文末的参考文献一栏找到相应的资料进行阅读。B+TreeB-Tree有许多变种，其中最常见的是B+Tree，例如MySQL就普遍使用B+Tree实现其索引结构。与B-Tree相比，B+Tree有以下不同点：1. 每个节点的指针上限为2d而不是2d+1。2. 内节点不存储data，只存储key；叶子节点不存储指针。图3是一个简单的B+Tree示意。MySQL索引背后的数据结构及算法原理图3由于并不是所有节点都具有相同的域，因此B+Tree中叶节点和内节点一般大小不同。这点与B-Tree不同，虽然B-Tree中不同节点存放的key和指针可能数量不一致，但是每个节点的域和上限是一致的，所以在实现中B-Tree往往对每个节点申请同等大小的空间。一般来说，B+Tree比B-Tree更适合实现外存储索引结构，具体原因与外存储器原理及计算机存取原理有关，将在下面讨论。带有顺序访问指针的B+Tree一般在数据库系统或文件系统中使用的B+Tree结构都在经典B+Tree的基础上进行了优化，增加了顺序访问指针。MySQL索引背后的数据结构及算法原理图4如图4所示，在B+Tree的每个叶子节点增加一个指向相邻叶子节点的指针，就形成了带有顺序访问指针的B+Tree。做这个优化的目的是为了提高区间访问的性能，例如图4中如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。这一节对B-Tree和B+Tree进行了一个简单的介绍，下一节结合存储器存取原理介绍为什么目前B+Tree是数据库系统实现索引的首选数据结构。为什么使用B-Tree（B+Tree）上文说过，红黑树等数据结构也可以用来实现索引，但是文件系统及数据库系统普遍采用B-/+Tree作为索引结构，这一节将结合计算机组成原理相关知识讨论B-/+Tree作为索引的理论基础。一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。下面先介绍内存和磁盘存取原理，然后再结合这些原理分析B-/+Tree作为索引的效率。主存存取原理目前计算机使用的主存基本都是随机读写存储器（RAM），现代RAM的结构和存取原理比较复杂，这里本文抛却具体差别，抽象出一个十分简单的存取模型来说明RAM的工作原理。MySQL索引背后的数据结构及算法原理图5从抽象角度看，主存是一系列的存储单元组成的矩阵，每个存储单元存储固定大小的数据。每个存储单元有唯一的地址，现代主存的编址规则比较复杂，这里将其简化成一个二维地址：通过一个行地址和一个列地址可以唯一定位到一个存储单元。图5展示了一个4 x 4的主存模型。主存的存取过程如下：当系统需要读取主存时，则将地址信号放到地址总线上传给主存，主存读到地址信号后，解析信号并定位到指定存储单元，然后将此存储单元数据放到数据总线上，供其它部件读取。写主存的过程类似，系统将要写入单元地址和数据分别放在地址总线和数据总线上，主存读取两个总线的内容，做相应的写操作。这里可以看出，主存存取的时间仅与存取次数呈线性关系，因为不存在机械操作，两次存取的数据的“距离”不会对时间有任何影响，例如，先取A0再取A1和先取A0再取D3的时间消耗是一样的。磁盘存取原理上文说过，索引一般以文件形式存储在磁盘上，索引检索需要磁盘I/O操作。与主存不同，磁盘I/O存在机械运动耗费，因此磁盘I/O的时间消耗是巨大的。图6是磁盘的整体结构示意图。MySQL索引背后的数据结构及算法原理图6一个磁盘由大小相同且同轴的圆形盘片组成，磁盘可以转动（各个磁盘必须同步转动）。在磁盘的一侧有磁头支架，磁头支架固定了一组磁头，每个磁头负责存取一个磁盘的内容。磁头不能转动，但是可以沿磁盘半径方向运动（实际是斜切向运动），每个磁头同一时刻也必须是同轴的，即从正上方向下看，所有磁头任何时候都是重叠的（不过目前已经有多磁头独立技术，可不受此限制）。图7是磁盘结构的示意图。MySQL索引背后的数据结构及算法原理图7盘片被划分成一系列同心环，圆心是盘片中心，每个同心环叫做一个磁道，所有半径相同的磁道组成一个柱面。磁道被沿半径线划分成一个个小的段，每个段叫做一个扇区，每个扇区是磁盘的最小存储单元。为了简单起见，我们下面假设磁盘只有一个盘片和一个磁头。当需要从磁盘读取数据时，系统会将数据逻辑地址传给磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址，即确定要读的数据在哪个磁道，哪个扇区。为了读取这个扇区的数据，需要将磁头放到这个扇区上方，为了实现这一点，磁头需要移动对准相应磁道，这个过程叫做寻道，所耗费时间叫做寻道时间，然后磁盘旋转将目标扇区旋转到磁头下，这个过程耗费的时间叫做旋转时间。局部性原理与磁盘预读由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用。程序运行期间所需要的数据通常比较集中。由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。B-/+Tree索引的性能分析到这里终于可以分析B-/+Tree索引的性能了。上文说过一般使用磁盘I/O次数评价索引结构的优劣。先从B-Tree分析，根据B-Tree的定义，可知检索一次最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧：每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为O(h)=O(logdN)。一般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小（通常不超过3）。综上所述，用B-Tree作为索引结构效率是非常高的。而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，效率明显比B-Tree差很多。上文还说过，B+Tree更适合外存索引，原因和内节点出度d有关。从上面分析可以看到，d越大索引的性能越好，而出度的上限取决于节点内key和data的大小：dmax = floor(pagesize / (keysize + datasize + pointsize)) (pagesize – dmax &gt;= pointsize)或dmax = floor(pagesize / (keysize + datasize + pointsize)) – 1 (pagesize – dmax &lt; pointsize)floor表示向下取整。由于B+Tree内节点去掉了data域，因此可以拥有更大的出度，拥有更好的性能。这一章从理论角度讨论了与索引相关的数据结构与算法问题，下一章将讨论B+Tree是如何具体实现为MySQL中索引，同时将结合MyISAM和InnDB存储引擎介绍非聚集索引和聚集索引两种不同的索引实现形式。MySQL索引实现在MySQL中，索引属于存储引擎级别的概念，不同存储引擎对索引的实现方式是不同的，本文主要讨论MyISAM和InnoDB两个存储引擎的索引实现方式。MyISAM索引实现MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址。下图是MyISAM索引的原理图：MySQL索引背后的数据结构及算法原理图8这里设表一共有三列，假设我们以Col1为主键，则图8是一个MyISAM表的主索引（Primary key）示意。可以看出MyISAM的索引文件仅仅保存数据记录的地址。在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。如果我们在Col2上建立一个辅助索引，则此索引的结构如下图所示：MySQL索引背后的数据结构及算法原理图9同样也是一颗B+Tree，data域保存数据记录的地址。因此，MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。MyISAM的索引方式也叫做“非聚集”的，之所以这么称呼是为了与InnoDB的聚集索引区分。InnoDB索引实现虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。MySQL索引背后的数据结构及算法原理图10图10是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。例如，图11为定义在Col3上的一个辅助索引：MySQL索引背后的数据结构及算法原理图11这里以英文字符的ASCII码作为比较准则。聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。了解不同存储引擎的索引实现方式对于正确使用和优化索引都非常有帮助，例如知道了InnoDB的索引实现后，就很容易明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。再例如，用非单调的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择。下一章将具体讨论这些与索引有关的优化策略。索引使用策略及优化MySQL的优化主要分为结构优化（Scheme optimization）和查询优化（Query optimization）。本章讨论的高性能索引策略主要属于结构优化范畴。本章的内容完全基于上文的理论基础，实际上一旦理解了索引背后的机制，那么选择高性能的策略就变成了纯粹的推理，并且可以理解这些策略背后的逻辑。示例数据库为了讨论索引策略，需要一个数据量不算小的数据库作为示例。本文选用MySQL官方文档中提供的示例数据库之一：employees。这个数据库关系复杂度适中，且数据量较大。下图是这个数据库的E-R关系图（引用自MySQL官方手册）：MySQL索引背后的数据结构及算法原理图12MySQL官方文档中关于此数据库的页面为http://dev.mysql.com/doc/employee/en/employee.html。里面详细介绍了此数据库，并提供了下载地址和导入方法，如果有兴趣导入此数据库到自己的MySQL可以参考文中内容。最左前缀原理与相关优化高效使用索引的首要条件是知道什么样的查询会使用到索引，这个问题和B+Tree中的“最左前缀原理”有关，下面通过例子说明最左前缀原理。这里先说一下联合索引的概念。在上文中，我们都是假设索引只引用了单个的列，实际上，MySQL中的索引可以以一定顺序引用多个列，这种索引叫做联合索引，一般的，一个联合索引是一个有序元组&lt;a1, a2, …, an&gt;，其中各个元素均为数据表的一列，实际上要严格定义索引需要用到关系代数，但是这里我不想讨论太多关系代数的话题，因为那样会显得很枯燥，所以这里就不再做严格定义。另外，单列索引可以看成联合索引元素数为1的特例。以employees.titles表为例，下面先查看其上都有哪些索引：MySQLSHOW INDEX FROM employees.titles;+--------+------------+----------+--------------+-------------+-----------+-------------+------+------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Null | Index_type |+--------+------------+----------+--------------+-------------+-----------+-------------+------+------------+| titles | 0 | PRIMARY | 1 | emp_no | A | NULL | | BTREE || titles | 0 | PRIMARY | 2 | title | A | NULL | | BTREE || titles | 0 | PRIMARY | 3 | from_date | A | 443308 | | BTREE || titles | 1 | emp_no | 1 | emp_no | A | 443308 | | BTREE |+--------+------------+----------+--------------+-------------+-----------+-------------+------+------------+123456789SHOW INDEX FROM employees.titles;+--------+------------+----------+--------------+-------------+-----------+-------------+------+------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Null | Index_type |+--------+------------+----------+--------------+-------------+-----------+-------------+------+------------+| titles | 0 | PRIMARY | 1 | emp_no | A | NULL | | BTREE || titles | 0 | PRIMARY | 2 | title | A | NULL | | BTREE || titles | 0 | PRIMARY | 3 | from_date | A | 443308 | | BTREE || titles | 1 | emp_no | 1 | emp_no | A | 443308 | | BTREE |+--------+------------+----------+--------------+-------------+-----------+-------------+------+------------+从结果中可以到titles表的主索引为&lt;emp_no, title, from_date&gt;，还有一个辅助索引&lt;emp_no&gt;。为了避免多个索引使事情变复杂（MySQL的SQL优化器在多索引时行为比较复杂），这里我们将辅助索引drop掉：MySQLALTER TABLE employees.titles DROP INDEX emp_no;1ALTER TABLE employees.titles DROP INDEX emp_no;这样就可以专心分析索引PRIMARY的行为了。情况一：全列匹配。MySQLEXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND title=&apos;Senior Engineer&apos; AND from_date=&apos;1986-06-26&apos;;+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+| 1 | SIMPLE | titles | const | PRIMARY | PRIMARY | 59 | const,const,const | 1 | |+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND title=&apos;Senior Engineer&apos; AND from_date=&apos;1986-06-26&apos;;+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+| 1 | SIMPLE | titles | const | PRIMARY | PRIMARY | 59 | const,const,const | 1 | |+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+很明显，当按照索引中所有列进行精确匹配（这里精确匹配指“=”或“IN”匹配）时，索引可以被用到。这里有一点需要注意，理论上索引对顺序是敏感的，但是由于MySQL的查询优化器会自动调整where子句的条件顺序以使用适合的索引，例如我们将where中的条件顺序颠倒：MySQLEXPLAIN SELECT * FROM employees.titles WHERE from_date=&apos;1986-06-26&apos; AND emp_no=&apos;10001&apos; AND title=&apos;Senior Engineer&apos;;+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+| 1 | SIMPLE | titles | const | PRIMARY | PRIMARY | 59 | const,const,const | 1 | |+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+123456EXPLAIN SELECT * FROM employees.titles WHERE from_date=&apos;1986-06-26&apos; AND emp_no=&apos;10001&apos; AND title=&apos;Senior Engineer&apos;;+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+| 1 | SIMPLE | titles | const | PRIMARY | PRIMARY | 59 | const,const,const | 1 | |+----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+效果是一样的。情况二：最左前缀匹配。MySQLEXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos;;+----+-------------+--------+------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | titles | ref | PRIMARY | PRIMARY | 4 | const | 1 | |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------+123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos;;+----+-------------+--------+------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | titles | ref | PRIMARY | PRIMARY | 4 | const | 1 | |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------+当查询条件精确匹配索引的左边连续一个或几个列时，如&lt;emp_no&gt;或&lt;emp_no, title&gt;，所以可以被用到，但是只能用到一部分，即条件所组成的最左前缀。上面的查询从分析结果看用到了PRIMARY索引，但是key_len为4，说明只用到了索引的第一列前缀。情况三：查询条件用到了索引中列的精确匹配，但是中间某个条件未提供。MySQLEXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND from_date=&apos;1986-06-26&apos;;+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+| 1 | SIMPLE | titles | ref | PRIMARY | PRIMARY | 4 | const | 1 | Using where |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND from_date=&apos;1986-06-26&apos;;+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+| 1 | SIMPLE | titles | ref | PRIMARY | PRIMARY | 4 | const | 1 | Using where |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+此时索引使用情况和情况二相同，因为title未提供，所以查询只用到了索引的第一列，而后面的from_date虽然也在索引中，但是由于title不存在而无法和左前缀连接，因此需要对结果进行扫描过滤from_date（这里由于emp_no唯一，所以不存在扫描）。如果想让from_date也使用索引而不是where过滤，可以增加一个辅助索引&lt;emp_no, from_date&gt;，此时上面的查询会使用这个索引。除此之外，还可以使用一种称之为“隔离列”的优化方法，将emp_no与from_date之间的“坑”填上。首先我们看下title一共有几种不同的值：MySQLSELECT DISTINCT(title) FROM employees.titles;+--------------------+| title |+--------------------+| Senior Engineer || Staff || Engineer || Senior Staff || Assistant Engineer || Technique Leader || Manager |+--------------------+123456789101112SELECT DISTINCT(title) FROM employees.titles;+--------------------+| title |+--------------------+| Senior Engineer || Staff || Engineer || Senior Staff || Assistant Engineer || Technique Leader || Manager |+--------------------+只有7种。在这种成为“坑”的列值比较少的情况下，可以考虑用“IN”来填补这个“坑”从而形成最左前缀：MySQLEXPLAIN SELECT * FROM employees.titlesWHERE emp_no=&apos;10001&apos;AND title IN (&apos;Senior Engineer&apos;, &apos;Staff&apos;, &apos;Engineer&apos;, &apos;Senior Staff&apos;, &apos;Assistant Engineer&apos;, &apos;Technique Leader&apos;, &apos;Manager&apos;)AND from_date=&apos;1986-06-26&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 59 | NULL | 7 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+123456789EXPLAIN SELECT * FROM employees.titlesWHERE emp_no=&apos;10001&apos;AND title IN (&apos;Senior Engineer&apos;, &apos;Staff&apos;, &apos;Engineer&apos;, &apos;Senior Staff&apos;, &apos;Assistant Engineer&apos;, &apos;Technique Leader&apos;, &apos;Manager&apos;)AND from_date=&apos;1986-06-26&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 59 | NULL | 7 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+这次key_len为59，说明索引被用全了，但是从type和rows看出IN实际上执行了一个range查询，这里检查了7个key。看下两种查询的性能比较：MySQLSHOW PROFILES;+----------+------------+-------------------------------------------------------------------------------+| Query_ID | Duration | Query |+----------+------------+-------------------------------------------------------------------------------+| 10 | 0.00058000 | SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND from_date=&apos;1986-06-26&apos;|| 11 | 0.00052500 | SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND title IN ... |+----------+------------+-------------------------------------------------------------------------------+1234567SHOW PROFILES;+----------+------------+-------------------------------------------------------------------------------+| Query_ID | Duration | Query |+----------+------------+-------------------------------------------------------------------------------+| 10 | 0.00058000 | SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND from_date=&apos;1986-06-26&apos;|| 11 | 0.00052500 | SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND title IN ... |+----------+------------+-------------------------------------------------------------------------------+“填坑”后性能提升了一点。如果经过emp_no筛选后余下很多数据，则后者性能优势会更加明显。当然，如果title的值很多，用填坑就不合适了，必须建立辅助索引。情况四：查询条件没有指定索引第一列。MySQLEXPLAIN SELECT * FROM employees.titles WHERE from_date=&apos;1986-06-26&apos;;+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+| 1 | SIMPLE | titles | ALL | NULL | NULL | NULL | NULL | 443308 | Using where |+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+123456EXPLAIN SELECT * FROM employees.titles WHERE from_date=&apos;1986-06-26&apos;;+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+| 1 | SIMPLE | titles | ALL | NULL | NULL | NULL | NULL | 443308 | Using where |+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+由于不是最左前缀，索引这样的查询显然用不到索引。情况五：匹配某列的前缀字符串。MySQLEXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND title LIKE &apos;Senior%&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 56 | NULL | 1 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND title LIKE &apos;Senior%&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 56 | NULL | 1 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+此时可以用到索引，但是如果通配符不是只出现在末尾，则无法使用索引。（原文表述有误，如果通配符%不出现在开头，则可以用到索引，但根据具体情况不同可能只会用其中一个前缀）情况六：范围查询。MySQLEXPLAIN SELECT * FROM employees.titles WHERE emp_no &lt; &apos;10010&apos; and title=&apos;Senior Engineer&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 4 | NULL | 16 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no &lt; &apos;10010&apos; and title=&apos;Senior Engineer&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 4 | NULL | 16 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+范围列可以用到索引（必须是最左前缀），但是范围列后面的列无法用到索引。同时，索引最多用于一个范围列，因此如果查询条件中有两个范围列则无法全用到索引。MySQLEXPLAIN SELECT * FROM employees.titlesWHERE emp_no &lt; 10010&apos;AND title=&apos;Senior Engineer&apos;AND from_date BETWEEN &apos;1986-01-01&apos; AND &apos;1986-12-31&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 4 | NULL | 16 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+123456789EXPLAIN SELECT * FROM employees.titlesWHERE emp_no &lt; 10010&apos;AND title=&apos;Senior Engineer&apos;AND from_date BETWEEN &apos;1986-01-01&apos; AND &apos;1986-12-31&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 4 | NULL | 16 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+可以看到索引对第二个范围索引无能为力。这里特别要说明MySQL一个有意思的地方，那就是仅用explain可能无法区分范围索引和多值匹配，因为在type中这两者都显示为range。同时，用了“between”并不意味着就是范围查询，例如下面的查询：MySQLEXPLAIN SELECT * FROM employees.titlesWHERE emp_no BETWEEN &apos;10001&apos; AND &apos;10010&apos;AND title=&apos;Senior Engineer&apos;AND from_date BETWEEN &apos;1986-01-01&apos; AND &apos;1986-12-31&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 59 | NULL | 16 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+123456789EXPLAIN SELECT * FROM employees.titlesWHERE emp_no BETWEEN &apos;10001&apos; AND &apos;10010&apos;AND title=&apos;Senior Engineer&apos;AND from_date BETWEEN &apos;1986-01-01&apos; AND &apos;1986-12-31&apos;;+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 59 | NULL | 16 | Using where |+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+看起来是用了两个范围查询，但作用于emp_no上的“BETWEEN”实际上相当于“IN”，也就是说emp_no实际是多值精确匹配。可以看到这个查询用到了索引全部三个列。因此在MySQL中要谨慎地区分多值匹配和范围匹配，否则会对MySQL的行为产生困惑。情况七：查询条件中含有函数或表达式。很不幸，如果查询条件中含有函数或表达式，则MySQL不会为这列使用索引（虽然某些在数学意义上可以使用）。例如：MySQLEXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND left(title, 6)=&apos;Senior&apos;;+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+| 1 | SIMPLE | titles | ref | PRIMARY | PRIMARY | 4 | const | 1 | Using where |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no=&apos;10001&apos; AND left(title, 6)=&apos;Senior&apos;;+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+| 1 | SIMPLE | titles | ref | PRIMARY | PRIMARY | 4 | const | 1 | Using where |+----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+虽然这个查询和情况五中功能相同，但是由于使用了函数left，则无法为title列应用索引，而情况五中用LIKE则可以。再如：MySQLEXPLAIN SELECT * FROM employees.titles WHERE emp_no - 1=&apos;10000&apos;;+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+| 1 | SIMPLE | titles | ALL | NULL | NULL | NULL | NULL | 443308 | Using where |+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+123456EXPLAIN SELECT * FROM employees.titles WHERE emp_no - 1=&apos;10000&apos;;+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+| 1 | SIMPLE | titles | ALL | NULL | NULL | NULL | NULL | 443308 | Using where |+----+-------------+--------+------+---------------+------+---------+------+--------+-------------+显然这个查询等价于查询emp_no为10001的函数，但是由于查询条件是一个表达式，MySQL无法为其使用索引。看来MySQL还没有智能到自动优化常量表达式的程度，因此在写查询语句时尽量避免表达式出现在查询中，而是先手工私下代数运算，转换为无表达式的查询语句。索引选择性与前缀索引既然索引可以加快查询速度，那么是不是只要是查询语句需要，就建上索引？答案是否定的。因为索引虽然加快了查询速度，但索引也是有代价的：索引文件本身要消耗存储空间，同时索引会加重插入、删除和修改记录时的负担，另外，MySQL在运行时也要消耗资源维护索引，因此索引并不是越多越好。一般两种情况下不建议建索引。第一种情况是表记录比较少，例如一两千条甚至只有几百条记录的表，没必要建索引，让查询做全表扫描就好了。至于多少条记录才算多，这个个人有个人的看法，我个人的经验是以2000作为分界线，记录数不超过 2000可以考虑不建索引，超过2000条可以酌情考虑索引。另一种不建议建索引的情况是索引的选择性较低。所谓索引的选择性（Selectivity），是指不重复的索引值（也叫基数，Cardinality）与表记录数（#T）的比值：Index Selectivity = Cardinality / #T显然选择性的取值范围为(0, 1]，选择性越高的索引价值越大，这是由B+Tree的性质决定的。例如，上文用到的employees.titles表，如果title字段经常被单独查询，是否需要建索引，我们看一下它的选择性：MySQLSELECT count(DISTINCT(title))/count(*) AS Selectivity FROM employees.titles;+-------------+| Selectivity |+-------------+| 0.0000 |+-------------+123456SELECT count(DISTINCT(title))/count(*) AS Selectivity FROM employees.titles;+-------------+| Selectivity |+-------------+| 0.0000 |+-------------+title的选择性不足0.0001（精确值为0.00001579），所以实在没有什么必要为其单独建索引。有一种与索引选择性有关的索引优化策略叫做前缀索引，就是用列的前缀代替整个列作为索引key，当前缀长度合适时，可以做到既使得前缀索引的选择性接近全列索引，同时因为索引key变短而减少了索引文件的大小和维护开销。下面以employees.employees表为例介绍前缀索引的选择和使用。从图12可以看到employees表只有一个索引&lt;emp_no&gt;，那么如果我们想按名字搜索一个人，就只能全表扫描了：MySQLEXPLAIN SELECT * FROM employees.employees WHERE first_name=&apos;Eric&apos; AND last_name=&apos;Anido&apos;;+----+-------------+-----------+------+---------------+------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+--------+-------------+| 1 | SIMPLE | employees | ALL | NULL | NULL | NULL | NULL | 300024 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+--------+-------------+123456EXPLAIN SELECT * FROM employees.employees WHERE first_name=&apos;Eric&apos; AND last_name=&apos;Anido&apos;;+----+-------------+-----------+------+---------------+------+---------+------+--------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+--------+-------------+| 1 | SIMPLE | employees | ALL | NULL | NULL | NULL | NULL | 300024 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+--------+-------------+如果频繁按名字搜索员工，这样显然效率很低，因此我们可以考虑建索引。有两种选择，建&lt;first_name&gt;或&lt;first_name, last_name&gt;，看下两个索引的选择性：MySQLSELECT count(DISTINCT(first_name))/count(*) AS Selectivity FROM employees.employees;+-------------+| Selectivity |+-------------+| 0.0042 |+-------------+SELECT count(DISTINCT(concat(first_name, last_name)))/count(*) AS Selectivity FROM employees.employees;+-------------+| Selectivity |+-------------+| 0.9313 |+-------------+12345678910111213SELECT count(DISTINCT(first_name))/count(*) AS Selectivity FROM employees.employees;+-------------+| Selectivity |+-------------+| 0.0042 |+-------------+SELECT count(DISTINCT(concat(first_name, last_name)))/count(*) AS Selectivity FROM employees.employees;+-------------+| Selectivity |+-------------+| 0.9313 |+-------------+&lt;first_name&gt;显然选择性太低，&lt;first_name, last_name&gt;选择性很好，但是first_name和last_name加起来长度为30，有没有兼顾长度和选择性的办法？可以考虑用first_name和last_name的前几个字符建立索引，例如&lt;first_name, left(last_name, 3)&gt;，看看其选择性： SELECT count(DISTINCT(concat(first_name, left(last_name, 3))))/count(*) AS Selectivity FROM employees.employees;+————-+| Selectivity |+————-+| 0.7879 |+————-+1选择性还不错，但离0.9313还是有点距离，那么把last_name前缀加到4： SELECT count(DISTINCT(concat(first_name, left(last_name, 4))))/count(*) AS Selectivity FROM employees.employees;+————-+| Selectivity |+————-+| 0.9007 |+————-+ SELECT count(DISTINCT(concat(first_name, left(last_name, 4))))/count(*) AS Selectivity FROM employees.employees;+————-+| Selectivity |+————-+| 0.9007 |+————-+1这时选择性已经很理想了，而这个索引的长度只有18，比&lt;first_name, last_name&gt;短了接近一半，我们把这个前缀索引 建上： ALTER TABLE employees.employeesADD INDEX first_name_last_name4 (first_name, last_name(4));1此时再执行一遍按名字查询，比较分析一下与建索引前的结果： SHOW PROFILES;+———-+————+———————————————————————————+| Query_ID | Duration | Query |+———-+————+———————————————————————————+| 87 | 0.11941700 | SELECT FROM employees.employees WHERE first_name=’Eric’ AND last_name=’Anido’ || 90 | 0.00092400 | SELECT FROM employees.employees WHERE first_name=’Eric’ AND last_name=’Anido’ |+———-+————+———————————————————————————+```性能的提升是显著的，查询速度提高了120多倍。 前缀索引兼顾索引大小和查询速度，但是其缺点是不能用于ORDER BY和GROUP BY操作，也不能用于Covering index（即当索引本身包含查询所需全部数据时，不再访问数据文件本身）。 InnoDB的主键选择与插入优化在使用InnoDB存储引擎时，如果没有特别的需要，请永远使用一个与业务无关的自增字段作为主键。 经常看到有帖子或博客讨论主键选择问题，有人建议使用业务无关的自增主键，有人觉得没有必要，完全可以使用如学号或身份证号这种唯一字段作为主键。不论支持哪种论点，大多数论据都是业务层面的。如果从数据库索引优化角度看，使用InnoDB引擎而不使用自增主键绝对是一个糟糕的主意。 上文讨论过InnoDB的索引实现，InnoDB使用聚集索引，数据记录本身被存于主索引（一颗B+Tree）的叶子节点上。这就要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放，因此每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点）。 如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。如下图所示： 这样就会形成一个紧凑的索引结构，近似顺序填满。由于每次插入时也不需要移动已有数据，因此效率很高，也不会增加很多开销在维护索引上。 如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置： 此时MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。 因此，只要可以，请尽量在InnoDB上采用自增字段做主键。 后记这篇文章断断续续写了半个月，主要内容就是上面这些了。不可否认，这篇文章在一定程度上有纸上谈兵之嫌，因为我本人对MySQL的使用属于菜鸟级别，更没有太多数据库调优的经验，在这里大谈数据库索引调优有点大言不惭。就当是我个人的一篇学习笔记了。 其实数据库索引调优是一项技术活，不能仅仅靠理论，因为实际情况千变万化，而且MySQL本身存在很复杂的机制，如查询优化策略和各种引擎的实现差异等都会使情况变得更加复杂。但同时这些理论是索引调优的基础，只有在明白理论的基础上，才能对调优策略进行合理推断并了解其背后的机制，然后结合实践中不断的实验和摸索，从而真正达到高效使用MySQL索引的目的。 另外，MySQL索引及其优化涵盖范围非常广，本文只是涉及到其中一部分。如与排序（ORDER BY）相关的索引优化及覆盖索引（Covering index）的话题本文并未涉及，同时除B-Tree索引外MySQL还根据不同引擎支持的哈希索引、全文索引等等本文也并未涉及。如果有机会，希望再对本文未涉及的部分进行补充吧。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解Java内存模型（七）——总结]]></title>
    <url>%2F2013%2F03%2F15%2F2013-03-15-java-memory-model-7%2F</url>
    <content type="text"><![CDATA[原文链接: http://www.infoq.com/cn/articles/java-memory-model-7 处理器内存模型顺序一致性内存模型是一个理论参考模型，JMM和处理器内存模型在设计时通常会把顺序一致性内存模型作为参照。JMM和处理器内存模型在设计时会对顺序一致性模型做一些放松，因为如果完全按照顺序一致性模型来实现处理器和JMM，那么很多的处理器和编译器优化都要被禁止，这对执行性能将会有很大的影响。 根据对不同类型读/写操作组合的执行顺序的放松，可以把常见处理器的内存模型划分为下面几种类型： 放松程序中写-读操作的顺序，由此产生了total store ordering内存模型（简称为TSO）。 在前面1的基础上，继续放松程序中写-写操作的顺序，由此产生了partial store order 内存模型（简称为PSO）。 在前面1和2的基础上，继续放松程序中读-写和读-读操作的顺序，由此产生了relaxed memory order内存模型（简称为RMO）和PowerPC内存模型。 注意，这里处理器对读/写操作的放松，是以两个操作之间不存在数据依赖性为前提的（因为处理器要遵守as-if-serial语义，处理器不会对存在数据依赖性的两个内存操作做重排序）。 内存模型名称 对应的处理器 Store-Load 重排序 Store-Store重排序 Load-Load 和Load-Store重排序 可以更早读取到其它处理器的写 可以更早读取到当前处理器的写 TSO sparc-TSO X64 Y Y PSO sparc-PSO Y Y Y RMO ia64 Y Y Y Y PowerPC PowerPC Y Y Y Y Y 在这个表格中，我们可以看到所有处理器内存模型都允许写-读重排序，原因在第一章以说明过：它们都使用了写缓存区，写缓存区可能导致写-读操作重排序。同时，我们可以看到这些处理器内存模型都允许更早读到当前处理器的写，原因同样是因为写缓存区：由于写缓存区仅对当前处理器可见，这个特性导致当前处理器可以比其他处理器先看到临时保存在自己的写缓存区中的写。 上面表格中的各种处理器内存模型，从上到下，模型由强变弱。越是追求性能的处理器，内存模型设计的会越弱。因为这些处理器希望内存模型对它们的束缚越少越好，这样它们就可以做尽可能多的优化来提高性能。 由于常见的处理器内存模型比JMM要弱，java编译器在生成字节码时，会在执行指令序列的适当位置插入内存屏障来限制处理器的重排序。同时，由于各种处理器内存模型的强弱并不相同，为了在不同的处理器平台向程序员展示一个一致的内存模型，JMM在不同的处理器中需要插入的内存屏障的数量和种类也不相同。下图展示了JMM在不同处理器内存模型中需要插入的内存屏障的示意图： 如上图所示，JMM屏蔽了不同处理器内存模型的差异，它在不同的处理器平台之上为java程序员呈现了一个一致的内存模型。 JMM，处理器内存模型与顺序一致性内存模型之间的关系JMM是一个语言级的内存模型，处理器内存模型是硬件级的内存模型，顺序一致性内存模型是一个理论参考模型。下面是语言内存模型，处理器内存模型和顺序一致性内存模型的强弱对比示意图： 从上图我们可以看出：常见的4种处理器内存模型比常用的3中语言内存模型要弱，处理器内存模型和语言内存模型都比顺序一致性内存模型要弱。同处理器内存模型一样，越是追求执行性能的语言，内存模型设计的会越弱。 JMM的设计从JMM设计者的角度来说，在设计JMM时，需要考虑两个关键因素： 程序员对内存模型的使用。程序员希望内存模型易于理解，易于编程。程序员希望基于一个强内存模型来编写代码。 编译器和处理器对内存模型的实现。编译器和处理器希望内存模型对它们的束缚越少越好，这样它们就可以做尽可能多的优化来提高性能。编译器和处理器希望实现一个弱内存模型。 由于这两个因素互相矛盾，所以JSR-133专家组在设计JMM时的核心目标就是找到一个好的平衡点：一方面要为程序员提供足够强的内存可见性保证；另一方面，对编译器和处理器的限制要尽可能的放松。下面让我们看看JSR-133是如何实现这一目标的。 为了具体说明，请看前面提到过的计算圆面积的示例代码：123double pi = 3.14; //Adouble r = 1.0; //Bdouble area = pi * r * r; //C 上面计算圆的面积的示例代码存在三个happens- before关系： A happens- before B； B happens- before C； A happens- before C； 由于A happens- before B，happens- before的定义会要求：A操作执行的结果要对B可见，且A操作的执行顺序排在B操作之前。 但是从程序语义的角度来说，对A和B做重排序即不会改变程序的执行结果，也还能提高程序的执行性能（允许这种重排序减少了对编译器和处理器优化的束缚）。也就是说，上面这3个happens- before关系中，虽然2和3是必需要的，但1是不必要的。因此，JMM把happens- before要求禁止的重排序分为了下面两类： 会改变程序执行结果的重排序。 不会改变程序执行结果的重排序。 JMM对这两种不同性质的重排序，采取了不同的策略： 对于会改变程序执行结果的重排序，JMM要求编译器和处理器必须禁止这种重排序。 对于不会改变程序执行结果的重排序，JMM对编译器和处理器不作要求（JMM允许这种重排序）。下面是JMM的设计示意图： 从上图可以看出两点： JMM向程序员提供的happens- before规则能满足程序员的需求。JMM的happens- before规则不但简单易懂，而且也向程序员提供了足够强的内存可见性保证（有些内存可见性保证其实并不一定真实存在，比如上面的A happens- before B）。 JMM对编译器和处理器的束缚已经尽可能的少。从上面的分析我们可以看出，JMM其实是在遵循一个基本原则：只要不改变程序的执行结果（指的是单线程程序和正确同步的多线程程序），编译器和处理器怎么优化都行。比如，如果编译器经过细致的分析后，认定一个锁只会被单个线程访问，那么这个锁可以被消除。再比如，如果编译器经过细致的分析后，认定一个volatile变量仅仅只会被单个线程访问，那么编译器可以把这个volatile变量当作一个普通变量来对待。这些优化既不会改变程序的执行结果，又能提高程序的执行效率。 JMM的内存可见性保证Java程序的内存可见性保证按程序类型可以分为下列三类： 单线程程序。单线程程序不会出现内存可见性问题。编译器，runtime和处理器会共同确保单线程程序的执行结果与该程序在顺序一致性模型中的执行结果相同。 正确同步的多线程程序。正确同步的多线程程序的执行将具有顺序一致性（程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同）。这是JMM关注的重点，JMM通过限制编译器和处理器的重排序来为程序员提供内存可见性保证。 未同步/未正确同步的多线程程序。JMM为它们提供了最小安全性保障：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值（0，null，false）。 下图展示了这三类程序在JMM中与在顺序一致性内存模型中的执行结果的异同：]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>java语言</tag>
        <tag>内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[入理解Java内存模型（六）——final]]></title>
    <url>%2F2013%2F03%2F08%2F2013-03-08-java-memory-model-6%2F</url>
    <content type="text"><![CDATA[原文链接： http://www.infoq.com/cn/articles/java-memory-model-6 与前面介绍的锁和volatile相比较，对final域的读和写更像是普通的变量访问。对于final域，编译器和处理器要遵守两个重排序规则： 在构造函数内对一个final域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 初次读一个包含final域的对象的引用，与随后初次读这个final域，这两个操作之间不能重排序。 下面，我们通过一些示例性的代码来分别说明这两个规则：1234567891011121314151617181920public class FinalExample &#123; int i; //普通变量 final int j; //final变量 static FinalExample obj; public FinalExample () &#123; //构造函数 i = 1; //写普通域 j = 2; //写final域 &#125; public static void writer () &#123; //写线程A执行 obj = new FinalExample (); &#125; public static void reader () &#123; //读线程B执行 FinalExample object = obj; //读对象引用 int a = object.i; //读普通域 int b = object.j; //读final域 &#125;&#125; 这里假设一个线程A执行writer ()方法，随后另一个线程B执行reader ()方法。下面我们通过这两个线程的交互来说明这两个规则。 写final域的重排序规则写final域的重排序规则禁止把final域的写重排序到构造函数之外。这个规则的实现包含下面2个方面： JMM禁止编译器把final域的写重排序到构造函数之外。 编译器会在final域的写之后，构造函数return之前，插入一个StoreStore屏障。这个屏障禁止处理器把final域的写重排序到构造函数之外。 现在让我们分析writer ()方法。writer ()方法只包含一行代码：finalExample = new FinalExample ()。这行代码包含两个步骤： 构造一个FinalExample类型的对象； 把这个对象的引用赋值给引用变量obj。 假设线程B读对象引用与读对象的成员域之间没有重排序（马上会说明为什么需要这个假设），下图是一种可能的执行时序： 在上图中，写普通域的操作被编译器重排序到了构造函数之外，读线程B错误的读取了普通变量i初始化之前的值。而写final域的操作，被写final域的重排序规则“限定”在了构造函数之内，读线程B正确的读取了final变量初始化之后的值。 写final域的重排序规则可以确保：在对象引用为任意线程可见之前，对象的final域已经被正确初始化过了，而普通域不具有这个保障。以上图为例，在读线程B“看到”对象引用obj时，很可能obj对象还没有构造完成（对普通域i的写操作被重排序到构造函数外，此时初始值2还没有写入普通域i）。 读final域的重排序规则读final域的重排序规则如下： 在一个线程中，初次读对象引用与初次读该对象包含的final域，JMM禁止处理器重排序这两个操作（注意，这个规则仅仅针对处理器）。编译器会在读final域操作的前面插入一个LoadLoad屏障。 初次读对象引用与初次读该对象包含的final域，这两个操作之间存在间接依赖关系。由于编译器遵守间接依赖关系，因此编译器不会重排序这两个操作。大多数处理器也会遵守间接依赖，大多数处理器也不会重排序这两个操作。但有少数处理器允许对存在间接依赖关系的操作做重排序（比如alpha处理器），这个规则就是专门用来针对这种处理器。 reader()方法包含三个操作： 初次读引用变量obj; 初次读引用变量obj指向对象的普通域j。 初次读引用变量obj指向对象的final域i。 现在我们假设写线程A没有发生任何重排序，同时程序在不遵守间接依赖的处理器上执行，下面是一种可能的执行时序： 在上图中，读对象的普通域的操作被处理器重排序到读对象引用之前。读普通域时，该域还没有被写线程A写入，这是一个错误的读取操作。而读final域的重排序规则会把读对象final域的操作“限定”在读对象引用之后，此时该final域已经被A线程初始化过了，这是一个正确的读取操作。 读final域的重排序规则可以确保：在读一个对象的final域之前，一定会先读包含这个final域的对象的引用。在这个示例程序中，如果该引用不为null，那么引用对象的final域一定已经被A线程初始化过了。 如果final域是引用类型上面我们看到的final域是基础数据类型，下面让我们看看如果final域是引用类型，将会有什么效果？ 请看下列示例代码：1234567891011121314151617181920212223public class FinalReferenceExample &#123;final int[] intArray; //final是引用类型static FinalReferenceExample obj;public FinalReferenceExample () &#123; //构造函数 intArray = new int[1]; //1 intArray[0] = 1; //2&#125;public static void writerOne () &#123; //写线程A执行 obj = new FinalReferenceExample (); //3&#125;public static void writerTwo () &#123; //写线程B执行 obj.intArray[0] = 2; //4&#125;public static void reader () &#123; //读线程C执行 if (obj != null) &#123; //5 int temp1 = obj.intArray[0]; //6 &#125;&#125;&#125; 这里final域为一个引用类型，它引用一个int型的数组对象。对于引用类型，写final域的重排序规则对编译器和处理器增加了如下约束： 在构造函数内对一个final引用的对象的成员域的写入，与随后在构造函数外把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 对上面的示例程序，我们假设首先线程A执行writerOne()方法，执行完后线程B执行writerTwo()方法，执行完后线程C执行reader ()方法。下面是一种可能的线程执行时序： 在上图中，1是对final域的写入，2是对这个final域引用的对象的成员域的写入，3是把被构造的对象的引用赋值给某个引用变量。这里除了前面提到的1不能和3重排序外，2和3也不能重排序。 JMM可以确保读线程C至少能看到写线程A在构造函数中对final引用对象的成员域的写入。即C至少能看到数组下标0的值为1。而写线程B对数组元素的写入，读线程C可能看的到，也可能看不到。JMM不保证线程B的写入对读线程C可见，因为写线程B和读线程C之间存在数据竞争，此时的执行结果不可预知。 如果想要确保读线程C看到写线程B对数组元素的写入，写线程B和读线程C之间需要使用同步原语（lock或volatile）来确保内存可见性。 为什么final引用不能从构造函数内“逸出”前面我们提到过，写final域的重排序规则可以确保：在引用变量为任意线程可见之前，该引用变量指向的对象的final域已经在构造函数中被正确初始化过了。其实要得到这个效果，还需要一个保证：在构造函数内部，不能让这个被构造对象的引用为其他线程可见，也就是对象引用不能在构造函数中“逸出”。为了说明问题，让我们来看下面示例代码：12345678910111213141516171819public class FinalReferenceEscapeExample &#123;final int i;static FinalReferenceEscapeExample obj;public FinalReferenceEscapeExample () &#123; i = 1; //1写final域 obj = this; //2 this引用在此“逸出”&#125;public static void writer() &#123; new FinalReferenceEscapeExample ();&#125;public static void reader &#123; if (obj != null) &#123; //3 int temp = obj.i; //4 &#125;&#125;&#125; 假设一个线程A执行writer()方法，另一个线程B执行reader()方法。这里的操作2使得对象还未完成构造前就为线程B可见。即使这里的操作2是构造函数的最后一步，且即使在程序中操作2排在操作1后面，执行read()方法的线程仍然可能无法看到final域被初始化后的值，因为这里的操作1和操作2之间可能被重排序。实际的执行时序可能如下图所示： 从上图我们可以看出：在构造函数返回前，被构造对象的引用不能为其他线程可见，因为此时的final域可能还没有被初始化。在构造函数返回后，任意线程都将保证能看到final域正确初始化之后的值。 final语义在处理器中的实现现在我们以x86处理器为例，说明final语义在处理器中的具体实现。 上面我们提到，写final域的重排序规则会要求译编器在final域的写之后，构造函数return之前，插入一个StoreStore障屏。读final域的重排序规则要求编译器在读final域的操作前面插入一个LoadLoad屏障。 由于x86处理器不会对写-写操作做重排序，所以在x86处理器中，写final域需要的StoreStore障屏会被省略掉。同样，由于x86处理器不会对存在间接依赖关系的操作做重排序，所以在x86处理器中，读final域需要的LoadLoad屏障也会被省略掉。也就是说在x86处理器中，final域的读/写不会插入任何内存屏障！ JSR-133为什么要增强final的语义在旧的Java内存模型中 ，最严重的一个缺陷就是线程可能看到final域的值会改变。比如，一个线程当前看到一个整形final域的值为0（还未初始化之前的默认值），过一段时间之后这个线程再去读这个final域的值时，却发现值变为了1（被某个线程初始化之后的值）。最常见的例子就是在旧的Java内存模型中，String的值可能会改变（参考文献2中有一个具体的例子，感兴趣的读者可以自行参考，这里就不赘述了）。 为了修补这个漏洞，JSR-133专家组增强了final的语义。通过为final域增加写和读重排序规则，可以为java程序员提供初始化安全保证：只要对象是正确构造的（被构造对象的引用在构造函数中没有“逸出”），那么不需要使用同步（指lock和volatile的使用），就可以保证任意线程都能看到这个final域在构造函数中被初始化之后的值。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>java语言</tag>
        <tag>内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解Java内存模型（五）——锁]]></title>
    <url>%2F2013%2F03%2F05%2F2013-03-05-java-memory-model-5%2F</url>
    <content type="text"><![CDATA[原文链接： http://www.infoq.com/cn/articles/java-memory-model-5 锁的释放-获取建立的happens before 关系锁是java并发编程中最重要的同步机制。锁除了让临界区互斥执行外，还可以让释放锁的线程向获取同一个锁的线程发送消息。 下面是锁释放-获取的示例代码：123456789101112class MonitorExample &#123; int a = 0; public synchronized void writer() &#123; //1 a++; //2 &#125; //3 public synchronized void reader() &#123; //4 int i = a; //5 …… &#125; //6&#125; 假设线程A执行writer()方法，随后线程B执行reader()方法。根据happens before规则，这个过程包含的happens before 关系可以分为两类： 根据程序次序规则，1 happens before 2, 2 happens before 3; 4 happens before 5, 5 happens before 6。 根据监视器锁规则，3 happens before 4。 根据happens before 的传递性，2 happens before 5。 上述happens before 关系的图形化表现形式如下： 在上图中，每一个箭头链接的两个节点，代表了一个happens before 关系。黑色箭头表示程序顺序规则；橙色箭头表示监视器锁规则；蓝色箭头表示组合这些规则后提供的happens before保证。 上图表示在线程A释放了锁之后，随后线程B获取同一个锁。在上图中，2 happens before 5。因此，线程A在释放锁之前所有可见的共享变量，在线程B获取同一个锁之后，将立刻变得对B线程可见。 锁释放和获取的内存语义当线程释放锁时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存中。以上面的MonitorExample程序为例，A线程释放锁后，共享数据的状态示意图如下： 当线程获取锁时，JMM会把该线程对应的本地内存置为无效。从而使得被监视器保护的临界区代码必须要从主内存中去读取共享变量。下面是锁获取的状态示意图： 对比锁释放-获取的内存语义与volatile写-读的内存语义，可以看出：锁释放与volatile写有相同的内存语义；锁获取与volatile读有相同的内存语义。 下面对锁释放和锁获取的内存语义做个总结： 线程A释放一个锁，实质上是线程A向接下来将要获取这个锁的某个线程发出了（线程A对共享变量所做修改的）消息。 线程B获取一个锁，实质上是线程B接收了之前某个线程发出的（在释放这个锁之前对共享变量所做修改的）消息。 线程A释放锁，随后线程B获取这个锁，这个过程实质上是线程A通过主内存向线程B发送消息。 锁内存语义的实现本文将借助ReentrantLock的源代码，来分析锁内存语义的具体实现机制。 请看下面的示例代码：1234567891011121314151617181920212223class ReentrantLockExample &#123;int a = 0;ReentrantLock lock = new ReentrantLock();public void writer() &#123; lock.lock(); //获取锁 try &#123; a++; &#125; finally &#123; lock.unlock(); //释放锁 &#125;&#125;public void reader () &#123; lock.lock(); //获取锁 try &#123; int i = a; …… &#125; finally &#123; lock.unlock(); //释放锁 &#125; &#125;&#125; 在ReentrantLock中，调用lock()方法获取锁；调用unlock()方法释放锁。 ReentrantLock的实现依赖于java同步器框架AbstractQueuedSynchronizer（本文简称之为AQS）。AQS使用一个整型的volatile变量（命名为state）来维护同步状态，马上我们会看到，这个volatile变量是ReentrantLock内存语义实现的关键。 下面是ReentrantLock的类图（仅画出与本文相关的部分）： ReentrantLock分为公平锁和非公平锁，我们首先分析公平锁。 使用公平锁时，加锁方法lock()的方法调用轨迹如下： ReentrantLock : lock() FairSync : lock() AbstractQueuedSynchronizer : acquire(int arg) ReentrantLock : tryAcquire(int acquires)在第4步真正开始加锁，下面是该方法的源代码：12345678910111213141516171819protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); //获取锁的开始，首先读volatile变量state if (c == 0) &#123; if (isFirst(current) &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false;&#125; 从上面源代码中我们可以看出，加锁方法首先读volatile变量state。 在使用公平锁时，解锁方法unlock()的方法调用轨迹如下： ReentrantLock : unlock() AbstractQueuedSynchronizer : release(int arg) Sync : tryRelease(int releases) 在第3步真正开始释放锁，下面是该方法的源代码：123456789101112protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); //释放锁的最后，写volatile变量state return free;&#125; 从上面的源代码我们可以看出，在释放锁的最后写volatile变量state。 公平锁在释放锁的最后写volatile变量state；在获取锁时首先读这个volatile变量。根据volatile的happens-before规则，释放锁的线程在写volatile变量之前可见的共享变量，在获取锁的线程读取同一个volatile变量后将立即变的对获取锁的线程可见。 现在我们分析非公平锁的内存语义的实现。 非公平锁的释放和公平锁完全一样，所以这里仅仅分析非公平锁的获取。 使用非公平锁时，加锁方法lock()的方法调用轨迹如下： ReentrantLock : lock() NonfairSync : lock() AbstractQueuedSynchronizer : compareAndSetState(int expect, int update) 在第3步真正开始加锁，下面是该方法的源代码：123protected final boolean compareAndSetState(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, stateOffset, expect, update);&#125; 该方法以原子操作的方式更新state变量，本文把java的compareAndSet()方法调用简称为CAS。JDK文档对该方法的说明如下：如果当前状态值等于预期值，则以原子方式将同步状态设置为给定的更新值。此操作具有 volatile 读和写的内存语义。 这里我们分别从编译器和处理器的角度来分析,CAS如何同时具有volatile读和volatile写的内存语义。 前文我们提到过，编译器不会对volatile读与volatile读后面的任意内存操作重排序；编译器不会对volatile写与volatile写前面的任意内存操作重排序。组合这两个条件，意味着为了同时实现volatile读和volatile写的内存语义，编译器不能对CAS与CAS前面和后面的任意内存操作重排序。 下面我们来分析在常见的intel x86处理器中，CAS是如何同时具有volatile读和volatile写的内存语义的。 下面是sun.misc.Unsafe类的compareAndSwapInt()方法的源代码：123public final native boolean compareAndSwapInt(Object o, long offset, int expected, int x); 可以看到这是个本地方法调用。这个本地方法在openjdk中依次调用的c++代码为：unsafe.cpp，atomic.cpp和atomicwindowsx86.inline.hpp。这个本地方法的最终实现在openjdk的如下位置：openjdk-7-fcs-src-b147-27jun2011\openjdk\hotspot\src\oscpu\windowsx86\vm\ atomicwindowsx86.inline.hpp（对应于windows操作系统，X86处理器）。下面是对应于intel x86处理器的源代码的片段：1234567891011121314151617181920// Adding a lock prefix to an instruction on MP machine// VC++ doesn't like the lock prefix to be on a single line// so we can't insert a label after the lock prefix.// By emitting a lock prefix, we can define a label after it.#define LOCK_IF_MP(mp) __asm cmp mp, 0 \ __asm je L0 \ __asm _emit 0xF0 \ __asm L0:inline jint Atomic::cmpxchg (jint exchange_value, volatile jint* dest, jint compare_value) &#123; // alternative for InterlockedCompareExchange int mp = os::is_MP(); __asm &#123; mov edx, dest mov ecx, exchange_value mov eax, compare_value LOCK_IF_MP(mp) cmpxchg dword ptr [edx], ecx &#125;&#125; 如上面源代码所示，程序会根据当前处理器的类型来决定是否为cmpxchg指令添加lock前缀。如果程序是在多处理器上运行，就为cmpxchg指令加上lock前缀（lock cmpxchg）。反之，如果程序是在单处理器上运行，就省略lock前缀（单处理器自身会维护单处理器内的顺序一致性，不需要lock前缀提供的内存屏障效果）。 intel的手册对lock前缀的说明如下： 确保对内存的读-改-写操作原子执行。在Pentium及Pentium之前的处理器中，带有lock前缀的指令在执行期间会锁住总线，使得其他处理器暂时无法通过总线访问内存。很显然，这会带来昂贵的开销。从Pentium 4，Intel Xeon及P6处理器开始，intel在原有总线锁的基础上做了一个很有意义的优化：如果要访问的内存区域（area of memory）在lock前缀指令执行期间已经在处理器内部的缓存中被锁定（即包含该内存区域的缓存行当前处于独占或以修改状态），并且该内存区域被完全包含在单个缓存行（cache line）中，那么处理器将直接执行该指令。由于在指令执行期间该缓存行会一直被锁定，其它处理器无法读/写该指令要访问的内存区域，因此能保证指令执行的原子性。这个操作过程叫做缓存锁定（cache locking），缓存锁定将大大降低lock前缀指令的执行开销，但是当多处理器之间的竞争程度很高或者指令访问的内存地址未对齐时，仍然会锁住总线。 禁止该指令与之前和之后的读和写指令重排序。 把写缓冲区中的所有数据刷新到内存中 上面的第2点和第3点所具有的内存屏障效果，足以同时实现volatile读和volatile写的内存语义。 经过上面的这些分析，现在我们终于能明白为什么JDK文档说CAS同时具有volatile读和volatile写的内存语义了。 现在对公平锁和非公平锁的内存语义做个总结： 公平锁和非公平锁释放时，最后都要写一个volatile变量state。 公平锁获取时，首先会去读这个volatile变量。 非公平锁获取时，首先会用CAS更新这个volatile变量,这个操作同时具有volatile读和volatile写的内存语义。 从本文对ReentrantLock的分析可以看出，锁释放-获取的内存语义的实现至少有下面两种方式： 利用volatile变量的写-读所具有的内存语义。 利用CAS所附带的volatile读和volatile写的内存语义。 concurrent包的实现由于java的CAS同时具有 volatile 读和volatile写的内存语义，因此Java线程之间的通信现在有了下面四种方式： A线程写volatile变量，随后B线程读这个volatile变量。 A线程写volatile变量，随后B线程用CAS更新这个volatile变量。 A线程用CAS更新一个volatile变量，随后B线程用CAS更新这个volatile变量。 A线程用CAS更新一个volatile变量，随后B线程读这个volatile变量。 Java的CAS会使用现代处理器上提供的高效机器级别原子指令，这些原子指令以原子方式对内存执行读-改-写操作，这是在多处理器中实现同步的关键（从本质上来说，能够支持原子性读-改-写指令的计算机器，是顺序计算图灵机的异步等价机器，因此任何现代的多处理器都会去支持某种能对内存执行原子性读-改-写操作的原子指令）。同时，volatile变量的读/写和CAS可以实现线程之间的通信。把这些特性整合在一起，就形成了整个concurrent包得以实现的基石。如果我们仔细分析concurrent包的源代码实现，会发现一个通用化的实现模式： 首先，声明共享变量为volatile； 然后，使用CAS的原子条件更新来实现线程之间的同步； 同时，配合以volatile的读/写和CAS所具有的volatile读和写的内存语义来实现线程之间的通信。 AQS，非阻塞数据结构和原子变量类（java.util.concurrent.atomic包中的类），这些concurrent包中的基础类都是使用这种模式来实现的，而concurrent包中的高层类又是依赖于这些基础类来实现的。从整体来看，concurrent包的实现示意图如下：]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>java语言</tag>
        <tag>内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[成为Java GC专家（4）—Apache的MaxClients参数详解及其在Tomcat执行FullGC时的影响]]></title>
    <url>%2F2013%2F03%2F04%2F2013-03-04.gc-4%2F</url>
    <content type="text"><![CDATA[原文链接:http://www.importnew.com/3151.html 这是“成为Java GC专家系列文章”的第四篇。 在第一篇文章 成为JavaGC专家Part I — 深入浅出Java垃圾回收机制 中我们学习了不同GC算法的执行过程，GC如何工作，新生代及老年代的基本概念，在JDK7中你应该了解的5种GC类型以及他们的性能如何。 在第二篇文章 成为JavaGC专家Part II — 如何监控Java垃圾回收机制 中我们学到了JVM到底是如何执行垃圾回收，我们如何监控GC，以及那些工具可以使得监控过程更高效。 在第三篇文章 成为Java GC专家系列Part III–如何优化Java垃圾回收机制中我们通过实际的例子学到了一些可以优化GC的参数。同时我们讲解了如何减少对象被转移到老年代空间，如何缩短Full GC时间，以及如何设置GC类型及内存空间。 在第四篇文章中，我们将阐述Apache中MaxClients 参数的重要性，以及他如何在GC发生时，显著地影响整个系统的性能。我将提供几个例子以方便你理解MaxClients 导致的问题。同时我还会说明如何根据系统的内存情况，设置最佳的MaxClients参数值。 MaxClients对于系统的影响NHN (译者注：NHN是作者工作的公司)服务的执行环境中存在一组Throttle valve-type参数（译者注：节流阀参数，用于控制系统负载）。这些参数对于系统来说十分重要。下面我们看一下Apache的 MaxClients 参数在Full GC 发生时是如何影响系统的。 大部分开发人员都知道在由于GC发生而导致的”停止世界现象(STW) “（详细请参见Understanding Java Garbage Collection）。尤其是，NHN的Java开发人员经常会遇到由于GC原因导致的Tomcat报错。由于Java 虚拟机 (JVM)管理着内存，以Java为基础的程序无法摆脱GC导致的STW现象。假如在某一个时间，当你正在操作你开发的应用时，GC开始执行。即使TTS错误没有发生，你的服务也会给客户展现未预期的503错误。 服务执行环境由于架构本身的特点，相比较而言纵向扩展，Web服务更适合横向扩展（译者注:增加服务器的数量，而不是提高件配置）。因此，总体来讲，物理设备会根据性能要求被配置成1台Apache+n台Tomcat。但是本文假设我们的环境是1台Apache+一台Tomcat同时安装在一台主机行，如下图所示。 图1：本文假设的服务执行环境 仅供参考，本文描述的参数基于Apache 2.2.21 (prefork MPM)，Tomcat 6.0.35，CentOS 4.72 (32-bit)，jdk 1.6.0_24。 系统可用内存2GB，垃圾收集器使用ParallelOldGC，AdaptiveSizePolicy采用默认的设置true，堆内存空间600M STW 和HTTP 503让我们假设访问Apache的请求为 200 req/s且有10个httpd进程在运行，另外我们暂时不考虑每个请求的响应时间。在这种前提下，我们假设由于full GC导致的暂停时间为1秒。当Full GC发生的时候Tomcat会怎样？ 第一件进入你脑海的事情应该是Tomcat会因为full GC而停止响应任何请求。在这种情况下，当Tomcat暂停相应请求时Apache会发生什么？ 当Tomcat暂停时，请求会以200 req/s的速度不断的涌入Apache。一般来说，在Full GC发生之前，请求的响应可以快速地被10个或更多的httpd进程处理掉。但是，因为Tomcat暂停了，httpd进程会被不停地创建以相应新进请求。直到超过httpd.conf 文件中定义 MaxClients 为止。由于默认值为256，Apache不会在乎请求以200 req/s的速度涌入。 这时，新创建的httpd线程将如何呢？ Httpd进程通过mod_jk 模块所管理的空闲的AJP连接，将请求转发给Tomcat。如果没有空闲连接，他会申请创建新的连接。但是，因为Tomcat暂停了，创建新连接的请求会被拒绝。因此这些请求会被存储在backlog队列中，数量的多少取决于server.xml中关于AJP Connector的设置。一旦请求数量超过backlog队列的空间限制。Apache就会返回拒绝连接错误。并且返回HTTP 503 错误给用户。 在这种假设条件下，默认的backlog队列空间是100，而请求到达速度是200 req/s。因此，full GC导致的一秒钟的暂停会使得超过100个请求返回503错误。 这样，当Full GC结束后，backlog队列中存储的内容会被Tomcat接受并在通过工作线程处理，线程的最大数量取决于MaxThreads的值（默认200）。 MaxClients 与backlog在这种情况下，设定哪个参数可以避免返回给用户503错误呢？ 首先，我们应该知道backlog的值要够大，以至于能够容纳所有因为Full GC导致暂停期间涌入的请求。换句话说太应该不小于200。 那么，这么设置之后会不会产生新的问题呢？ 让我们假设将backlog设置为200后再重复一下上面的过程。得到的结果比之前更加严重。系统内存使用量一般情况下为50%，但是，在发生Full GC时快速增加到100%，同时导致交换内存空间快速增加，更为严重的是导致Full GC的暂停时间从1秒变成了4秒甚至更多，系统在此期间完全宕机，不能响应任何请求。 在第一种情况下，只有100或更多的请求返回503错误。但是，当我们把backlog调整到200后，超过500个请求会挂起3秒甚至更多地时间无法得到应答 上面这个例子可以很好的说明当你没有完全理解各个设置之间的内在关系时（例如，对于系统的影响），盲目修改系统会导致什么后果。 那么，为什么会产生这个现象呢？ 问题的根源在于 MaxClients 参数的特性。 将MaxClients 设置为一个很大的值本身没有问题，但最重要的是在设定MaxClients 参数时，你要确保即使等同于MaxClients 数量的httpd进程被同时创建，内存使用量也不会超过80%。 系统的内存交换参数一般被设定为60（默认）。因此，当内存使用量超过80%时，就会进行内存交换。 让我们再来看一下为什么这个特性会导致上面那个严重的问题。当请求以200 req/s的速度涌向Tomcat时，Tomcat由于full GC暂停了。此时backlog被设置为200。Apache大约创建100个httpd进程。在这种情况下，一旦内存使用量超过80%，操作系统会激活交换内存区域，并且由于系统认为JVM的老年代中的对象在很长一段时间内未被使用，而将他们移动到交换区域。 最终的结果是，GC使用了内存交换空间，暂停时间剧增。因此httpd进程数进一步增加。从而导致上面描述的内存使用量达到100%的情况。 这两个场合的唯一区别就是backlog的值：100 vs.200。为什么只在200的情况下发生？ 两者不同的原因在于创建的httpd进程的数量。当backlog设置为100时并且Full GC发生时，会创建100个请求的连接并保存在backlog队列中。其他请求得到拒绝连接错误信息并发挥503错误。因此，总的httpd 进程数量仅仅会略高于100。而当backlog被设置为200时，200个请求会创建连接，因此。总的httpd进程数会多于200。这样超过阀值，从而导致内存交换的发生。紧接着，不考虑内存使用量而的设定 MaxClients参数，Full GC导致httpd进程数量暴增，引发内存交换，降低系统性能。 MaxClients参数的计算公式如果系统的内存使2GB，MaxClients 的值在任何情况下都不应该超过内存的80%（1.6GB），以避免由于内存交换导致的性能下降。换句话说。1.6GB的内存应该共享和分配给Apache，Tomcat以及那些默认被安装的代理程序。 让我们假设代理程序被默认安装在系统，并占用了200m内存，对于Tomcat堆内存的-Xmx 被设定为 600m。因此根据top命令的结果，Tomcat会一直占用725m（Perm Gen + Native Heap Area）。最终Apache可以使用700m内存空间。如下所示。 图2：测试系统的top截屏 如上所述，我们将内存设为700m后MaxClients 应该是多少呢？ 这要取决于加载模块的数量，对于NHN Web服务来说。Apache只是个简单的代理转发，每个httpd线程4m内存（根据top命令的结果）足以（参见图2）。因此。700m内存对应的 MaxClients应该是175。 ImportNew首页所有文章资讯Web架构基础技术书籍教程Java小组工具资源成为Java GC专家（4）—Apache的MaxClients参数详解及其在Tomcat执行FullGC时的影响2013/03/04 | 分类： 基础技术, 教程 | 5 条评论 | 标签： GC, JAVAGC专家分享到：本文作者： ImportNew - 王晓杰 未经许可，禁止转载！这是“成为Java GC专家系列文章”的第四篇。 在第一篇文章 成为JavaGC专家Part I — 深入浅出Java垃圾回收机制 中我们学习了不同GC算法的执行过程，GC如何工作，新生代及老年代的基本概念，在JDK7中你应该了解的5种GC类型以及他们的性能如何。 在第二篇文章 成为JavaGC专家Part II — 如何监控Java垃圾回收机制 中我们学到了JVM到底是如何执行垃圾回收，我们如何监控GC，以及那些工具可以使得监控过程更高效。 在第三篇文章 成为Java GC专家系列Part III–如何优化Java垃圾回收机制中我们通过实际的例子学到了一些可以优化GC的参数。同时我们讲解了如何减少对象被转移到老年代空间，如何缩短Full GC时间，以及如何设置GC类型及内存空间。 在第四篇文章中，我们将阐述Apache中MaxClients 参数的重要性，以及他如何在GC发生时，显著地影响整个系统的性能。我将提供几个例子以方便你理解MaxClients 导致的问题。同时我还会说明如何根据系统的内存情况，设置最佳的MaxClients参数值。 MaxClients对于系统的影响 NHN (译者注：NHN是作者工作的公司)服务的执行环境中存在一组Throttle valve-type参数（译者注：节流阀参数，用于控制系统负载）。这些参数对于系统来说十分重要。下面我们看一下Apache的 MaxClients 参数在Full GC 发生时是如何影响系统的。 大部分开发人员都知道在由于GC发生而导致的”停止世界现象(STW) “（详细请参见Understanding Java Garbage Collection）。尤其是，NHN的Java开发人员经常会遇到由于GC原因导致的Tomcat报错。由于Java 虚拟机 (JVM)管理着内存，以Java为基础的程序无法摆脱GC导致的STW现象。假如在某一个时间，当你正在操作你开发的应用时，GC开始执行。即使TTS错误没有发生，你的服务也会给客户展现未预期的503错误。 服务执行环境 由于架构本身的特点，相比较而言纵向扩展，Web服务更适合横向扩展（译者注:增加服务器的数量，而不是提高件配置）。因此，总体来讲，物理设备会根据性能要求被配置成1台Apache+n台Tomcat。但是本文假设我们的环境是1台Apache+一台Tomcat同时安装在一台主机行，如下图所示。 图1：本文假射的服务执行环境 仅供参考，本文描述的参数基于Apache 2.2.21 (prefork MPM)，Tomcat 6.0.35，CentOS 4.72 (32-bit)，jdk 1.6.0_24。 系统可用内存2GB，垃圾收集器使用ParallelOldGC，AdaptiveSizePolicy采用默认的设置true，堆内存空间600M STW 和HTTP 503 让我们假设访问Apache的请求为 200 req/s且有10个httpd进程在运行，另外我们暂时不考虑每个请求的响应时间。在这种前提下，我们假设由于full GC导致的暂停时间为1秒。当Full GC发生的时候Tomcat会怎样？ 第一件进入你脑海的事情应该是Tomcat会因为full GC而停止响应任何请求。在这种情况下，当Tomcat暂停相应请求时Apache会发生什么？ 当Tomcat暂停时，请求会以200 req/s的速度不断的涌入Apache。一般来说，在Full GC发生之前，请求的响应可以快速地被10个或更多的httpd进程处理掉。但是，因为Tomcat暂停了，httpd进程会被不停地创建以相应新进请求。直到超过httpd.conf 文件中定义 MaxClients 为止。由于默认值为256，Apache不会在乎请求以200 req/s的速度涌入。 这时，新创建的httpd线程将如何呢？ Httpd进程通过mod_jk 模块所管理的空闲的AJP连接，将请求转发给Tomcat。如果没有空闲连接，他会申请创建新的连接。但是，因为Tomcat暂停了，创建新连接的请求会被拒绝。因此这些请求会被存储在backlog队列中，数量的多少取决于server.xml中关于AJP Connector的设置。一旦请求数量超过backlog队列的空间限制。Apache就会返回拒绝连接错误。并且返回HTTP 503 错误给用户。 在这种假设条件下，默认的backlog队列空间是100，而请求到达速度是200 req/s。因此，full GC导致的一秒钟的暂停会使得超过100个请求返回503错误。 这样，当Full GC结束后，backlog队列中存储的内容会被Tomcat接受并在通过工作线程处理，线程的最大数量取决于MaxThreads的值（默认200）。 MaxClients 与backlog 在这种情况下，设定哪个参数可以避免返回给用户503错误呢？ 首先，我们应该知道backlog的值要够大，以至于能够容纳所有因为Full GC导致暂停期间涌入的请求。换句话说太应该不小于200。 那么，这么设置之后会不会产生新的问题呢？ 让我们假设将backlog设置为200后再重复一下上面的过程。得到的结果比之前更加严重。系统内存使用量一般情况下为50%，但是，在发生Full GC时快速增加到100%，同时导致交换内存空间快速增加，更为严重的是导致Full GC的暂停时间从1秒变成了4秒甚至更多，系统在此期间完全宕机，不能响应任何请求。 在第一种情况下，只有100或更多的请求返回503错误。但是，当我们把backlog调整到200后，超过500个请求会挂起3秒甚至更多地时间无法得到应答 上面这个例子可以很好的说明当你没有完全理解各个设置之间的内在关系时（例如，对于系统的影响），盲目修改系统会导致什么后果。 那么，为什么会产生这个现象呢？ 问题的根源在于 MaxClients 参数的特性。 将MaxClients 设置为一个很大的值本身没有问题，但最重要的是在设定MaxClients 参数时，你要确保即使等同于MaxClients 数量的httpd进程被同时创建，内存使用量也不会超过80%。 系统的内存交换参数一般被设定为60（默认）。因此，当内存使用量超过80%时，就会进行内存交换。 让我们再来看一下为什么这个特性会导致上面那个严重的问题。当请求以200 req/s的速度涌向Tomcat时，Tomcat由于full GC暂停了。此时backlog被设置为200。Apache大约创建100个httpd进程。在这种情况下，一旦内存使用量超过80%，操作系统会激活交换内存区域，并且由于系统认为JVM的老年代中的对象在很长一段时间内未被使用，而将他们移动到交换区域。 最终的结果是，GC使用了内存交换空间，暂停时间剧增。因此httpd进程数进一步增加。从而导致上面描述的内存使用量达到100%的情况。 这两个场合的唯一区别就是backlog的值：100 vs.200。为什么只在200的情况下发生？ 两者不同的原因在于创建的httpd进程的数量。当backlog设置为100时并且Full GC发生时，会创建100个请求的连接并保存在backlog队列中。其他请求得到拒绝连接错误信息并发挥503错误。因此，总的httpd 进程数量仅仅会略高于100。而当backlog被设置为200时，200个请求会创建连接，因此。总的httpd进程数会多于200。这样超过阀值，从而导致内存交换的发生。紧接着，不考虑内存使用量而的设定 MaxClients参数，Full GC导致httpd进程数量暴增，引发内存交换，降低系统性能。 MaxClients参数的计算公式 如果系统的内存使2GB，MaxClients 的值在任何情况下都不应该超过内存的80%（1.6GB），以避免由于内存交换导致的性能下降。换句话说。1.6GB的内存应该共享和分配给Apache，Tomcat以及那些默认被安装的代理程序。 让我们假设代理程序被默认安装在系统，并占用了200m内存，对于Tomcat堆内存的-Xmx 被设定为 600m。因此根据top命令的结果，Tomcat会一直占用725m（Perm Gen + Native Heap Area）。最终Apache可以使用700m内存空间。如下所示。 图2：测试系统的top截屏 如上所述，我们将内存设为700m后MaxClients 应该是多少呢？ 这要取决于加载模块的数量，对于NHN Web服务来说。Apache只是个简单的代理转发，每个httpd线程4m内存（根据top命令的结果）足以（参见图2）。因此。700m内存对应的 MaxClients应该是175。 总结一个健壮的服务配置至少应该能够降低在服务过载时宕机的时间，在合理的范围内成功的应答请求。针对基于Java的Web服务。你必须检查你的服务在Full GC导致的STW时间内能否稳定的响应请求。 为了响应更多的用户请求和应对DDoS攻击，在没有全面考虑系统内存等因素的情况下，贸然地将 MaxClients设置为一个很大的值，那么它将失去作为阀值的功能，而导致系统出现更严重的问题。 本文提到的情况只会持续3-5秒，因此绝大多数传统的监控工具都无法及时的发现。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>java语言</tag>
        <tag>gc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[成为Java GC专家（3）—如何优化Java垃圾回收机制]]></title>
    <url>%2F2013%2F03%2F01%2F2013-03-01.gc-3%2F</url>
    <content type="text"><![CDATA[原文链接:http://www.importnew.com/3146.html 本文是成为Java GC专家系列文章的第三篇。在第一篇《成为JavaGC专家Part I — 深入浅出Java垃圾回收机制》中我们学习了不同GC算法的执行过程，GC是如何工作的，什么是新生代和老年代，你应该了解的JDK7中的5种GC类型，以及这5种类型对于应用性能的影响。 在第二篇《成为JavaGC专家Part II — 如何监控Java垃圾回收机制》，我解释了JVM实际上是如何执行垃圾回收的，我们如何监控GC，以及那哪些具可以让我们的工作更快，更高效。在第三篇文章中，我们会基于实际的例子来解释一些优化GC的最佳实践。我认为在阅读本篇文章之前，你已经很好地理解了之前的文章，因此，为了你能够更好地学习本文，如果你还没有读过之前的两篇文章话，请先阅读。 为什么需要优化GC或者说的更确切一些，对于基于Java的服务，是否有必要优化GC？应该说，对于所有的基于Java的服务，并不总是需要进行GC优化，但前提是所运行的基于Java的系统，包含了如下参数或行为： 已经通过 -Xms 和–Xmx 设置了内存大小 包含了 -server 参数 系统中没有超时日志等错误日志 换句话说，如果你没有设定内存的大小，并且系统充斥着大量的超时日志时，你就需要在你的系统中进行GC优化了。 但是，你需要时刻铭记一条：GC优化永远是最后一项任务。 想一下进行GC优化的最根本原因，垃圾收集器清除在Java程序中创建的对象，GC执行的次数即需要被垃圾收集器清理的对象个数，与创建对象的数量成正比，因此，首先你应该减少创建对象的数量。 俗话说的好，“冰冻三尺非一日之寒”。我们应该从小事做起，否则日积月累就会很难管理。 我们需要使用StringBuilder 或者StringBuffer 来替代String 应该尽量少的输出日志 但是，我们知道有些情况会让我们束手无策，我们眼睁睁的看着XML以及JSON解析占用了大量的内存。即便我们已经尽可能少的使用String以及尽量少的输出日志，大量的临时内存被用于XML或者JSON解析，例如10-100MB。但是，舍弃XML和JSON是很难的。我们只要知道，他会占用很多内存。 如果应用内存使用量经过几次重复调整之后有所改善，你就可以开始GC优化了。 我为GC优化归纳了两个目的： 一个是将转移到老年代的对象数量降到最少 另一个是减少Full GC的执行时间 将转移到老年代的对象数量降到最少按代的GC机制由Oracle JVM提供，不包括可以在JDK7以及更高版本中使用的G1 GC。换句话说，对象被创建在伊甸园空间，而后转化到幸存者空间，最终剩余的对象被送到老年代。某些比较大的对象会在被创建在伊甸园空间后，直接转移到老年代空间。老年代空间上的GC处理会新生代花费更多的时间。因此，减少被移到老年代对象的数据可以显著地减少Full GC的频率。减少被移到老年代空间的对象的数量，可能被误解为将对象留在新生代。但是，这是不可能的。取而代之，你可以调整新生代空间的大小。 减少Full GC执行时间Full GC的执行时间比Minor GC要长很多。因此，如果Full GC花费了太多的时间（超过1秒），一些连接的部分可能会发生超时错误。 如果你试图通过消减老年代空间来减少Full GC的执行时间，可能会导致OutOfMemoryError 或者 Full GC执行的次数会增加。与之相反，如果你试图通过增加老年代空间来减少Full GC执行次数，执行时间会增加。因此，你需要将老年代空间设定为一个“合适”的值。 影响GC性能的参数正如我们在第二篇文章结尾提到的，不要幻想“某个人设定了GC参数后性能得到极大的提高，我们为什么不和他用一样的参数？”，因为不同的Web服务所创建对象的大小和他们的生命周期都不尽相同。 简单来说，如果一个任务的执行条件是A，B，C，D和E，同样的任务执行条件换为A和B，你会觉得哪个更快？从一般人的直觉来看，在A和B条件下执行的任务会更快。 Java GC参数也是相同的道理，设定一些参数不但没有提高GC执行速度，反而可能导致他更慢。GC优化的最基本原则是将不同的GC参数用于2台或者多台服务器，并进行对比，并将那些被证明提高了性能或者减少了GC执行时间的参数应用于服务器。请谨记这一点。 下面这个表格列出了GC参数中与内存大小相关的，可以影响性能的参数。 定义 参数 描述 堆内存空间 -Xms Heap area size when starting JVM(启动JVM时的堆内存空间。) -Xmx Maximum heap area size(堆内存最大限制) 新生代空间 -XX:NewRatio Ratio of New area and Old area(新生代和老年代的占比) -XX:NewSize New area size(新生代空间) -XX:SurvivorRatio Ratio of Eden area and Survivor area(伊甸园空间和幸存者空间的占比) 我在进行GC优化时经常使用-Xms，-Xmx和-XX:NewRatio。-Xms和-Xmx是必须的。你如何设定NewRatio 会对GC性能产生十分显著的影响。有些人可能会问如何设定Perm区域的大小？你可以通过-XX:PermSize 和-XX:MaxPermSize参数来设定， 当OutOfMemoryError 错误发生并且是由于Perm空间不足导致时，另一个可能影响GC性能的参数是GC类型。下表列出了所有可选的GC类型（基于JDK6.0） 表2：GC类型可选参数 除了G1 GC，可以通过每种类型第一行的参数来切换GC类型。最常用的GC类型是Serial GC。他专门针对客户端系统进行了优化。 影响GC性能的参数有很多，但是上面提到的参数会带来最显著的效果。请牢记，设定过多的参数不一定会减少GC执行时间。 GC优化过程GC优化的过程与大多数性能改善的过程及其类似。下面是我使用的GC优化过程。 1.监控GC状态首先你需要监控GC来检查在系统执行过程中GC的各种状态。 2.在分析监控结果后，决定是否进行GC优化在检查GC状态的过程中，你应该分析监控结果以便决定是否进行GC优化，如果分析结果表明执行GC的时间只有0.1-0.3秒，那你就没必要浪费时间去进行GC优化。但是，如果GC的执行时间是1-3秒，或者超过10秒，GC将势在必行。 但是，如果你已经为Java分配了10GB的内存，并且不能再减少内存大小，你将无法再对GC进行优化。在进行GC优化之前，你必须想清楚你为什么要分配如此大的内存空间。假如当你分1 GB 或 2 GB内存时出现OutOfMemoryError ，你应该执行堆内存转储（heap dump），并消除隐患。 注意： 堆内存转储是一个用来检查Java内存中的对象和数据的文件。该文件可以通过执行JDK中的jmap命令来创建。在创建文件的过程中，Java程序会暂停，因此不要再系统执行过程中创建该文件。 3. 调整GC类型/内存空间如果你已经决定要进行GC优化，那么就要选择GC类型和设定内存空间。在这时，如果你有几台不同服务器，请时刻牢记，检查每一台服务器的GC参数，并进行有针对性的优化。 4.分析结果在调整了GC参数并持续收集24小时之后，开始对结果进行分析，如果你幸运的话，你就找到那些最适合系统的GC参数。反之，你需要通过分析日志来检查内存是如何被分配的。然后你需要通过不断的调整GC类型和内存空间大小一边找到最佳的参数。 5. 如果结果令人满意，你可以将该参数应用于所有的服务器，并停止GC优化有过GC优化结果令人满意，你可以应用于所有的服务器，下面的章节中，我们将看到每个步骤的具体任务。 监控GC状态及分析结果查看运行中的Web Application Server (WAS)的GC状态的最佳方法是通过jstat命令，在第二篇文章成为JavaGC专家Part II — 如何监控Java垃圾回收机制中我已经详细解释过jstat命令，因此本篇文章我将重点描述数据部分。 下面这个例子展现了某个JVM在进行GC优化之前的状态。 （很遗憾，这不是一个操作服务器）1234$ jstat -gcutil 21719 1sS0 S1 E O P YGC YGCT FGC FGCT GCT48.66 0.00 48.10 49.70 77.45 3428 172.623 3 59.050 231.67348.66 0.00 48.10 49.70 77.45 3428 172.623 3 59.050 231.673 如上表，我们先看一下YGC 和YGCT，计算YGCT/ YGC得到0.050秒（50毫秒）。这意味着新生代空间上的GC操作平均花费50毫秒。在这种情况，你大可不必担心新生代空间上执行的GC操作。接下来，我们来看一下FGCT 和FGC。，计算FGCT/ FGC得到19.68秒，这意味着GC的平均执行时间为19.68秒，可能是每次花费19.68秒执行了三次，也可能是其中的两次执行了1秒而另一次执行了58秒。不论哪种情况，都需要进行GC优化。 通过jstat 命令可以很轻易地查看GC状态，但是，分析GC的最佳方式是通过–verbosegc参数来生成日志，在之前的文章中我已经解释了如何分析这些日志，HPJMeter 是我个人最喜欢的用于分析-verbosegc 日志的工具。他很易于使用和分析结果。通过HPJmeter你可以很轻易查看GC执行时间以及GC发生频率。如果GC执行时间满足下面所有的条件，就意味着无需进行GC优化了。 Minor GC执行的很快（小于50ms） Minor GC执行的并不频繁（大概10秒一次） Full GC执行的很快（小于1s） Full GC执行的并不频繁（10分钟一次） 上面提到的数字并不是绝对的；他们根据服务状态的不同而有所区别，某些服务可能满足于Full GC每次0.9秒的速度，但另一些可能不是。因此，针对不同的服务设定不同的值以决定是否进行GC优化。 在查看GC状态的时候有件事你需要特别注意，那就是不要只关注Minor GC 和Full GC的执行时间。还要关注GC执行的次数,例如，当新生代空间较小时，Minor GC会过于频繁的执行（有时每秒超过1次）。另外，转移到老年代的对象数增多，则会导致Full GC执行次数增多。因此，别忘了加上–gccapacity参数来查看具体占用了多少空间。 设定GC类型/内存空间大小设定GC类型OracleJVM有5种GC类型，但是在JDK7之前的版本中，只能在Parallel GC, Parallel Compacting GC 和CMS GC之中选择一个，对于选择哪个没有明确的原则和规则。 这样的话，我们该如何选择呢？强烈建议三者都选，但是，有一点是很明确的：CMS GC比Parallel GCs更快。如果真的如此，那么就选CMS GC了。但是，CMS GC也不总是更快。整体来看，CMS GC模式下的Full GC执行更快，不过，一旦出现并行模式失败，他将比Parallel GC更慢。 并发模式失败 我们来详细讲解一下并发模式失败。 Parallel GC 和 CMS GC 最大的不同来自于压缩任务。压缩任务是通过删除已分配内存空间中的空白空间以便压缩内存，清理内存碎片。 在Parallel GC模式下，压缩工作在Full GC执行时进行，这会费很多时间，但是，在执行完Full GC之后，由于能够顺序地分配空间，随后的内存能够被更快的分配。 与之相反的，CMS GC并不进行压缩处理，因此，CMS GC执行的更快。但是，由于没有压缩，在进行磁盘清理之前，内存中会有很多空白空间。这就是说，可能没有足够的空间存储大的对象，例如，虽然老年代空间还有300MB空间，但是一些10MB的对象无法被顺序的存储。在这种情况下，会出现“并行模式失败”警告，并执行压缩处理。在CMS GC模式下，压缩处理的执行时间要比Parallel GCs长很多。另外，这还将导致另外一个问题。关于并发模式失败的详细说明，可以参考Oracle工程师撰写的Understanding CMS GC Logs。 综上所述，你需要找到最适合你的系统的GC类型。 每个系统都有最适合他的GC类型等着你去寻找，如果你有6台服务器。我建议你每两台设置相同的参数。并添加 –verbosegc参数，分析结果。 设定内存空间大小下表展示了内存空间大小，GC执行次数以及GC执行时间三者间的关系。 大内存空间减小GC执行次数增加GC执行时间 小内存空间减小GC执行时间增加GC执行次数 关于如何设置内存空间的大小，没有唯一的标准答案。如果服务器资源足够，而且Full GC也可能在1秒内完成，设置为10GB当然可行。但绝大多数服务器并不是这样，当内存设为10GB时，可能要花费10~30秒来执行Full GC。当然，执行时间会随对象的大小而改变。 鉴于如此，我们应该如何设定内存空间大小呢？一般来说，我建议为500MB。不过请注意这不是让你将WAS的内存参数设置为–Xms500m 和–Xmx500m。根据优化GC之前的状态，如果Full GC执行之后内存空间剩余300MB，那么最好将内存设置为1GB（300MB（默认程序占用）+ 500MB（老年代最小空间）+200MB（空闲内存））。也就是说你要为老年代额外设置500MB。因此，如果你有三个执行服务器，内存分别设置为1GB，1.5GB，2GB，并且检查结果。 理论上来讲，GC执行速度应该遵循1GB&gt; 1.5GB&gt; 2GB,因此1GB执行GC速度最快。但是并不说明1GB空间的Full GC会花费1秒而2GB空间会花费2秒。时间取决于服务器的性能和对象的大小。因此，最佳的方式是建立尽可能多的衡量指标来监控他们。 对于内存空间大小，你应该额外设定NewRatio参数。NewRatio参数是新生代和老年代空间的比例，即XX:NewRatio=1意味着新生代与老年代之比为1:1。对于1GB来说就是新生代和老年代各500MB。如果NewRatio为2，意味着新生代老年代之比为1:2，新生代占堆的1/3,因此该值越大，老年代空间越大，新生代空间越小。 这看似一件不是很重要的事情，但NewRatio参数会显著地影响整个GC的性能。如果新生代空间很小，会用更多的对象被转移到老年代空间，这样导致频繁的Full GC，增加暂停时间。 你可以简单的认为NewRatio 为1是最佳的选择，但是，有时可能设置为2或3更好，我就见过很多这样的例子。 如何最快的完成GC优化？ 对比性能测试的结果应该是最快地方法，为每一台服务器设置不同的参数并监控他们的状态，强烈建议至少监控1或2天的数据。但是，当你对GC优化是，你要确保每次执行相同的负载。并且请求的比率，例如URL都应该是一致的。不过，即便对于专业测试人员要想精确的控制负载也是很难的，并要花费大量的时间准备。因此，相对来说比较方便和容易的方法是调整参数，之后花费较长的时间收集结果。 分析GC优化结果在设置了GC参数以及-verbosegc参数之后，通过tail命令确保日志被正确的生成。如果参数设置的不正确或者日志没有生成，你将白白浪费你的时间。如果日志正确的话，持续收集1到2天。随后最好将日志下载到本地PC并用HPJMeter来分析 Full GC 执行时间 Minor GC执行时间 Full GC 执行间隔 Minor GC 执行间隔 Entire Full GC 执行时间 Entire Minor GC 执行时间 Entire GC 执行时间 找到最佳的GC参数是件非常幸运的事情，然而在大多数场合，我们并不会得到幸运之神的眷顾，在进行GC优化时要尽量小心谨慎，想一步完成优化往往会导致OutOfMemoryError 。 优化示例 好了，我们一直在纸上谈兵，现在我们看一些实际的GC优化的例子。 示例1 下面这个例子针对 Service S的优化,对于最近被部署的 Service S，Full GC花费了太长的时间。 请看 jstat –gcutil的执行结果。12S0 S1 E O P YGC YGCT FGC FGCT GCT12.16 0.00 5.18 63.78 20.32 54 2.047 5 6.946 8.993 最左边的Perm 空间对于最初的GC优化不是很重要，这一次YGC参数的值更加有用。 Minor GC和Full GC的平均值如下表所示 表3：Service S的Minor GC 和Full GC的平均执行时间 GC 类型 GC 执行次数 GC 执行时间 平均 Minor GC 54 2.047 37 ms Full GC 5 6.946 1,389 s 最重要的是下面两个数据 新生代实际使用空间: 212,992 KB 老年代实际使用空间: 1,884,160 KB因此，总的内存空间为2GB,不算Perm空间的话，新生代与老年代之比为1:9。通过jstat和-verbosegc 日志进行数据收集，并把三台服务器按照如下方式设置。 NewRatio=2 NewRatio=3 NewRatio=4一天之后，检查系统的GC日志后发现，在设置了NewRatio参数后很幸运的没有发生Full GC，为什么？ NewRatio=2: 45 ms NewRatio=3: 34 ms NewRatio=4: 30 ms我们看到NewRatio=4 是最佳的参数，虽然它的新生代空间最小，但GC时间确最短。设定这个参数之后，系统没有执行过Full GC。 为了说明这个问题，下面是服务执行一段时间后执行jstat –gcutil的结果12S0 S1 E O P YGC YGCT FGC FGCT GCT8.61 0.00 30.67 24.62 22.38 2424 30.219 0 0.000 30.219 你可能会认为因为服务器接受的请求少才导致的GC执行频率下降。实际上，虽然Full GC没有执行，但是Minor GC被执行了 2,424次。 示例2 这是一个针对ServiceA的例子，我们通过公司内部的应用性能管理系统（APM）发现JVM暂停了相当长的时间（超过8秒），因此我们进行了GC优化。我们找到了Full GC执行时间过长的原因，并着手解决。 进行GC优化的第一步，就是我们添加了-verbosegc参数，并得到如下结果。 图1：进行GC优化之前的STW时间 如上图所示，由HPJMeter自动生成的图片之一。X坐标表示JVM执行的时间。Y坐标表示每次GC的时间。CMS绿点，表示Full GC结果。Parallel Scavenge蓝点，表示Minor GC结果。 之前我曾经说过CMS GC是最快的，但是上面的的结果显示出于某种原因，它最多花费了15秒。是什么导致这个结果？是否想起我之前提过的，CMS在进行内存清理时，会变慢。与此同时，服务的内存被设定为 –Xms1g和–Xmx4g ，且实际分配了4GB内存。 因此，我将GC类型从CMS改为Parallel GC。并且将内存改为2GB，设定NewRatio 为3。几小时之后我使用 jstat –gcutil得到如下结果12S0 S1 E O P YGC YGCT FGC FGCT GCT0.00 30.48 3.31 26.54 37.01 226 11.131 4 11.758 22.890 相对于4GB时的15秒，Full GC变成了平均每次3秒。但是3秒一样比较慢，因此我设计了如下6种场景。 Case 1: -XX:+UseParallelGC -Xms1536m -Xmx1536m -XX:NewRatio=2 Case 2: -XX:+UseParallelGC -Xms1536m -Xmx1536m -XX:NewRatio=3 Case 3: -XX:+UseParallelGC -Xms1g -Xmx1g -XX:NewRatio=3 Case 4: -XX:+UseParallelOldGC -Xms1536m -Xmx1536m -XX:NewRatio=2 Case 5: -XX:+UseParallelOldGC -Xms1536m -Xmx1536m -XX:NewRatio=3 Case 6: -XX:+UseParallelOldGC -Xms1g -Xmx1g -XX:NewRatio=3 那一个最快呢？结果显示，内存越小，结果越好。下图展示了Case6的结果。这是GC的性能最好。最长的响应时间只有1.7秒。平均时间在1秒之内。 图2：Case6的时间图表 基于以上结果。我们按照Case6调整了GC参数。但是，这导致了每天晚上都会发生OutOfMemoryError。在这里很难解释具体的原因。简单来说，批处理程序导致了内存泄漏。相关的问题已经被解决。 如果对GC日志只分析很短的时间就贸然对所有服务器进行优化是非常危险的。请时刻牢记，你必须同时分析GC日志和应用程序。 我们回顾了两个关于GC优化的例子，正如我之前提到的，例子中提到的GC参数，可以设置在相同的服务器之上，但前提是他们具有相同的CPU，操作系统，JDK版本以及运行着相同的服务。但是不要直接把我用过的参数用到你的服务至上，它们未必能很好的工作。 结论我凭借经验进行GC优化，而没有执行堆转储并分析内存的详细内容。精确地分析内存可以得到更好的优化效果。但是，这种分析一般适用于内存使用量相对固定的场合。不过，如果服务严重过载并占用的大量的内存，强力建议根据之前的经验进行GC优化。 我已经在一些服务上设置了G1 GC参数，并进行过性能测试。但还没有应用与正式环境，G1 GC参数的速度要快于其他任何GC类型。但是，你必须要升级到JDK7。另外，他的稳定性也暂时没有保障，没人知道是否会出现致命的错误。因此还不到将其正式应用的时候 在未来的某一天，等到JDK7真正稳定了（这不是说他现在不稳定），并且WAS针对JDK7进行优化后，G1 GC最终能够按照预期的那样工作了，我们可能就不需要在进行GC优化了。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>java语言</tag>
        <tag>gc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解Java内存模型（四）——volatile]]></title>
    <url>%2F2013%2F02%2F05%2F2013-02-05-java-memory-model-4%2F</url>
    <content type="text"><![CDATA[原文链接: http://www.infoq.com/cn/articles/java-memory-model-4 volatile的特性当我们声明共享变量为volatile后，对这个变量的读/写将会很特别。理解volatile特性的一个好方法是：把对volatile变量的单个读/写，看成是使用同一个监视器锁对这些单个读/写操作做了同步。下面我们通过具体的示例来说明，请看下面的示例代码：12345678910111213141516class VolatileFeaturesExample &#123; volatile long vl = 0L; //使用volatile声明64位的long型变量 public void set(long l) &#123; vl = l; //单个volatile变量的写 &#125; public void getAndIncrement () &#123; vl++; //复合（多个）volatile变量的读/写 &#125; public long get() &#123; return vl; //单个volatile变量的读 &#125;&#125; 假设有多个线程分别调用上面程序的三个方法，这个程序在语意上和下面程序等价：1234567891011121314151617class VolatileFeaturesExample &#123; long vl = 0L; // 64位的long型普通变量 public synchronized void set(long l) &#123; //对单个的普通 变量的写用同一个监视器同步 vl = l; &#125; public void getAndIncrement () &#123; //普通方法调用 long temp = get(); //调用已同步的读方法 temp += 1L; //普通写操作 set(temp); //调用已同步的写方法 &#125; public synchronized long get() &#123; //对单个的普通变量的读用同一个监视器同步 return vl; &#125;&#125; 如上面示例程序所示，对一个volatile变量的单个读/写操作，与对一个普通变量的读/写操作使用同一个监视器锁来同步，它们之间的执行效果相同。 监视器锁的happens-before规则保证释放监视器和获取监视器的两个线程之间的内存可见性，这意味着对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。 监视器锁的语义决定了临界区代码的执行具有原子性。这意味着即使是64位的long型和double型变量，只要它是volatile变量，对该变量的读写就将具有原子性。如果是多个volatile操作或类似于volatile++这种复合操作，这些操作整体上不具有原子性。 简而言之，volatile变量自身具有下列特性： 可见性。对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。 原子性：对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种复合操作不具有原子性。 volatile写-读建立的happens before关系上面讲的是volatile变量自身的特性，对程序员来说，volatile对线程的内存可见性的影响比volatile自身的特性更为重要，也更需要我们去关注。 从JSR-133开始，volatile变量的写-读可以实现线程之间的通信。 从内存语义的角度来说，volatile与监视器锁有相同的效果：volatile写和监视器的释放有相同的内存语义；volatile读与监视器的获取有相同的内存语义。 请看下面使用volatile变量的示例代码：12345678910111213141516class VolatileExample &#123; int a = 0; volatile boolean flag = false; public void writer() &#123; a = 1; //1 flag = true; //2 &#125; public void reader() &#123; if (flag) &#123; //3 int i = a; //4 …… &#125; &#125;&#125; 假设线程A执行writer()方法之后，线程B执行reader()方法。根据happens before规则，这个过程建立的happens before 关系可以分为两类： 根据程序次序规则，1 happens before 2; 3 happens before 4。 根据volatile规则，2 happens before 3。 根据happens before 的传递性规则，1 happens before 4。 上述happens before 关系的图形化表现形式如下： 在上图中，每一个箭头链接的两个节点，代表了一个happens before 关系。黑色箭头表示程序顺序规则；橙色箭头表示volatile规则；蓝色箭头表示组合这些规则后提供的happens before保证。 这里A线程写一个volatile变量后，B线程读同一个volatile变量。A线程在写volatile变量之前所有可见的共享变量，在B线程读同一个volatile变量后，将立即变得对B线程可见。 volatile写-读的内存语义volatile写的内存语义如下： 当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存。 以上面示例程序VolatileExample为例，假设线程A首先执行writer()方法，随后线程B执行reader()方法，初始时两个线程的本地内存中的flag和a都是初始状态。下图是线程A执行volatile写后，共享变量的状态示意图： 如上图所示，线程A在写flag变量后，本地内存A中被线程A更新过的两个共享变量的值被刷新到主内存中。此时，本地内存A和主内存中的共享变量的值是一致的。 volatile读的内存语义如下： 当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。 下面是线程B读同一个volatile变量后，共享变量的状态示意图： 如果我们把volatile写和volatile读这两个步骤综合起来看的话，在读线程B读一个volatile变量后，写线程A在写这个volatile变量之前所有可见的共享变量的值都将立即变得对读线程B可见。 下面对volatile写和volatile读的内存语义做个总结： 线程A写一个volatile变量，实质上是线程A向接下来将要读这个volatile变量的某个线程发出了（其对共享变量所在修改的）消息。 线程B读一个volatile变量，实质上是线程B接收了之前某个线程发出的（在写这个volatile变量之前对共享变量所做修改的）消息。 线程A写一个volatile变量，随后线程B读这个volatile变量，这个过程实质上是线程A通过主内存向线程B发送消息。 volatile内存语义的实现下面，让我们来看看JMM如何实现volatile写/读的内存语义。 前文我们提到过重排序分为编译器重排序和处理器重排序。为了实现volatile内存语义，JMM会分别限制这两种类型的重排序类型。下面是JMM针对编译器制定的volatile重排序规则表： 是否能重排序第二个操作第一个操作普通读/写volatile读volatile写普通读/写NOvolatile读NONONOvolatile写NONO 举例来说，第三行最后一个单元格的意思是：在程序顺序中，当第一个操作为普通变量的读或写时，如果第二个操作为volatile写，则编译器不能重排序这两个操作。 从上表我们可以看出： 当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile写之前的操作不会被编译器重排序到volatile写之后。 当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile读之后的操作不会被编译器重排序到volatile读之前。 当第一个操作是volatile写，第二个操作是volatile读时，不能重排序。 为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。对于编译器来说，发现一个最优布置来最小化插入屏障的总数几乎不可能，为此，JMM采取保守策略。下面是基于保守策略的JMM内存屏障插入策略： 在每个volatile写操作的前面插入一个StoreStore屏障。 在每个volatile写操作的后面插入一个StoreLoad屏障。 在每个volatile读操作的后面插入一个LoadLoad屏障。 在每个volatile读操作的后面插入一个LoadStore屏障。 上述内存屏障插入策略非常保守，但它可以保证在任意处理器平台，任意的程序中都能得到正确的volatile内存语义。 下面是保守策略下，volatile写插入内存屏障后生成的指令序列示意图： 上图中的StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作已经对任意处理器可见了。这是因为StoreStore屏障将保障上面所有的普通写在volatile写之前刷新到主内存。 这里比较有意思的是volatile写后面的StoreLoad屏障。这个屏障的作用是避免volatile写与后面可能有的volatile读/写操作重排序。因为编译器常常无法准确判断在一个volatile写的后面，是否需要插入一个StoreLoad屏障（比如，一个volatile写之后方法立即return）。为了保证能正确实现volatile的内存语义，JMM在这里采取了保守策略：在每个volatile写的后面或在每个volatile读的前面插入一个StoreLoad屏障。从整体执行效率的角度考虑，JMM选择了在每个volatile写的后面插入一个StoreLoad屏障。因为volatile写-读内存语义的常见使用模式是：一个写线程写volatile变量，多个读线程读同一个volatile变量。当读线程的数量大大超过写线程时，选择在volatile写之后插入StoreLoad屏障将带来可观的执行效率的提升。从这里我们可以看到JMM在实现上的一个特点：首先确保正确性，然后再去追求执行效率。 下面是在保守策略下，volatile读插入内存屏障后生成的指令序列示意图： 上图中的LoadLoad屏障用来禁止处理器把上面的volatile读与下面的普通读重排序。LoadStore屏障用来禁止处理器把上面的volatile读与下面的普通写重排序。 上述volatile写和volatile读的内存屏障插入策略非常保守。在实际执行时，只要不改变volatile写-读的内存语义，编译器可以根据具体情况省略不必要的屏障。下面我们通过具体的示例代码来说明：123456789101112131415class VolatileBarrierExample &#123; int a; volatile int v1 = 1; volatile int v2 = 2; void readAndWrite() &#123; int i = v1; //第一个volatile读 int j = v2; // 第二个volatile读 a = i + j; //普通写 v1 = i + 1; // 第一个volatile写 v2 = j * 2; //第二个 volatile写 &#125; … //其他方法&#125; 针对readAndWrite()方法，编译器在生成字节码时可以做如下的优化： 注意，最后的StoreLoad屏障不能省略。因为第二个volatile写之后，方法立即return。此时编译器可能无法准确断定后面是否会有volatile读或写，为了安全起见，编译器常常会在这里插入一个StoreLoad屏障。 上面的优化是针对任意处理器平台，由于不同的处理器有不同“松紧度”的处理器内存模型，内存屏障的插入还可以根据具体的处理器内存模型继续优化。以x86处理器为例，上图中除最后的StoreLoad屏障外，其它的屏障都会被省略。 前面保守策略下的volatile读和写，在 x86处理器平台可以优化成： 前文提到过，x86处理器仅会对写-读操作做重排序。X86不会对读-读，读-写和写-写操作做重排序，因此在x86处理器中会省略掉这三种操作类型对应的内存屏障。在x86中，JMM仅需在volatile写后面插入一个StoreLoad屏障即可正确实现volatile写-读的内存语义。这意味着在x86处理器中，volatile写的开销比volatile读的开销会大很多（因为执行StoreLoad屏障开销会比较大）。 JSR-133为什么要增强volatile的内存语义在JSR-133之前的旧Java内存模型中，虽然不允许volatile变量之间重排序，但旧的Java内存模型允许volatile变量与普通变量之间重排序。在旧的内存模型中，VolatileExample示例程序可能被重排序成下列时序来执行： 在旧的内存模型中，当1和2之间没有数据依赖关系时，1和2之间就可能被重排序（3和4类似）。其结果就是：读线程B执行4时，不一定能看到写线程A在执行1时对共享变量的修改。 因此在旧的内存模型中 ，volatile的写-读没有监视器的释放-获所具有的内存语义。为了提供一种比监视器锁更轻量级的线程之间通信的机制，JSR-133专家组决定增强volatile的内存语义：严格限制编译器和处理器对volatile变量与普通变量的重排序，确保volatile的写-读和监视器的释放-获取一样，具有相同的内存语义。从编译器重排序规则和处理器内存屏障插入策略来看，只要volatile变量与普通变量之间的重排序可能会破坏volatile的内存语意，这种重排序就会被编译器重排序规则和处理器内存屏障插入策略禁止。 由于volatile仅仅保证对单个volatile变量的读/写具有原子性，而监视器锁的互斥执行的特性可以确保对整个临界区代码的执行具有原子性。在功能上，监视器锁比volatile更强大；在可伸缩性和执行性能上，volatile更有优势。如果读者想在程序中用volatile代替监视器锁，请一定谨慎。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>java语言</tag>
        <tag>内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解Java内存模型（三）——顺序一致性]]></title>
    <url>%2F2013%2F01%2F29%2F2013-01-29-java-memory-model-3%2F</url>
    <content type="text"><![CDATA[原文链接：http://www.infoq.com/cn/articles/java-memory-model-3 数据竞争与顺序一致性保证当程序未正确同步时，就会存在数据竞争。java内存模型规范对数据竞争的定义如下： 在一个线程中写一个变量， 在另一个线程读同一个变量， 而且写和读没有通过同步来排序。 当代码中包含数据竞争时，程序的执行往往产生违反直觉的结果（前一章的示例正是如此）。如果一个多线程程序能正确同步，这个程序将是一个没有数据竞争的程序。 JMM对正确同步的多线程程序的内存一致性做了如下保证： 如果程序是正确同步的，程序的执行将具有顺序一致性（sequentially consistent）–即程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同（马上我们将会看到，这对于程序员来说是一个极强的保证）。这里的同步是指广义上的同步，包括对常用同步原语（lock，volatile和final）的正确使用。 顺序一致性内存模型顺序一致性内存模型是一个被计算机科学家理想化了的理论参考模型，它为程序员提供了极强的内存可见性保证。顺序一致性内存模型有两大特性： 一个线程中的所有操作必须按照程序的顺序来执行。 （不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。 顺序一致性内存模型为程序员提供的视图如下： 在概念上，顺序一致性模型有一个单一的全局内存，这个内存通过一个左右摆动的开关可以连接到任意一个线程。同时，每一个线程必须按程序的顺序来执行内存读/写操作。从上图我们可以看出，在任意时间点最多只能有一个线程可以连接到内存。当多个线程并发执行时，图中的开关装置能把所有线程的所有内存读/写操作串行化。 为了更好的理解，下面我们通过两个示意图来对顺序一致性模型的特性做进一步的说明。 假设有两个线程A和B并发执行。其中A线程有三个操作，它们在程序中的顺序是：A1-&gt;A2-&gt;A3。B线程也有三个操作，它们在程序中的顺序是：B1-&gt;B2-&gt;B3。 假设这两个线程使用监视器来正确同步：A线程的三个操作执行后释放监视器，随后B线程获取同一个监视器。那么程序在顺序一致性模型中的执行效果将如下图所示： 现在我们再假设这两个线程没有做同步，下面是这个未同步程序在顺序一致性模型中的执行示意图： 未同步程序在顺序一致性模型中虽然整体执行顺序是无序的，但所有线程都只能看到一个一致的整体执行顺序。以上图为例，线程A和B看到的执行顺序都是：B1-&gt;A1-&gt;A2-&gt;B2-&gt;A3-&gt;B3。之所以能得到这个保证是因为顺序一致性内存模型中的每个操作必须立即对任意线程可见。 但是，在JMM中就没有这个保证。未同步程序在JMM中不但整体的执行顺序是无序的，而且所有线程看到的操作执行顺序也可能不一致。比如，在当前线程把写过的数据缓存在本地内存中，且还没有刷新到主内存之前，这个写操作仅对当前线程可见；从其他线程的角度来观察，会认为这个写操作根本还没有被当前线程执行。只有当前线程把本地内存中写过的数据刷新到主内存之后，这个写操作才能对其他线程可见。在这种情况下，当前线程和其它线程看到的操作执行顺序将不一致。 同步程序的顺序一致性效果下面我们对前面的示例程序ReorderExample用监视器来同步，看看正确同步的程序如何具有顺序一致性。 请看下面的示例代码：12345678910111213141516class SynchronizedExample &#123; int a = 0; boolean flag = false; public synchronized void writer() &#123; a = 1; flag = true; &#125; public synchronized void reader() &#123; if (flag) &#123; int i = a; …… &#125; &#125;&#125; 上面示例代码中，假设A线程执行writer()方法后，B线程执行reader()方法。这是一个正确同步的多线程程序。根据JMM规范，该程序的执行结果将与该程序在顺序一致性模型中的执行结果相同。下面是该程序在两个内存模型中的执行时序对比图： 在顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在JMM中，临界区内的代码可以重排序（但JMM不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。JMM会在退出监视器和进入监视器这两个关键时间点做一些特别处理，使得线程在这两个时间点具有与顺序一致性模型相同的内存视图（具体细节后文会说明）。虽然线程A在临界区内做了重排序，但由于监视器的互斥执行的特性，这里的线程B根本无法“观察”到线程A在临界区内的重排序。这种重排序既提高了执行效率，又没有改变程序的执行结果。 从这里我们可以看到JMM在具体实现上的基本方针：在不改变（正确同步的）程序执行结果的前提下，尽可能的为编译器和处理器的优化打开方便之门。 未同步程序的执行特性对于未同步或未正确同步的多线程程序，JMM只提供最小安全性：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值（0，null，false），JMM保证线程读操作读取到的值不会无中生有（out of thin air）的冒出来。为了实现最小安全性，JVM在堆上分配对象时，首先会清零内存空间，然后才会在上面分配对象（JVM内部会同步这两个操作）。因此，在以清零的内存空间（pre-zeroed memory）分配对象时，域的默认初始化已经完成了。 JMM不保证未同步程序的执行结果与该程序在顺序一致性模型中的执行结果一致。因为未同步程序在顺序一致性模型中执行时，整体上是无序的，其执行结果无法预知。保证未同步程序在两个模型中的执行结果一致毫无意义。 和顺序一致性模型一样，未同步程序在JMM中的执行时，整体上也是无序的，其执行结果也无法预知。同时，未同步程序在这两个模型中的执行特性有下面几个差异： 顺序一致性模型保证单线程内的操作会按程序的顺序执行，而JMM不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。这一点前面已经讲过了，这里就不再赘述。 顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而JMM不保证所有线程能看到一致的操作执行顺序。这一点前面也已经讲过，这里就不再赘述。 JMM不保证对64位的long型和double型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。 第3个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一系列步骤称之为总线事务（bus transaction）。总线事务包括读事务（read transaction）和写事务（write transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字。这里的关键是，总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和I/O设备执行内存的读/写。下面让我们通过一个示意图来说明总线的工作机制： 如上图所示，假设处理器A，B和C同时向总线发起总线事务，这时总线仲裁（bus arbitration）会对竞争作出裁决，这里我们假设总线在仲裁后判定处理器A在竞争中获胜（总线仲裁会确保所有处理器都能公平的访问内存）。此时处理器A继续它的总线事务，而其它两个处理器则要等待处理器A的总线事务完成后才能开始再次执行内存访问。假设在处理器A执行总线事务期间（不管这个总线事务是读事务还是写事务），处理器D向总线发起了总线事务，此时处理器D的这个请求会被总线禁止。 总线的这些工作机制可以把所有处理器对内存的访问以串行化的方式来执行；在任意时间点，最多只能有一个处理器能访问内存。这个特性确保了单个总线事务之中的内存读/写操作具有原子性。 在一些32位的处理器上，如果要求对64位数据的读/写操作具有原子性，会有比较大的开销。为了照顾这种处理器，java语言规范鼓励但不强求JVM对64位的long型变量和double型变量的读/写具有原子性。当JVM在这种处理器上运行时，会把一个64位long/ double型变量的读/写操作拆分为两个32位的读/写操作来执行。这两个32位的读/写操作可能会被分配到不同的总线事务中执行，此时对这个64位变量的读/写将不具有原子性。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>java语言</tag>
        <tag>内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解Java内存模型（二）——重排序]]></title>
    <url>%2F2013%2F01%2F26%2F2013-01-26-java-memory-model-2%2F</url>
    <content type="text"><![CDATA[原文链接：原文链接: http://www.infoq.com/cn/articles/java-memory-model-2 数据依赖性如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。数据依赖分下列三种类型： 名称 代码示例 说明 写后读 a = 1;b = a; 写一个变量之后，再读这个位置。 写后写 a = 1;a = 2; 写一个变量之后，再写这个变量。 读后写 a = b;b = 1; 读一个变量之后，再写这个变量。 上面三种情况，只要重排序两个操作的执行顺序，程序的执行结果将会被改变。 前面提到过，编译器和处理器可能会对操作做重排序。编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序。 注意，这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。 as-if-serial语义as-if-serial语义的意思指：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器，runtime 和处理器都必须遵守as-if-serial语义。 为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作可能被编译器和处理器重排序。为了具体说明，请看下面计算圆面积的代码示例：123double pi = 3.14; //Adouble r = 1.0; //Bdouble area = pi * r * r; //C 上面三个操作的数据依赖关系如下图所示： 如上图所示，A和C之间存在数据依赖关系，同时B和C之间也存在数据依赖关系。因此在最终执行的指令序列中，C不能被重排序到A和B的前面（C排到A和B的前面，程序的结果将会被改变）。但A和B之间没有数据依赖关系，编译器和处理器可以重排序A和B之间的执行顺序。下图是该程序的两种执行顺序： as-if-serial语义把单线程程序保护了起来，遵守as-if-serial语义的编译器，runtime 和处理器共同为编写单线程程序的程序员创建了一个幻觉：单线程程序是按程序的顺序来执行的。as-if-serial语义使单线程程序员无需担心重排序会干扰他们，也无需担心内存可见性问题。 程序顺序规则根据happens- before的程序顺序规则，上面计算圆的面积的示例代码存在三个happens- before关系： A happens- before B； B happens- before C； A happens- before C； 这里的第3个happens- before关系，是根据happens- before的传递性推导出来的。 这里A happens- before B，但实际执行时B却可以排在A之前执行（看上面的重排序后的执行顺序）。在第一章提到过，如果A happens- before B，JMM并不要求A一定要在B之前执行。JMM仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前。这里操作A的执行结果不需要对操作B可见；而且重排序操作A和操作B后的执行结果，与操作A和操作B按happens- before顺序执行的结果一致。在这种情况下，JMM会认为这种重排序并不非法（not illegal），JMM允许这种重排序。 在计算机中，软件技术和硬件技术有一个共同的目标：在不改变程序执行结果的前提下，尽可能的开发并行度。编译器和处理器遵从这一目标，从happens- before的定义我们可以看出，JMM同样遵从这一目标。 重排序对多线程的影响现在让我们来看看，重排序是否会改变多线程程序的执行结果。请看下面的示例代码：12345678910111213141516class ReorderExample &#123; int a = 0; boolean flag = false; public void writer() &#123; a = 1; //1 flag = true; //2 &#125; public void reader() &#123; if (flag) &#123; //3 int i = a * a; //4 …… &#125; &#125;&#125; lag变量是个标记，用来标识变量a是否已被写入。这里假设有两个线程A和B，A首先执行writer()方法，随后B线程接着执行reader()方法。线程B在执行操作4时，能否看到线程A在操作1对共享变量a的写入？ 答案是：不一定能看到。 由于操作1和操作2没有数据依赖关系，编译器和处理器可以对这两个操作重排序；同样，操作3和操作4没有数据依赖关系，编译器和处理器也可以对这两个操作重排序。让我们先来看看，当操作1和操作2重排序时，可能会产生什么效果？请看下面的程序执行时序图： 如上图所示，操作1和操作2做了重排序。程序执行时，线程A首先写标记变量flag，随后线程B读这个变量。由于条件判断为真，线程B将读取变量a。此时，变量a还根本没有被线程A写入，在这里多线程程序的语义被重排序破坏了！ ※注：本文统一用红色的虚箭线表示错误的读操作，用绿色的虚箭线表示正确的读操作。 下面再让我们看看，当操作3和操作4重排序时会产生什么效果（借助这个重排序，可以顺便说明控制依赖性）。下面是操作3和操作4重排序后，程序的执行时序图： 在程序中，操作3和操作4存在控制依赖关系。当代码中存在控制依赖性时，会影响指令序列执行的并行度。为此，编译器和处理器会采用猜测（Speculation）执行来克服控制相关性对并行度的影响。以处理器的猜测执行为例，执行线程B的处理器可以提前读取并计算a*a，然后把计算结果临时保存到一个名为重排序缓冲（reorder buffer ROB）的硬件缓存中。当接下来操作3的条件判断为真时，就把该计算结果写入变量i中。 从图中我们可以看出，猜测执行实质上对操作3和4做了重排序。重排序在这里破坏了多线程程序的语义！ 在单线程程序中，对存在控制依赖的操作重排序，不会改变执行结果（这也是as-if-serial语义允许对存在控制依赖的操作做重排序的原因）；但在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>java语言</tag>
        <tag>内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解Java内存模型（一）——基础]]></title>
    <url>%2F2013%2F01%2F22%2F2013-01-22-java-memory-model-1%2F</url>
    <content type="text"><![CDATA[原文链接: http://www.infoq.com/cn/articles/java-memory-model-1 并发编程模型的分类在并发编程中，我们需要处理两个关键问题：线程之间如何通信及线程之间如何同步（这里的线程是指并发执行的活动实体）。通信是指线程之间以何种机制来交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。 在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显式进行通信。 同步是指程序用于控制不同线程之间操作发生相对顺序的机制。在共享内存并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。 Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。如果编写多线程程序的Java程序员不理解隐式进行的线程之间通信的工作机制，很可能会遇到各种奇怪的内存可见性问题。 Java内存模型的抽象在java中，所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享（本文使用“共享变量”这个术语代指实例域，静态域和数组元素）。局部变量（Local variables），方法定义参数（java语言规范称之为formal method parameters）和异常处理器参数（exception handler parameters) 不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。 Java线程之间的通信由Java内存模型（本文简称为JMM）控制，JMM决定一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存(main memory) 中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。本地内存是JMM的一个抽象概念，并不真实存在。它涵盖了缓存，写缓冲区，寄存器以及其他的硬件和编译器优化。Java内存模型的抽象示意图如下： 从上图来看，线程A与线程B之间如要通信的话，必须要经历下面2个步骤： 首先，线程A把本地内存A中更新过的共享变量刷新到主内存中去。 然后，线程B到主内存中去读取线程A之前已更新过的共享变量。 下面通过示意图来说明这两个步骤： 如上图所示，本地内存A和B有主内存中共享变量x的副本。假设初始时，这三个内存中的x值都为0。线程A在执行时，把更新后的x值（假设值为1）临时存放在自己的本地内存A中。当线程A和线程B需要通信时，线程A首先会把自己本地内存中修改后的x值刷新到主内存中，此时主内存中的x值变为了1。随后，线程B到主内存中去读取线程A更新后的x值，此时线程B的本地内存的x值也变为了1。 从整体来看，这两个步骤实质上是线程A在向线程B发送消息，而且这个通信过程必须要经过主内存。JMM通过控制主内存与每个线程的本地内存之间的交互，来为java程序员提供内存可见性保证。 重排序在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型： 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 从java源代码到最终实际执行的指令序列，会分别经历下面三种重排序： 上述的1属于编译器重排序，2和3属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。对于编译器，JMM的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM的处理器重排序规则会要求java编译器在生成指令序列时，插入特定类型的内存屏障（memory barriers，intel称之为memory fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序（不是所有的处理器重排序都要禁止）。 JMM属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。 处理器重排序与内存屏障指令现代的处理器使用写缓冲区来临时保存向内存写入的数据。写缓冲区可以保证指令流水线持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟。同时，通过以批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，可以减少对内存总线的占用。虽然写缓冲区有这么多好处，但每个处理器上的写缓冲区，仅仅对它所在的处理器可见。这个特性会对内存操作的执行顺序产生重要的影响：处理器对内存的读/写操作的执行顺序，不一定与内存实际发生的读/写操作顺序一致！ 为了具体说明，请看下面示例： Processor A Processor B a = 1; //A1 x = b; //A2 b = 2; //B1 y = a; //B2 初始状态：a = b = 0 处理器允许执行后得到结果：x = y = 0 假设处理器A和处理器B按程序的顺序并行执行内存访问，最终却可能得到x = y = 0的结果。具体的原因如下图所示： 这里处理器A和处理器B可以同时把共享变量写入自己的写缓冲区（A1，B1），然后从内存中读取另一个共享变量（A2，B2），最后才把自己写缓存区中保存的脏数据刷新到内存中（A3，B3）。当以这种时序执行时，程序就可以得到x = y = 0的结果。 从内存操作实际发生的顺序来看，直到处理器A执行A3来刷新自己的写缓存区，写操作A1才算真正执行了。虽然处理器A执行内存操作的顺序为：A1-&gt;A2，但内存操作实际发生的顺序却是：A2-&gt;A1。此时，处理器A的内存操作顺序被重排序了（处理器B的情况和处理器A一样，这里就不赘述了）。 这里的关键是，由于写缓冲区仅对自己的处理器可见，它会导致处理器执行内存操作的顺序可能会与内存实际的操作执行顺序不一致。由于现代的处理器都会使用写缓冲区，因此现代的处理器都会允许对写-读操做重排序。 下面是常见处理器允许的重排序类型的列表： Load-Load Load-Store Store-Store Store-Load 数据依赖 sparc-TSO N N N Y N x86 N N N Y N ia64 Y Y Y Y N PowerPC Y Y Y Y N 上表单元格中的“N”表示处理器不允许两个操作重排序，“Y”表示允许重排序。 从上表我们可以看出：常见的处理器都允许Store-Load重排序；常见的处理器都不允许对存在数据依赖的操作做重排序。sparc-TSO和x86拥有相对较强的处理器内存模型，它们仅允许对写-读操作做重排序（因为它们都使用了写缓冲区）。 ※ 注1：sparc-TSO是指以TSO(Total Store Order)内存模型运行时，sparc处理器的特性。 ※ 注2：上表中的x86包括x64及AMD64。 ※ 注3：由于ARM处理器的内存模型与PowerPC处理器的内存模型非常类似，本文将忽略它。 ※ 注4：数据依赖性后文会专门说明。 为了保证内存可见性，java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。JMM把内存屏障指令分为下列四类： 屏障类型 指令示例 说明 LoadLoad Barriers Load1; LoadLoad; Load2 确保Load1数据的装载，之前于Load2及所有后续装载指令的装载。 StoreStore Barriers Store1; StoreStore; Store2 确保Store1数据对其他处理器可见（刷新到内存），之前于Store2及所有后续存储指令的存储。 LoadStore Barriers Load1; LoadStore; Store2 确保Load1数据装载，之前于Store2及所有后续的存储指令刷新到内存。 StoreLoad Barriers Store1; StoreLoad; Load2 确保Store1数据对其他处理器变得可见（指刷新到内存），之前于Load2及所有后续装载指令的装载。StoreLoad Barriers会使该屏障之前的所有内存访问指令（存储和装载指令）完成之后，才执行该屏障之后的内存访问指令。 StoreLoad Barriers是一个“全能型”的屏障，它同时具有其他三个屏障的效果。现代的多处理器大都支持该屏障（其他类型的屏障不一定被所有处理器支持）。执行该屏障开销会很昂贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中（buffer fully flush）。 happens-before从JDK5开始，java使用新的JSR -133内存模型（本文除非特别说明，针对的都是JSR- 133内存模型）。JSR-133提出了happens-before的概念，通过这个概念来阐述操作之间的内存可见性。如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在happens-before关系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。 与程序员密切相关的happens-before规则如下： 程序顺序规则：一个线程中的每个操作，happens- before 于该线程中的任意后续操作。 监视器锁规则：对一个监视器锁的解锁，happens- before 于随后对这个监视器锁的加锁。 volatile变量规则：对一个volatile域的写，happens- before 于任意后续对这个volatile域的读。 传递性：如果A happens- before B，且B happens- before C，那么A happens- before C。 注意，两个操作之间具有happens-before关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前（the first is visible to and ordered before the second）。happens- before的定义很微妙，后文会具体说明happens-before为什么要这么定义。 happens-before与JMM的关系如下图所示： 如上图所示，一个happens-before规则通常对应于多个编译器重排序规则和处理器重排序规则。对于java程序员来说，happens-before规则简单易懂，它避免程序员为了理解JMM提供的内存可见性保证而去学习复杂的重排序规则以及这些规则的具体实现。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>java语言</tag>
        <tag>内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[成为JavaGC专家（2）—如何监控Java垃圾回收机制]]></title>
    <url>%2F2012%2F12%2F28%2F2012-12-28.gc-2%2F</url>
    <content type="text"><![CDATA[原文链接:http://www.importnew.com/2057.html 本文是成为Java GC专家系列文章的第二篇。在第一篇《深入浅出Java垃圾回收机制》中我们学习了不同GC算法的执行过程，GC是如何工作的，什么是新生代和老年代，你应该了解的JDK7中的5种GC类型，以及这5种类型对于应用性能的影响。 在本文中，我将解释JVM到底是如何执行垃圾回收处理的。 什么是GC监控？ 垃圾回收收集监控指的是搞清楚JVM如何执行GC的过程，例如，我们可以查明： 何时一个新生代中的对象被移动到老年代时，所花费的时间。 Stop-the-world 何时发生的，持续了多长时间。 GC监控是为了鉴别JVM是否在高效地执行GC，以及是否有必要进行额外的性能调优。基于以上信息，我们可以修改应用程序或者调整GC算法（GC优化）。 如何监控GC 有很多种方法可以监控GC，但其差别仅仅是GC操作通过何种方式展现而已。GC操作是由JVM来完成，而GC监控工具只是将JVM提供的GC信息展现给你，因此，不论你使用何种方式监控GC都将得到相同的结果。所以你也就不必去学习所有的监控GC的方法。但是因为学习每种监控方法不会占用太多时间，了解多一点可以帮助你根据不同的场景选择最为合适的方式。 下面所列的工具以及JVM参数并不适用于所有的HVM供应商。这是因为并没有关于GC信息的强制标准。本文我们将使用HotSpot JVM (Oracle JVM)。因为NHN 一直在使用Oracle (Sun) JVM，所以用它作为示例来解释我们提到的工具和JVM参数更容易些。首先，GC监控方法根据访问的接口不同，可以分成CUI 和GUI 两大类。CUI GC监控方法使用一个独立的叫做”jstat”的CUI应用，或者在启动JVM的时候选择JVM参数”verbosegc”。GUI GC监控由一个单独的图形化应用来完成，其中三个最常用的应用是”jconsole”, “jvisualvm” 和 “Visual GC”。下面我们来详细学习每种方法。 jstat jstat 是HotSpot JVM提供的一个监控工具。其他监控工具还有jps 和jstatd。有些时候，你可能需要同时使用三种工具来监控你的应用。jstat 不仅提供GC操作的信息，还提供类装载操作的信息以及运行时编译器操作的信息。本文将只涉及jstat能够提供的信息中与监控GC操作信息相关的功能。jstat 被放置在$JDK_HOME/bin。因此只要java 和 javac能执行，jstat 同样可以执行。你可以在命令行环境下执行如下语句。1234567$&gt; jstat –gc $&lt;vmid$&gt; 1000 S0C S1C S0U S1U EC EU OC OU PC PU YGC YGCT FGC FGCT GCT3008.0 3072.0 0.0 1511.1 343360.0 46383.0 699072.0 283690.2 75392.0 41064.3 2540 18.454 4 1.133 19.5883008.0 3072.0 0.0 1511.1 343360.0 47530.9 699072.0 283690.2 75392.0 41064.3 2540 18.454 4 1.133 19.5883008.0 3072.0 0.0 1511.1 343360.0 47793.0 699072.0 283690.2 75392.0 41064.3 2540 18.454 4 1.133 19.588``` 在上图的例子中，实际的数据会按照如下列输出： 1 S0C S1C S0U S1U EC EU OC OU PC12345678910111213141516**vmid (虚拟机 ID)**，正如其名字描述的，它是虚拟机的ID，Java应用不论运行在本地还是远程的机器都会拥有自己独立的vmid。运行在本地机器上的vmid称之为lvmid (本地vmid)，通常是PID。如果想得到PID的值你可以使用ps命令或者windows任务管理器，但我们推荐使用**jps**来获取，因为PID和lvmid有时会不一致。jps 通过Java PS实现，**jps命令会返回vmids和main方法的信息**，正如ps命令展现PIDS和进程名字那样。首先通过jps命令找到你要监控的Java应用的vmid，并把它作为jstat的参数。当几个WAS实例运行在同一台设备上时，如果你只使用jps命令，将只能看到启动（bootstrap）信息。我们建议在这种情况下使用ps -ef | grep java与jps配合使用。想要得到GC性能相关的数据需要持续不断地监控，因此在执行jstat时，要规则地输出GC监控的信息。例如，执行”jstat –gc 1000″ (或 1s)会每隔一秒展示GC监控数据。”jstat –gc 1000 10″会每隔1秒展现一次，且一共10次。![](http://governlab.cn/images/gc-1.png)**你可以只关心那些最常用的命令，你会经常用到 -gcutil (或-gccause), -gc and –gccapacity。**· -gcutil 被用于检查堆间的使用情况，GC执行的次数以及GC操作所花费的时间。· -gccapacity以及其他的参数可以用于检查实际分配内存的大小。使用-gc 参数你可以看到如下输出： S0C S1C … GCT1248.0 896.0 … 1.2461248.0 896.0 … 1.246… … … …1234567891011121314151617181920212223242526272829303132333435363738不同的jstat参数输出不同类型的列，如下表所示，根据你使用的”jstat option”会输出不同列的信息。|列|说明|Jstat参数||--|--|--||S0C| 输出Survivor0空间的大小。单位KB。| -gc -gccapacity -gcnew -gcnewcapacity||S1C| 输出Survivor1空间的大小。单位KB。| -gc -gccapacity -gcnew -gcnewcapacity||S0U| 输出Survivor0已用空间的大小。单位KB。| -gc -gcnew||S1U| 输出Survivor1已用空间的大小。单位KB。| -gc -gcnew||EC|输出Eden空间的大小。单位KB。| -gc -gccapacity -gcnew -gcnewcapacity||EU |输出Eden已用空间的大小。单位KB。| -gc -gcnew||OC| 输出老年代空间的大小。单位KB。|-gc -gccapacity -gcold -gcoldcapacity||OU| 输出老年代已用空间的大小。单位KB。| -gc -gcold||PC|输出持久代空间的大小。单位KB。| -gc -gccapacity -gcold -gcoldcapacity||PU|输出持久代已用空间的大小。单位KB。| -gc -gcold||YGC| 新生代空间GC时间发生的次数。| -gc -gccapacity -gcnew -gcnewcapacity -gcold -gcoldcapacity -gcpermcapacity -gcutil -gccause||YGCT| 新生代GC处理花费的时间。 |-gc -gcnew -gcutil -gccause||FGC| full GC发生的次数。| -gc -gccapacity -gcnew -gcnewcapacity -gcold -gcoldcapacity -gcpermcapacity -gcutil -gccause||FGCT| full GC操作花费的时间| -gc -gcold -gcoldcapacity -gcpermcapacity -gcutil -gccause||GCT| GC操作花费的总时间。| -gc -gcold -gcoldcapacity -gcpermcapacity -gcutil -gccause||NGCMN| 新生代最小空间容量，单位KB。| -gccapacity -gcnewcapacity||NGCMX| 新生代最大空间容量，单位KB。| -gccapacity -gcnewcapacity||NGC| 新生代当前空间容量，单位KB。 |-gccapacity -gcnewcapacity||OGCMN| 老年代最小空间容量，单位KB。| -gccapacity -gcoldcapacity||OGCMX| 老年代最大空间容量，单位KB。| -gccapacity -gcoldcapacity||OGC| 老年代当前空间容量制，单位KB。| -gccapacity -gcoldcapacity||PGCMN| 持久代最小空间容量，单位KB。| -gccapacity -gcpermcapacity||PGCMX| 持久代最大空间容量，单位KB。| -gccapacity -gcpermcapacity||PGC| 持久代当前空间容量，单位KB。| -gccapacity -gcpermcapacity||PC| 持久代当前空间大小，单位KB| -gccapacity -gcpermcapacity||PU| 持久代当前已用空间大小，单位KB| -gc -gcold||LGCC |最后一次GC发生的原因| -gccause||GCC| 当前GC发生的原因 |-gccause||TT |老年化阈值。被移动到老年代之前，在新生代空存活的次数。| -gcnew||MTT| 最大老年化阈值。被移动到老年代之前，在新生代空存活的次数。| -gcnew||DSS |幸存者区所需空间大小，单位KB。 |-gcnew|jstat 的好处是它可以持续的监控GC操作数据，不论Java应用是运行在本地还是远程，只要有控制台的地方就可以使用。当使用–gcutil 会输出如下信息。在GC优化的时候，你需要特别注意YGC, YGCT, FGC, FGCT 和GCT。 S0 S1 E O P YGC YGCT FGC FGCT GCT0.00 66.44 54.12 10.58 86.63 217 0.928 2 0.067 0.9950.00 66.44 54.12 10.58 86.63 217 0.928 2 0.067 0.9950.00 66.44 54.12 10.58 86.63 217 0.928 2 0.067 0.995123456789101112131415161718192021222324这些信息很重要，因为它们展示了GC处理到底花费了多少时间。 在这个例子中，YGC 是217而YGCT 是0.928，这样在简单的计算数据平均数后(YGCT/YGC)，你可以知道每次新生代的GC大概需要4ms（0.004秒），而full GC的平均时间为33ms。但是，只看数据平均数经常无法分析出真正的GC问题。这是主要是因为**GC操作时间严重的偏差**（换句话说，假如两次full GC的时间是 67ms，那么其中的一次full GC可能执行了10ms而另一个可能执行了57ms。）为了更好地检测每次GC处理时间，最好使用 –verbosegc来替代数据平均数。**-verbosegc****-verbosegc 是在启动一个Java应用时可以指定的JVM参数之一**。而jstat 可以监控任何JVM应用，即便它没有制定任何参数。 -verbosegc 需要在启动的时候指定，因此你可能会认为它没有必要（因为jstat可以替代之）。但是， -verbosegc 会以更浅显易懂的方式展现GC发生的结果，因此他对于监控监控GC信息十分有用。|jstat| -verbosegc||--|--|--||监控对象| 运行在本机的Java应用可以把日志输出到终端上，或者借助jstatd命令通过网络连接远程的Java应用。| 只有那些把-verbogc作为启动参数的JVM。||输出信息| 堆状态（已用空间，最大限制，GC执行次数/时间，等等）| 执行GC前后新生代和老年代空间大小，GC执行时间。||输出时间| Every designated time 每次设定好的时间。 |每次GC发生的时候。||何时有用| 当你试图观察堆空间变化情况| 当你试图了解单次GC产生的效果。|下面是-verbosegc 的可用参数* -XX:+PrintGCDetails* -XX:+PrintGCTimeStamps* -XX:+PrintHeapAtGC* -XX:+PrintGCDateStamps (from JDK 6 update 4)如果只是用了 -verbosegc 。那么默认会加上 -XX:+PrintGCDetails。 –verbosgc 的附加参数并不是独立的。而是经常组合起来使用。 使用 –verbosegc后，每次GC发生你都会看到如下格式的结果。 [GC [: -&gt; , secs] -&gt; , secs]12345678910|收集器 |minor gc使用的收集器的名字。||--|--||starting occupancy1| GC执行前新生代空间大小。||ending occupancy1| GC执行后新生代空间大小。||pause time1 |因为执行minor GC，Java应用暂停的时间。||starting occupancy3| GC执行前堆区域总大小||ending occupancy3| GC执行后堆区域总大小||pause time3 |Java应用由于执行堆空间GC（包括major GC）而停止的时间。|这是 Full GC发生时的例子 [Full GC [Tenured: 3485K-&gt;4095K(4096K), 0.1745373 secs] 61244K-&gt;7418K(63104K), [Perm : 10756K-&gt;10756K(12288K)], 0.1762129 secs] [Times: user=0.19 sys=0.00, real=0.19 secs]```如果使用了 CMS collector，那么如下CMS信息也会被输出。由于 –verbosegc 参数在每次GC事件发生的时候都会输出日志，我们可以很轻易地观察到GC操作对于堆空间的影响。 (Java) VisualVM + Visual GC Java Visual VM是由Oracle JDK提供的图形化的汇总和监控工具。 图1: VisualVM 截图 除了JDK中自带的版本，你还可以直接从官网下载Visual VM。出于便利性的考虑，JDK中包含的版本被命名为Java VisualVM (jvisualvm),而官网提供的版本被命名为Visual VM (visualvm)。两者的功能基本相同，只有一些细小的差别，例如安装组件的时候。就个人而言，我更喜欢可以从官网下载的Visual VM。 通过Visual GC，你可以更直观的看到执行jstatd 所得到的信息。 本文我们主要讲述了如果监控GC操作信息，这将是GC优化的前提。就我个人经验而言，我推荐使用jstat 来监控GC操作，如果你感觉到GC操作的执行时间过长，那就可以使用verbosegc 参数来分析GC。GC优化的大体步骤就是在添加verbosegc 参数后，调整GC参数，分析修改后的结果。在下一篇文章中，我们将通过真实的例子来讲解优化GC的最佳选择。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>java语言</tag>
        <tag>gc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[成为JavaGC专家（1）—深入浅出Java垃圾回收机制]]></title>
    <url>%2F2012%2F01%2F22%2F2012-12-27.gc-1%2F</url>
    <content type="text"><![CDATA[原文链接:http://www.importnew.com/1993.html 对于Java开发人员来说，了解垃圾回收机制（GC）有哪些好处呢？首先可以满足作为一名软件工程师的求知欲，其次，深入了解GC如何工作可以帮你写出更好的Java应用。 这仅仅代表我个人的意见，但我坚信一个精通GC的人往往是一个好的Java开发者。如果你对GC的处理过程感兴趣，说明你已经具备较大规模应用的开发经验。如果你曾经想过如何正确的选择GC算法，那意味着你已经完全理解你所开发的应用的特点。当然，我们不能以偏概全，这不能作为评价一个好的开发人员的共通标准。但是，我要说的是，深入理解GC是成为一名伟大的程序员的必经之路。 这是成为JavaGC专家系列文章的第一篇，本篇主要针对GC机制进行介绍，在下一篇中，我们将重点探讨分析GC状态以及来自NHN的GC调优的例子。 本文的目的是以一种简单的方式向你介绍GC机制。我希望这些文章能够帮到你。实际上，我的学生已经在Twitter上发布了一些很好的关于Java内核的文章，并且大受欢迎。有兴趣的话，你也可以关注他们。 回到正题，咱们继续谈垃圾回收，在学习GC之前，你首先应该记住一个单词：“stop-the-world”。Stop-the-world会在任何一种GC算法中发生。Stop-the-world意味着 JVM 因为要执行GC而停止了应用程序的执行。当Stop-the-world发生时，除了GC所需的线程以外，所有线程都处于等待状态，直到GC任务完成。GC优化很多时候就是指减少Stop-the-world发生的时间。 按代的垃圾回收机制在Java程序中不能显式地分配和注销内存。有些人把相关的对象设置为null或者调用System.gc()来试图显式地清理内存。设置为null至少没什么坏处，但是调用System.gc()会显著地影响系统性能，必须彻底杜绝（还好，我还没有见到NHN的哪个开发者调用这个方法）。 在Java中，开发人员无法直接在程序代码中清理内存，而是由垃圾回收器自动寻找不必要的垃圾对象，并且清理掉他们。垃圾回收器会在下面两种假设（hypotheses）成立的情况下被创建（称之为假设不如改为推测（suppositions）或者前提（preconditions））。 大多数对象会很快变得不可达 只有很少的由老对象（创建时间较长的对象）指向新生对象的引用 这些假设我们称之为弱年代假设（ weak generational hypothesis）。为了强化这一假设，HotSpot虚拟机将其物理上划分为两个–新生代（young generation）和老年代（old generation）。新生代（Young generation）: 绝大多数最新被创建的对象会被分配到这里，由于大部分对象在创建后会很快变得不可到达，所以很多对象被创建在新生代，然后消失。对象从这个区域消失的过程我们称之为”minor GC“。 老年代（Old generation）: 对象没有变得不可达，并且从新生代中存活下来，会被拷贝到这里。其所占用的空间要比新生代多。也正由于其相对较大的空间，发生在老年代上的GC要比新生代少得多。对象从老年代中消失的过程，我们称之为”major GC“（或者”full GC“） 请看下面这个图表。 图1 : GC 空间 &amp; 数据流 上图中的持久代（ permanent generation ）也被称为方法区（method area）。他用来保存类常量以及字符串常量。因此，这个区域不是用来永久的存储那些从老年代存活下来的对象。这个区域也可能发生GC。并且发生在这个区域上的GC事件也会被算为major GC。 有些人可能会问：如果老年代的对象需要引用一个新生代的对象，会发生什么呢？为了解决这个问题，老年代中存在一个”card table“，他是一个512 byte大小的块。所有老年代的对象指向新生代对象的引用都会被记录在这个表中。当针对新生代执行GC的时候，只需要查询card table来决定是否可以被收集，而不用查询整个老年代。这个card table由一个write barrier来管理。write barrier给GC带来了很大的性能提升，虽然由此可能带来一些开销，但GC的整体时间被显著的减少。 图 2: Card Table 结构 新生代的构成为了更好地理解GC，我们现在来学习新生代，新生代是用来保存那些第一次被创建的对象，他可以被分为三个空间 一个伊甸园空间（Eden ） 两个幸存者空间（Survivor ）一共有三个空间，其中包含两个幸存者空间。每个空间的执行顺序如下： 绝大多数刚刚被创建的对象会存放在伊甸园空间。 在伊甸园空间执行了第一次GC之后，存活的对象被移动到其中一个幸存者空间。 此后，在伊甸园空间执行GC之后，存活的对象会被堆积在同一个幸存者空间。 当一个幸存者空间饱和，还在存活的对象会被移动到另一个幸存者空间。之后会清空已经饱和的那个幸存者空间。 在以上的步骤中重复几次依然存活的对象，就会被移动到老年代。 如果你仔细观察这些步骤就会发现，其中一个幸存者空间必须保持是空的。如果两个幸存者空间都有数据，或者两个空间都是空的，那一定标志着你的系统出现了某种错误。通过频繁的minor GC将数据移动到老年代的过程可以用下图来描述： 图 3: GC执行前后对比 需要注意的是HotSpot虚拟机使用了两种技术来加快内存分配。他们分别是是”bump-the-pointer“和“TLABs（Thread-Local Allocation Buffers）”。 Bump-the-pointer技术跟踪在伊甸园空间创建的最后一个对象。这个对象会被放在伊甸园空间的顶部。如果之后再需要创建对象，只需要检查伊甸园空间是否有足够的剩余空间。如果有足够的空间，对象就会被创建在伊甸园空间，并且被放置在顶部。这样以来，每次创建新的对象时，只需要检查最后被创建的对象。这将极大地加快内存分配速度。但是，如果我们在多线程的情况下，事情将截然不同。如果想要以线程安全的方式以多线程在伊甸园空间存储对象，不可避免的需要加锁，而这将极大地的影响性能。TLABs 是HotSpot虚拟机针对这一问题的解决方案。该方案为每一个线程在伊甸园空间分配一块独享的空间，这样每个线程只访问他们自己的TLAB空间，再与bump-the-pointer技术结合可以在不加锁的情况下分配内存。以上是针对新生代空间GC技术的简要介绍，你不需要刻意记住我刚刚提到的两种技术。不知道他们不会对你产生什么影响，但是请务必记住在对象刚刚被创建之后，是保存在伊甸园空间的。那些长期存活的对象会经由幸存者空间转存在老年代空间。 老年代GC处理机制老年代空间的GC事件基本上是在空间已满时发生，执行的过程根据GC类型不同而不同，因此，了解不同的GC类型将有助于你理解本节的内容。JDK7一共有5种GC类型： Serial GC Parallel GC Parallel Old GC (Parallel Compacting GC) Concurrent Mark &amp; Sweep GC (or “CMS”) Garbage First (G1) GC 其中，Serial GC不应该被用在服务器上。这种GC类型在单核CPU的桌面电脑时代就存在了。使用Serial GC会显著的降低应用的性能指标。现在，让我们共同学习每一种GC类型 1. Serial GC (-XX:+UseSerialGC)新生代空间的GC方式我们在前面已经介绍过了，在老年代空间中的GC采取称之为”mark-sweep-compact“的算法。 算法的第一步是标记老年代中依然存活对象。（标记）第二步，从头开始检查堆内存空间，并且只留下依然幸存的对象。（清理）最后一步，从头开始，顺序地填满堆内存空间，并且将对内存空间分成两部分：一个保存着对象，另一个空着（压缩）。 2. Parallel GC (-XX:+UseParallelGC) 图 4: Serial GC 与 Parallel GC的区别 从上图中，你可以轻易地看出serial GC和parallel GC的区别，serial GC只使用一个线程执行GC，而parallel GC使用多个线程，因此parallel GC更高效。这种GC在内存充足以及多核的情况下会很有用，因此我们也称之为”throughput GC“。 3. Parallel Old GC(-XX:+UseParallelOldGC)Parallel Old GC在JDK5之后出现。与parallel GC相比，唯一的区别在于针对老年代的GC算法。Parallel Old GC分为三步：标记-汇总-压缩（mark – summary – compaction）。汇总（summary）步骤与清理（sweep）的不同之处在于，其将依然幸存的对象分发到GC预先处理好的不同区域，算法相对清理来说略微复杂一点。 4. CMS GC (-XX:+UseConcMarkSweepGC) 图 5: Serial GC &amp; CMS GC 就像你从上图看到的那样, CMS GC比我之前解释的各种算法都要复杂很多。第一步初始化标记（initial mark） 比较简单。这一步骤只是查找那些距离类加载器最近的幸存对象。因此，停顿的时间非常短暂。在之后的并行标记（ concurrent mark ）步骤，所有被幸存对象引用的对象会被确认是否已经被追踪和校验。这一步的不同之处在于，在标记的过程中，其他的线程依然在执行。在重新标记（remark）步骤，会再次检查那些在并行标记步骤中增加或者删除的与幸存对象引用的对象。最后，在并行交换（ concurrent sweep ）步骤，转交垃圾回收过程处理。垃圾回收工作会在其他线程的执行过程中展开。一旦采取了这种GC类型，由GC导致的暂停时间会极其短暂。CMS GC也被称为低延迟GC。它经常被用在那些对于响应时间要求十分苛刻的应用之上。 当然，这种GC类型在拥有stop-the-world时间很短的优点的同时，也有如下缺点： 它会比其他GC类型占用更多的内存和CPU 默认情况下不支持压缩步骤 在使用这个GC类型之前你需要慎重考虑。如果因为内存碎片过多而导致压缩任务不得不执行，那么stop-the-world的时间要比其他任何GC类型都长，你需要考虑压缩任务的发生频率以及执行时间。 5. G1 GC最后，我们来学习垃圾回收优先（G1）GC类型。 图 6: G1 GC的结构 如果你想要理解G1，首先你要忘记你所学过的新生代和老年代的概念。正如你在上图所看到的，每个对象被分配到不同的格子，随后GC执行。当一个区域装满之后，对象被分配到另一个区域，并执行GC。这中间不再有从新生代移动到老年代的三个步骤。这个类型是为了替代CMS GC而被创建的，因为CMS GC在长时间持续运作时会产生很多问题。 G1最大的好处是性能，他比我们在上面讨论过的任何一种GC都要快。但是在JDK 6中，他还只是一个早期试用版本。在JDK7之后才由官方正式发布。就我个人看来，NHN在将JDK 7正式投入商用之前需要很长的一段测试期（至少一年）。因此你可能需要再等一段时间。并且，我也听过几次使用了JDK 6中的G1而导致Java虚拟机宕机的事件。请耐心的等到它更稳定吧。 下一次我将讨论GC优化相关的问题，但是在此之前我要先明确一件事情，假如应用中创建的所有对象的大小和类型都是统一的，那么公司使用的WAS的GC参数可以是相同的。但是WAS所创建对象的大小和生命周期根据服务以及硬件的不同而不同。换句话说，不能因为某个应用使用的GC参数“A”，就说明同样的参数也能给其他服务带来最佳的效果。而是要因地制宜，有的放矢。我们需要找到适合每个WAS线程的参数，并且持续的监控和优化每个设备上的WAS实例。这并不是我的一家之谈，而是负责Oracle Java虚拟机研发的工程师在 JavaOne 2010上已经讨论过的。 本文中我们简略的介绍了Java的GC机制，请继续关于我们的后续文章，我们将会讨论如何监控Java GC状态以及优化GC。]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>java语言</tag>
        <tag>gc</tag>
      </tags>
  </entry>
</search>
